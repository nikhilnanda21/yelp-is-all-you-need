{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions for Project 1 - Sentiment Classification\n",
    "\n",
    "Hello everyone, this is Zihao. I am very happy to host the first project\n",
    "\n",
    "In this project, you will conduct a sentiment analysis task.\n",
    "You will build a model to predict the scores (a.k.a. stars, from 1-5) of each review.\n",
    "For each review, you are given a piece of text as well as some other features (Explore yourself!).\n",
    "You can consider the predicted variables to be categorical, ordinal or numerical.\n",
    "\n",
    "DDL: *April 6, 2021*\n",
    "- *March 23, 2021* release the validation score of weak baseline\n",
    "- *March 30, 2021* release the validation score of strong baseline\n",
    "\n",
    "Submission: Each team leader is required to submit the groupNo.zip file in the canvas. It shoud contain \n",
    "- `pre.csv` Predictions on test data (please make sure you can successfully evaluate your validation predictions on the validation data with the help of evaluate.py)\n",
    "- report (1-2 pages of pdf)\n",
    "- code (Frameworks and programming languages are not restricted.)\n",
    "\n",
    "We will check your report with your code and the accuracy.\n",
    "\n",
    "| Grade | Classifier (80%)                                                   | Report (20%)                      |\n",
    "|-------|--------------------------------------------------------------------|-----------------------------------|\n",
    "| 50%   | example code in tutorials or in Project 1 without any modification | submission                        |\n",
    "| 60%   | an easy baseline that most students can outperform                 | algorithm you used                |\n",
    "| 80%   | a competitive baseline that about half students can surpass        | detailed explanation              |\n",
    "| 90%   | a very competitive baseline without any special mechanism          | detailed explanation and analysis, such as explorative data analysis and ablation study |\n",
    "| 100%  | a very competitive baseline with at least one mechanism            | excellent ideas, detailed explanation and solid analysis |\n",
    "\n",
    "\n",
    "\n",
    "In this notebook, you are provided with the code snippets for you to start.\n",
    "\n",
    "The content follows previous lectures and tutorials. But I may mention some useful python packages.\n",
    "\n",
    "## Instruction Content\n",
    "\n",
    "1. Load & Dump the data\n",
    "    1. Load the data\n",
    "    1. Dump the data\n",
    "1. Preprocessing\n",
    "    1. Text data processing recap\n",
    "    1. Explorative data analysis\n",
    "1. Learning Baselines\n",
    "\n",
    "## 1. Load & Dump the data\n",
    "\n",
    "The same as previous tutorials, we use `pandas` as the basic tool to load & dump the data.\n",
    "The key ingredient of our operation is the `DataFrame` in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Load the data\n",
    "\n",
    "Here is a function to load your data, remember put the dataset in the `data_2021_spring` folder.\n",
    "\n",
    "Each year we release different data, so old models are not guaranteed to solve the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(split_name='train', columns=['text', 'stars']):\n",
    "    try:\n",
    "        print(f\"select [{', '.join(columns)}] columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
    "        df = df.loc[:,columns]\n",
    "        print(\"succeed!\")\n",
    "        return df\n",
    "    except:\n",
    "        print(\"Failed, then try to \")\n",
    "        print(f\"select all columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can extract the data by specifying the desired split and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text] columns from the train split\n",
      "succeed!\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data('train', columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nice to have a diner still around. Food was go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tried this a while back, got the fried chicken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I expected more pork selections on menu. Food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YUMMY!!! This place is phenomenal. It is Price...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Truffle Macaroni &amp; Cheese and Potatoes Au ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Nice to have a diner still around. Food was go...\n",
       "1  Tried this a while back, got the fried chicken...\n",
       "2  I expected more pork selections on menu. Food ...\n",
       "3  YUMMY!!! This place is phenomenal. It is Price...\n",
       "4  The Truffle Macaroni & Cheese and Potatoes Au ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, stars] columns from the test split\n",
      "Failed, then try to \n",
      "select all columns from the test split\n"
     ]
    }
   ],
   "source": [
    "test_df = load_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Dump the random answer\n",
    "\n",
    "In this project, your predictions on test data are supposed to be submitted by a csv file of two columns, i.e. (review_id and stars)\n",
    "\n",
    "Here we compose the random answer in a DataFrame and dump the answer into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ans = pd.DataFrame(data={\n",
    "    'review_id': test_df['review_id'],\n",
    "    'stars': np.random.randint(0, 6, size=len(test_df))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b8-ELBwhmDKcmcM8icT86g</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rBpAJhIen_V-zLoXZIcROg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_pALaDG6se9OTkGGhyhnNA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ru8fpA1Uk0tTFtO5hLM49g</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fRPgwuFoY6SriToXZyaOQA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id  stars\n",
       "0  b8-ELBwhmDKcmcM8icT86g      2\n",
       "1  rBpAJhIen_V-zLoXZIcROg      0\n",
       "2  _pALaDG6se9OTkGGhyhnNA      0\n",
       "3  ru8fpA1Uk0tTFtO5hLM49g      2\n",
       "4  fRPgwuFoY6SriToXZyaOQA      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_ans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_number = -1\n",
    "random_ans.to_csv(f'{group_number}-random_ans.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "Preprocessing and feature engineering is important in machine learning\n",
    "\n",
    "### A. Text data processing recap\n",
    "In our tutorials, Haoran have showed you how to extract textual features by the `nltk` package\n",
    "\n",
    "Remember to use the NLTK Downloader to obtain the resource:\n",
    "```\n",
    "  >>> import nltk\n",
    "  >>> nltk.download('stopwords')\n",
    "  >>> nltk.download('punkt')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def lower(s):\n",
    "    \"\"\"\n",
    "    :param s: a string.\n",
    "    return a string with lower characters\n",
    "    Note that we allow the input to be nested string of a list.\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: 'text mining is to identify useful information.'\n",
    "    \"\"\"\n",
    "    if isinstance(s, list):\n",
    "        return [lower(t) for t in s]\n",
    "    if isinstance(s, str):\n",
    "        return s.lower()\n",
    "    else:\n",
    "        raise NotImplementedError(\"unknown datatype\")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results\n",
    "\n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_onehot_vector(feats, feats_dict):\n",
    "    \"\"\"\n",
    "    :param data: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
    "    for f in feats:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can use the `map` function to apply your preprocessing functions into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [i, took, up, train, union, station, catch, ai...\n",
      "1    [we, worked, fitness, twist, part, best, frien...\n",
      "2    [it, 's, typical, ,, average, ,, run-of-the-mi...\n",
      "3    [we, went, outback, today, celebrate, daughter...\n",
      "4    [we, went, see, nashville, unplugged, country,...\n"
     ]
    }
   ],
   "source": [
    "test_df['tokens'] = test_df['text'].map(tokenize).map(filter_stopwords).map(lower)\n",
    "print(test_df['tokens'].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides `nltk`, I would like to introduce `SpaCy`, a newer text processing toolkit of industrial strength.\n",
    "\n",
    "You can explore it at https://spacy.io/\n",
    "\n",
    "Let's install it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -m pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy enables you use linguistic features of texts\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw       ,\t stem      ,\t PartOfSpeech,\t dependency,\t shape     ,\t is alpha  ,\t is stop   ,\t its childrens in the parsing tree,\t \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Apple     ,\t Apple     ,\t PROPN     ,\t nsubj     ,\t Xxxxx     ,\t True      ,\t False     ,\t []        ,\t \n",
      "is        ,\t be        ,\t AUX       ,\t aux       ,\t xx        ,\t True      ,\t True      ,\t []        ,\t \n",
      "looking   ,\t look      ,\t VERB      ,\t ROOT      ,\t xxxx      ,\t True      ,\t False     ,\t [Apple, is, at, startup],\t \n",
      "at        ,\t at        ,\t ADP       ,\t prep      ,\t xx        ,\t True      ,\t True      ,\t [buying]  ,\t \n",
      "buying    ,\t buy       ,\t VERB      ,\t pcomp     ,\t xxxx      ,\t True      ,\t False     ,\t [U.K.]    ,\t \n",
      "U.K.      ,\t U.K.      ,\t PROPN     ,\t dobj      ,\t X.X.      ,\t False     ,\t False     ,\t []        ,\t \n",
      "startup   ,\t startup   ,\t NOUN      ,\t advcl     ,\t xxxx      ,\t True      ,\t False     ,\t [for]     ,\t \n",
      "for       ,\t for       ,\t ADP       ,\t prep      ,\t xxx       ,\t True      ,\t True      ,\t [billion] ,\t \n",
      "$         ,\t $         ,\t SYM       ,\t quantmod  ,\t $         ,\t False     ,\t False     ,\t []        ,\t \n",
      "1         ,\t 1         ,\t NUM       ,\t compound  ,\t d         ,\t False     ,\t False     ,\t []        ,\t \n",
      "billion   ,\t billion   ,\t NUM       ,\t pobj      ,\t xxxx      ,\t True      ,\t False     ,\t [$, 1]    ,\t \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "fmt = \"{:10s},\\t \" * 8\n",
    "print(fmt.format('raw', 'stem', 'PartOfSpeech', 'dependency', 'shape', 'is alpha', 'is stop', 'its childrens in the parsing tree'))\n",
    "print('-'*140)\n",
    "for token in doc:\n",
    "    print(fmt.format(token.text, token.lemma_, token.pos_, token.dep_,\n",
    "            token.shape_, str(token.is_alpha), str(token.is_stop), str(list(token.children))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy also allows you use the embeddings for both sentence and words\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is looking at buying U.K. startup for $1 billion [ 0.4847193   0.34561655  0.23650904 -0.27294627  0.30828613] ...\n",
      "Apple [ 0.9396687   0.46727175 -0.3862503  -0.23296848  0.25683203] ...\n",
      "is [-0.21470308 -0.36800703  1.8618155  -0.43874717 -0.6448474 ] ...\n",
      "looking [ 1.5960355  -0.01218066 -0.1948367   0.7979922   0.36900565] ...\n",
      "at [-1.2617028  -0.8116296  -0.55736023  0.08604071 -0.43663728] ...\n",
      "buying [ 0.3020423  -0.9611639   1.2695026   0.10633498  2.8583994 ] ...\n",
      "U.K. [ 2.2959712   0.78135234 -1.0174923  -0.5566485   0.69199914] ...\n",
      "startup [0.6782811  0.03798376 0.07798427 0.1210558  0.5636424 ] ...\n",
      "for [-0.07904667 -0.21996386 -1.3529027  -0.24131706  0.43687835] ...\n",
      "$ [ 0.44878927  0.75564337  0.5757578  -1.1713823   0.7438692 ] ...\n",
      "1 [-0.3846085  2.7049747  2.7081459 -1.4393395 -0.5412608] ...\n",
      "billion [ 1.011186    1.4275012  -0.38276425 -0.03342953 -0.9067332 ] ...\n"
     ]
    }
   ],
   "source": [
    "print(doc, doc.vector[:5], '...')\n",
    "for t in doc:\n",
    "    print(t, t.vector[:5], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more usage of SpaCy, you can refer to the documentation of spacy https://spacy.io/usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Explorative data analysis\n",
    "\n",
    "For our dataset, we have features more than text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [f, u, l, l] columns from the train split\n",
      "Failed, then try to \n",
      "select all columns from the train split\n"
     ]
    }
   ],
   "source": [
    "train_df_full = load_data('train', columns='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39rLHYJOy2774ZIUouuWLw</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-06-28 21:44:02</td>\n",
       "      <td>0</td>\n",
       "      <td>ynzOFepQYSCDGdfWDWxiZw</td>\n",
       "      <td>4</td>\n",
       "      <td>Nice to have a diner still around. Food was go...</td>\n",
       "      <td>0</td>\n",
       "      <td>Sl6VgFOB-XXfFIAYp7TFkw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E-Kq1Yu1d6N3TL2qX0aqjA</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-09 03:45:19</td>\n",
       "      <td>0</td>\n",
       "      <td>sQX9ncJBEdBf16AWsvO6Vg</td>\n",
       "      <td>2</td>\n",
       "      <td>Tried this a while back, got the fried chicken...</td>\n",
       "      <td>0</td>\n",
       "      <td>gcx01pMqWzkni2UC-zoZrA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nWW6fBfBljiRFa4sG7TyxA</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-07-19 02:00:04</td>\n",
       "      <td>0</td>\n",
       "      <td>bVIf2kqbzvif3miNe3ARNw</td>\n",
       "      <td>4</td>\n",
       "      <td>I expected more pork selections on menu. Food ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Mn9VzPbrCYU4EcP_C1oBOg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qmIHO-6T_KEfPC9jyGDamQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-11-11 08:10:24</td>\n",
       "      <td>0</td>\n",
       "      <td>LNj1OFxy2ool3PZANGchPA</td>\n",
       "      <td>4</td>\n",
       "      <td>YUMMY!!! This place is phenomenal. It is Price...</td>\n",
       "      <td>0</td>\n",
       "      <td>SKV1heo00fdciCbCN9Z33A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pKk7jCFIm96qDdk0laVT2w</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-01-16 20:04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>bZXxa0hO6wQlHD-MkMf4iw</td>\n",
       "      <td>5</td>\n",
       "      <td>The Truffle Macaroni &amp; Cheese and Potatoes Au ...</td>\n",
       "      <td>1</td>\n",
       "      <td>p1r7rZYruZR92x1A649PTQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                 date  funny  \\\n",
       "0  39rLHYJOy2774ZIUouuWLw     0  2017-06-28 21:44:02      0   \n",
       "1  E-Kq1Yu1d6N3TL2qX0aqjA     0  2018-04-09 03:45:19      0   \n",
       "2  nWW6fBfBljiRFa4sG7TyxA     0  2014-07-19 02:00:04      0   \n",
       "3  qmIHO-6T_KEfPC9jyGDamQ     0  2011-11-11 08:10:24      0   \n",
       "4  pKk7jCFIm96qDdk0laVT2w     1  2010-01-16 20:04:00      1   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  ynzOFepQYSCDGdfWDWxiZw      4   \n",
       "1  sQX9ncJBEdBf16AWsvO6Vg      2   \n",
       "2  bVIf2kqbzvif3miNe3ARNw      4   \n",
       "3  LNj1OFxy2ool3PZANGchPA      4   \n",
       "4  bZXxa0hO6wQlHD-MkMf4iw      5   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  Nice to have a diner still around. Food was go...       0   \n",
       "1  Tried this a while back, got the fried chicken...       0   \n",
       "2  I expected more pork selections on menu. Food ...       0   \n",
       "3  YUMMY!!! This place is phenomenal. It is Price...       0   \n",
       "4  The Truffle Macaroni & Cheese and Potatoes Au ...       1   \n",
       "\n",
       "                  user_id  \n",
       "0  Sl6VgFOB-XXfFIAYp7TFkw  \n",
       "1  gcx01pMqWzkni2UC-zoZrA  \n",
       "2  Mn9VzPbrCYU4EcP_C1oBOg  \n",
       "3  SKV1heo00fdciCbCN9Z33A  \n",
       "4  p1r7rZYruZR92x1A649PTQ  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explore the relationship between different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fa07779c340>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(train_df_full['cool'], train_df_full['funny'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2534., 1354., 1888., 2110., 2114.]),\n",
       " array([1. , 1.8, 2.6, 3.4, 4.2, 5. ]),\n",
       " <BarContainer object of 5 artists>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_df_full['stars'], bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, you may use the id feature to aggregate data samples\n",
    "\n",
    "For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 business_id  funny  cool  stars\n",
      "7043  -0qht1roIqleKiQkBLDkbw      1     0      3\n",
      "7363  -0qht1roIqleKiQkBLDkbw      0     0      5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOPUlEQVR4nO3da4hc933G8e8Tyaa5NU6jberqEqmgXpRiB3eruAltlYa2kp1UBPJCSqipSRAudikUWvuVS8mbmlAIaewIYYQbSmIKuamubKdQ0pS6diS1vilBYau49lYGy3HqkqRgZP/6YkbxeDy7c1aemV3//f3AoDnn/OfMw+GvR2fO7hylqpAkvfq9brUDSJImw0KXpEZY6JLUCAtdkhphoUtSI9av1htv2LChtm7dulpvL0mvSidOnHi6quZGbVu1Qt+6dSvHjx9frbeXpFelJP+11DYvuUhSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGjC30JIeTPJXk0SW2J8mnkywkeTjJFZOPKUkap8sZ+h3A7mW27wG29x8HgM++8liSpJUaW+hV9Q3gmWWG7AU+Vz33A5ckuXRSASVJ3Uzim6IbgScGlhf7654cHpjkAL2zeLZs2XLBb7j1pn+44Ne+Wj32l1evdgRpKvz7PDmT+KFoRqwb+d8gVdWhqpqvqvm5uZG3IpAkXaBJFPoisHlgeRNwZgL7lSStwCQK/QhwTf+3Xa4Enq2ql11ukSRN19hr6Em+AOwCNiRZBP4cuAigqg4CR4GrgAXgR8C10worSVra2EKvqv1jthdw/cQSSZIuiN8UlaRGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDWiU6En2Z3kVJKFJDeN2P6WJH+f5KEkJ5NcO/mokqTljC30JOuAW4E9wA5gf5IdQ8OuB75VVZcDu4C/SnLxhLNKkpbR5Qx9J7BQVaer6jngTmDv0JgC3pwkwJuAZ4BzE00qSVpWl0LfCDwxsLzYXzfoM8AvAWeAR4A/rqoXhneU5ECS40mOnz179gIjS5JG6VLoGbGuhpZ/F3gQ+FngXcBnkvzky15Udaiq5qtqfm5uboVRJUnL6VLoi8DmgeVN9M7EB10LfKl6FoDvAr84mYiSpC66FPoxYHuSbf0fdO4DjgyNeRx4P0CStwO/AJyeZFBJ0vLWjxtQVeeS3ADcC6wDDlfVySTX9bcfBD4B3JHkEXqXaG6sqqenmFuSNGRsoQNU1VHg6NC6gwPPzwC/M9lokqSV8JuiktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZ0KvQku5OcSrKQ5KYlxuxK8mCSk0n+ebIxJUnjrB83IMk64Fbgt4FF4FiSI1X1rYExlwC3Abur6vEkPz2lvJKkJXQ5Q98JLFTV6ap6DrgT2Ds05iPAl6rqcYCqemqyMSVJ43Qp9I3AEwPLi/11g34eeGuSryc5keSaSQWUJHUz9pILkBHrasR+fgV4P/B64N+S3F9V33nJjpIDwAGALVu2rDytJGlJXc7QF4HNA8ubgDMjxtxTVT+sqqeBbwCXD++oqg5V1XxVzc/NzV1oZknSCF0K/RiwPcm2JBcD+4AjQ2O+Cvx6kvVJ3gC8G/j2ZKNKkpYz9pJLVZ1LcgNwL7AOOFxVJ5Nc199+sKq+neQe4GHgBeD2qnp0msElSS/V5Ro6VXUUODq07uDQ8ieBT04umiRpJfymqCQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNaJToSfZneRUkoUkNy0z7leTPJ/kw5OLKEnqYmyhJ1kH3ArsAXYA+5PsWGLcLcC9kw4pSRqvyxn6TmChqk5X1XPAncDeEeP+CPgi8NQE80mSOupS6BuBJwaWF/vrfizJRuBDwMHldpTkQJLjSY6fPXt2pVklScvoUugZsa6Glj8F3FhVzy+3o6o6VFXzVTU/NzfXMaIkqYv1HcYsApsHljcBZ4bGzAN3JgHYAFyV5FxVfWUSISVJ43Up9GPA9iTbgP8G9gEfGRxQVdvOP09yB3CXZS5JszW20KvqXJIb6P32yjrgcFWdTHJdf/uy180lSbPR5QydqjoKHB1aN7LIq+oPXnksSdJK+U1RSWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiM6FXqS3UlOJVlIctOI7R9N8nD/cV+SyycfVZK0nLGFnmQdcCuwB9gB7E+yY2jYd4HfrKrLgE8AhyYdVJK0vC5n6DuBhao6XVXPAXcCewcHVNV9VfX9/uL9wKbJxpQkjdOl0DcCTwwsL/bXLeVjwN2jNiQ5kOR4kuNnz57tnlKSNFaXQs+IdTVyYPI+eoV+46jtVXWoquaran5ubq57SknSWOs7jFkENg8sbwLODA9KchlwO7Cnqr43mXiSpK66nKEfA7Yn2ZbkYmAfcGRwQJItwJeA36+q70w+piRpnLFn6FV1LskNwL3AOuBwVZ1Mcl1/+0HgZuBtwG1JAM5V1fz0YkuShnW55EJVHQWODq07OPD848DHJxtNkrQSflNUkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIa0anQk+xOcirJQpKbRmxPkk/3tz+c5IrJR5UkLWdsoSdZB9wK7AF2APuT7BgatgfY3n8cAD474ZySpDG6nKHvBBaq6nRVPQfcCewdGrMX+Fz13A9ckuTSCWeVJC1jfYcxG4EnBpYXgXd3GLMReHJwUJID9M7gAX6Q5NSK0r5oA/D0Bb52mqaWK7e84l285o7ZK2SulTHXCuSWV5TrHUtt6FLoGbGuLmAMVXUIONThPZcPlByvqvlXup9JW6u5YO1mM9fKmGtlXmu5ulxyWQQ2DyxvAs5cwBhJ0hR1KfRjwPYk25JcDOwDjgyNOQJc0/9tlyuBZ6vqyeEdSZKmZ+wll6o6l+QG4F5gHXC4qk4mua6//SBwFLgKWAB+BFw7vcjABC7bTMlazQVrN5u5VsZcK/OaypWql13qliS9CvlNUUlqhIUuSY1YU4We5CeSfDPJQ0lOJvmLEWOWvM3AuFsUTDnXR/t5Hk5yX5LLB7Y9luSRJA8mOT7jXLuSPNt/7weT3DywbTWP158OZHo0yfNJfqq/bSrHa+C91yX5jyR3jdg28/nVMdfM51fHXDOfXx1zrcr8Grfvqc+vqlozD3q/z/6m/vOLgAeAK4fGXAXc3R97JfBAf/064D+BnwMuBh4Cdsww13uAt/af7zmfq7/8GLBhlY7XLuCuEa9d1eM1NP6DwD9N+3gN7P9PgM8vcVxmPr865pr5/OqYa+bzq0uu1Zpf4/Y97fm1ps7Qq+cH/cWL+o/hn9oudZuBLrcomFquqrqvqr7fX7yf3u/iT1XH47WUVT1eQ/YDX5jEe4+TZBNwNXD7EkNmPr+65FqN+dUl1zJW9XgNmdn86mCq82tNFTr8+GPUg8BTwD9W1QNDQ5a6zcBS62eVa9DH6P0rfF4BX0tyIr3bH0xMx1y/1r/8cXeSd/bXrYnjleQNwG7giwOrp3a8gE8Bfwa8sMT2VZlfHXINmtn86phr5vOrY67VmF/j9j3V+bXmCr2qnq+qd9E7A9mZ5JeHhix1m4FOtx+YYq5euOR99P7C3Tiw+r1VdQW9j8rXJ/mNGeb6d+AdVXU58NfAV85HHbW7GeY674PAv1bVMwPrpnK8knwAeKqqTiw3bMS6qc6vjrnOj53Z/OqYa+bzayXHixnOr477nur8WnOFfl5V/Q/wdXr/ug5a6jYDM7n9wDK5SHIZvY+Ae6vqewOvOdP/8yngy/Q+Xs0kV1X97/nLH1V1FLgoyQbWwPHq28fQx+EpHq/3Ar+X5DF6H2l/K8nfDo1ZjfnVJddqzK+xuVZpfnU6Xn2znF9d9j3d+bXSi+7TfABzwCX9568H/gX4wNCYq3npDxW+2V+/HjgNbOPFHyq8c4a5ttD7pux7hta/EXjzwPP7gN0zzPUzvPgFsp3A4/1jt6rHq7/tLcAzwBtncbyG3nsXo3+YN/P51THXzOdXx1wzn19dcq3G/Oqy72nPry53W5ylS4G/Se8/1Xgd8HdVdVc63GaglrhFwQxz3Qy8DbgtCcC56t1N7e3Al/vr1gOfr6p7Zpjrw8AfJjkH/B+wr3ozaLWPF8CHgK9V1Q8HXjvN4zXSGphfXXKtxvzqkms15leXXDD7+TVy37OcX371X5IasWavoUuSVsZCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY34f+gRaWjSoo4gAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for bid, sub_df in train_df_full.groupby('business_id'):\n",
    "    if len(sub_df) > 1:\n",
    "        print(sub_df[['business_id', 'funny', 'cool', 'stars']].head())\n",
    "        plt.hist(sub_df['stars'], bins=5)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     user_id  funny  cool  stars\n",
      "1173  -SjQXQd-IRfOdUdYYwWGOQ      0     1      4\n",
      "4503  -SjQXQd-IRfOdUdYYwWGOQ      0     0      1\n"
     ]
    }
   ],
   "source": [
    "for bid, sub_df in train_df_full.groupby('user_id'):\n",
    "    if len(sub_df) > 1:\n",
    "        print(sub_df[['user_id', 'funny', 'cool', 'stars']].head())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baselines\n",
    "\n",
    "Finally, we come up with two baselines for you to refer.\n",
    "We only use text data here and only consider first 5k training samples.\n",
    "\n",
    "For example, a baseline can be a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, stars] columns from the train split\n",
      "succeed!\n",
      "select [text, stars] columns from the valid split\n",
      "succeed!\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data('train')[:5000]\n",
    "valid_df = load_data('valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split above is what we have done for you. You can use the data as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df['text']\n",
    "y_train = train_df['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tfidf',\n",
      "                 TfidfVectorizer(tokenizer=<function tokenize at 0x7fa08c9590d0>)),\n",
      "                ('lr', LogisticRegression())])\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenize)\n",
    "lr = LogisticRegression()\n",
    "steps = [('tfidf', tfidf),('lr', lr)]\n",
    "pipe = Pipeline(steps)\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(tokenizer=<function tokenize at 0x7fa08c9590d0>)),\n",
       "                ('lr', LogisticRegression())])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.88      0.75       517\n",
      "           2       0.41      0.14      0.21       278\n",
      "           3       0.44      0.47      0.45       344\n",
      "           4       0.50      0.51      0.50       427\n",
      "           5       0.70      0.67      0.68       434\n",
      "\n",
      "    accuracy                           0.58      2000\n",
      "   macro avg       0.54      0.53      0.52      2000\n",
      "weighted avg       0.56      0.58      0.56      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[456  23  21  11   6]\n",
      " [119  38  96  20   5]\n",
      " [ 64  22 160  87  11]\n",
      " [ 22   6  78 217 104]\n",
      " [ 32   3  11  99 289]]\n",
      "accuracy 0.58\n"
     ]
    }
   ],
   "source": [
    "x_valid = valid_df['text']\n",
    "y_valid = valid_df['stars']\n",
    "y_pred = pipe.predict(x_valid)\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(confusion_matrix(y_valid, y_pred))\n",
    "print('accuracy', np.mean(y_valid == y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can use deep learning.\n",
    "Here is a pytorch based baseline using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install torch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_df['text'].map(tokenize).map(filter_stopwords).map(stem)\n",
    "valid_text = valid_df['text'].map(tokenize).map(filter_stopwords).map(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "for tokens in train_text:\n",
    "    for t in tokens:\n",
    "        if not t in word2id:\n",
    "            word2id[t] = len(word2id)\n",
    "word2id['<pad>'] = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_id_seq(texts, padding_length=500):\n",
    "    records = []\n",
    "    for tokens in texts:\n",
    "        record = []\n",
    "        for t in tokens:\n",
    "            record.append(word2id.get(t, len(word2id)))\n",
    "        if len(record) >= padding_length:\n",
    "            records.append(record[:padding_length])\n",
    "        else:\n",
    "            records.append(record + [word2id['<pad>']] * (padding_length - len(record)))\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = texts_to_id_seq(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seqs = texts_to_id_seq(valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seq, y):\n",
    "        assert len(seq) == len(y)\n",
    "        self.seq = seq\n",
    "        self.y = y-1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return np.asarray(self.seq[idx]), self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(MyDataset(train_seqs, y_train), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(MyDataset(valid_seqs, y_valid), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mlp, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(word2id)+1, embedding_dim=64)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=3,\n",
    "                      stride=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=64,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=3,\n",
    "                      stride=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.linear = nn.Linear(64, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = self.cnn(x)\n",
    "        x = torch.max(x, dim=-1)[0]\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlp()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/313 [00:00<00:22, 14.09it/s, loss=0.104, acc=0.219]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:18<00:00, 17.23it/s, loss=0.0946, acc=0.329]\n",
      "100%|██████████| 125/125 [00:01<00:00, 75.48it/s]\n",
      "  1%|          | 2/313 [00:00<00:18, 17.18it/s, loss=0.0935, acc=0.333]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.90      0.51       517\n",
      "           1       0.14      0.01      0.02       278\n",
      "           2       0.24      0.01      0.02       344\n",
      "           3       0.39      0.20      0.26       427\n",
      "           4       0.47      0.47      0.47       434\n",
      "\n",
      "    accuracy                           0.38      2000\n",
      "   macro avg       0.32      0.32      0.26      2000\n",
      "weighted avg       0.34      0.38      0.30      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[464   3   1  10  39]\n",
      " [221   3   5  25  24]\n",
      " [245   7   4  47  41]\n",
      " [207   4   7  84 125]\n",
      " [172   4   0  52 206]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:17<00:00, 17.46it/s, loss=0.0796, acc=0.467]\n",
      "100%|██████████| 125/125 [00:01<00:00, 82.10it/s]\n",
      "  1%|          | 2/313 [00:00<00:18, 16.59it/s, loss=0.0668, acc=0.562]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.69      0.66       517\n",
      "           1       0.32      0.12      0.18       278\n",
      "           2       0.32      0.38      0.35       344\n",
      "           3       0.41      0.39      0.40       427\n",
      "           4       0.53      0.62      0.57       434\n",
      "\n",
      "    accuracy                           0.48      2000\n",
      "   macro avg       0.44      0.44      0.43      2000\n",
      "weighted avg       0.46      0.48      0.46      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[358  28  58  20  53]\n",
      " [ 92  34  90  34  28]\n",
      " [ 54  26 131  98  35]\n",
      " [ 28  11 100 166 122]\n",
      " [ 44   6  28  88 268]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:17<00:00, 17.53it/s, loss=0.0673, acc=0.56] \n",
      "100%|██████████| 125/125 [00:01<00:00, 81.81it/s]\n",
      "  1%|          | 2/313 [00:00<00:18, 16.67it/s, loss=0.0541, acc=0.708]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.59      0.65       517\n",
      "           1       0.29      0.38      0.33       278\n",
      "           2       0.38      0.12      0.18       344\n",
      "           3       0.39      0.50      0.44       427\n",
      "           4       0.52      0.66      0.58       434\n",
      "\n",
      "    accuracy                           0.48      2000\n",
      "   macro avg       0.46      0.45      0.44      2000\n",
      "weighted avg       0.49      0.48      0.46      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[304 111  10  37  55]\n",
      " [ 60 107  26  52  33]\n",
      " [ 29 101  40 137  37]\n",
      " [ 12  38  24 214 139]\n",
      " [ 17  13   6 110 288]]\n",
      "epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:17<00:00, 17.56it/s, loss=0.0556, acc=0.644]\n",
      "100%|██████████| 125/125 [00:01<00:00, 77.18it/s]\n",
      "  1%|          | 2/313 [00:00<00:18, 16.72it/s, loss=0.0409, acc=0.812]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.81      0.68       517\n",
      "           1       0.31      0.17      0.22       278\n",
      "           2       0.42      0.12      0.18       344\n",
      "           3       0.39      0.57      0.47       427\n",
      "           4       0.58      0.56      0.57       434\n",
      "\n",
      "    accuracy                           0.50      2000\n",
      "   macro avg       0.46      0.45      0.42      2000\n",
      "weighted avg       0.48      0.50      0.46      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[420  26  10  32  29]\n",
      " [137  46  23  55  17]\n",
      " [ 75  50  40 152  27]\n",
      " [ 46  19  18 244 100]\n",
      " [ 43   6   4 138 243]]\n",
      "epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:17<00:00, 17.55it/s, loss=0.0435, acc=0.736]\n",
      "100%|██████████| 125/125 [00:01<00:00, 81.56it/s]\n",
      "  1%|          | 2/313 [00:00<00:18, 16.97it/s, loss=0.0295, acc=0.833]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.64      0.67       517\n",
      "           1       0.26      0.15      0.19       278\n",
      "           2       0.37      0.36      0.36       344\n",
      "           3       0.39      0.60      0.47       427\n",
      "           4       0.60      0.51      0.55       434\n",
      "\n",
      "    accuracy                           0.49      2000\n",
      "   macro avg       0.46      0.45      0.45      2000\n",
      "weighted avg       0.49      0.49      0.48      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[332  61  51  41  32]\n",
      " [ 76  42  85  59  16]\n",
      " [ 29  40 124 133  18]\n",
      " [ 12  12  63 255  85]\n",
      " [ 18   8  15 171 222]]\n",
      "epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:18<00:00, 17.07it/s, loss=0.0311, acc=0.825]\n",
      "100%|██████████| 125/125 [00:01<00:00, 73.06it/s]\n",
      "  1%|          | 2/313 [00:00<00:18, 17.13it/s, loss=0.0214, acc=0.875]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.69       517\n",
      "           1       0.25      0.23      0.24       278\n",
      "           2       0.43      0.21      0.28       344\n",
      "           3       0.41      0.47      0.44       427\n",
      "           4       0.55      0.61      0.58       434\n",
      "\n",
      "    accuracy                           0.49      2000\n",
      "   macro avg       0.46      0.45      0.45      2000\n",
      "weighted avg       0.48      0.49      0.48      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[387  61  17  23  29]\n",
      " [117  64  32  42  23]\n",
      " [ 50  81  73 105  35]\n",
      " [ 26  35  37 200 129]\n",
      " [ 32  11  12 114 265]]\n",
      "epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:17<00:00, 17.45it/s, loss=0.0215, acc=0.892]\n",
      "100%|██████████| 125/125 [00:01<00:00, 75.66it/s]\n",
      "  1%|          | 2/313 [00:00<00:17, 17.48it/s, loss=0.0122, acc=0.958]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.63      0.67       517\n",
      "           1       0.29      0.15      0.20       278\n",
      "           2       0.37      0.29      0.32       344\n",
      "           3       0.38      0.52      0.44       427\n",
      "           4       0.49      0.62      0.55       434\n",
      "\n",
      "    accuracy                           0.48      2000\n",
      "   macro avg       0.45      0.44      0.44      2000\n",
      "weighted avg       0.48      0.48      0.47      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[326  47  40  43  61]\n",
      " [ 83  42  63  50  40]\n",
      " [ 31  36  99 132  46]\n",
      " [ 10  11  55 223 128]\n",
      " [ 11   7  14 133 269]]\n",
      "epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:17<00:00, 17.59it/s, loss=0.0127, acc=0.948]\n",
      "100%|██████████| 125/125 [00:01<00:00, 74.28it/s]\n",
      "  1%|          | 2/313 [00:00<00:18, 17.09it/s, loss=0.0071, acc=0.979] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.65      0.66       517\n",
      "           1       0.26      0.28      0.27       278\n",
      "           2       0.34      0.33      0.33       344\n",
      "           3       0.44      0.33      0.37       427\n",
      "           4       0.53      0.65      0.59       434\n",
      "\n",
      "    accuracy                           0.48      2000\n",
      "   macro avg       0.45      0.45      0.44      2000\n",
      "weighted avg       0.47      0.48      0.47      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[337  94  37  11  38]\n",
      " [ 85  78  62  20  33]\n",
      " [ 35  77 113  75  44]\n",
      " [ 24  31  97 140 135]\n",
      " [ 27  22  28  74 283]]\n",
      "epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:17<00:00, 17.61it/s, loss=0.00747, acc=0.976]\n",
      "100%|██████████| 125/125 [00:01<00:00, 74.84it/s]\n",
      "  1%|          | 2/313 [00:00<00:18, 16.54it/s, loss=0.00388, acc=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.71      0.68       517\n",
      "           1       0.25      0.25      0.25       278\n",
      "           2       0.34      0.27      0.30       344\n",
      "           3       0.39      0.51      0.44       427\n",
      "           4       0.60      0.47      0.52       434\n",
      "\n",
      "    accuracy                           0.48      2000\n",
      "   macro avg       0.45      0.44      0.44      2000\n",
      "weighted avg       0.48      0.47      0.47      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[369  76  31  22  19]\n",
      " [ 99  70  52  42  15]\n",
      " [ 42  77  92 114  19]\n",
      " [ 25  31  70 216  85]\n",
      " [ 27  22  25 157 203]]\n",
      "epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:17<00:00, 17.77it/s, loss=0.00426, acc=0.991]\n",
      "100%|██████████| 125/125 [00:01<00:00, 78.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.70      0.68       517\n",
      "           1       0.28      0.14      0.18       278\n",
      "           2       0.35      0.38      0.36       344\n",
      "           3       0.42      0.35      0.38       427\n",
      "           4       0.49      0.68      0.57       434\n",
      "\n",
      "    accuracy                           0.49      2000\n",
      "   macro avg       0.44      0.45      0.44      2000\n",
      "weighted avg       0.47      0.49      0.47      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[360  38  47  17  55]\n",
      " [ 96  38  73  26  45]\n",
      " [ 37  42 129  84  52]\n",
      " [ 23  11  86 150 157]\n",
      " [ 22   9  29  77 297]]\n"
     ]
    }
   ],
   "source": [
    "for e in range(1, 11):    \n",
    "    print('epoch', e)\n",
    "    model.train()\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    with tqdm.tqdm(train_loader) as t:\n",
    "        for x, y in t:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            total_acc += (logits.argmax(1) == y).sum().item()\n",
    "            total_count += y.size(0)\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            t.set_postfix({'loss': total_loss/total_count, 'acc': total_acc/total_count})\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with tqdm.tqdm(valid_loader) as t:\n",
    "        for x, y in t:\n",
    "            logits = model(x)\n",
    "            total_acc += (logits.argmax(1) == y).sum().item()\n",
    "            total_count += len(y)\n",
    "            y_pred += logits.argmax(1).tolist()\n",
    "            y_true += y.tolist()\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"\\n\\n\")\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning are full of tricks. \n",
    "\n",
    "In the second example above, the implementation of CNN is not good enough to beat even TFIDF+Logistic regression.\n",
    "\n",
    "You can use all the techniques introduced in the lectures and tutorials to enhance your methods.\n",
    "\n",
    "Of course, you can use ideas have not been mentioned to make your model distinguished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
