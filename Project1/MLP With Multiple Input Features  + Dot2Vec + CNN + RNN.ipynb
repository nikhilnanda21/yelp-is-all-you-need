{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tqdm\n",
    "\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout,SpatialDropout1D, GRU, SimpleRNN\n",
    "from keras.utils import to_categorical\n",
    "from keras import layers\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "from keras.layers import MaxPool1D, MaxPooling1D, Conv1D\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import itertools\n",
    "\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [f, u, l, l] columns from the train split\n",
      "Failed, then try to \n",
      "select all columns from the train split\n",
      "select [f, u, l, l] columns from the valid split\n",
      "Failed, then try to \n",
      "select all columns from the valid split\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "def load_data(split_name='train', columns=['text', 'stars']):\n",
    "    try:\n",
    "        print(f\"select [{', '.join(columns)}] columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
    "        df = df.loc[:,columns]\n",
    "        print(\"succeed!\")\n",
    "        return df\n",
    "    except:\n",
    "        print(\"Failed, then try to \")\n",
    "        print(f\"select all columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
    "        return df\n",
    "    \n",
    "#train_df = load_data('train', columns=['text'])\n",
    "train_df = load_data('train', columns='full')\n",
    "valid_df = load_data('valid', columns='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       2\n",
      "       ..\n",
      "9995    0\n",
      "9996    2\n",
      "9997    0\n",
      "9998    0\n",
      "9999    1\n",
      "Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df.head()\n",
    "print(train_df[\"cool\"]+train_df[\"funny\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing sentence data\n",
      "processing helpful data\n",
      "processing adjective data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tokens :  [[13, 1330, 1, 2, 4436, 41, 2, 195, 158], [80, 8, 145, 20, 153, 1273, 28, 58, 1585, 2008, 474, 747, 20, 153, 154, 650, 1980, 2878, 4232, 20, 573, 31, 40, 119, 76, 153, 86, 205, 40, 2, 32, 153, 646, 766, 198, 6, 4233, 15], [447, 222, 1384, 33, 1, 2, 166, 199, 2, 222, 1189, 1227, 30, 11, 5982, 2, 5328, 131, 467, 49], [560, 3, 1784, 549, 109, 7518, 3941, 3, 1, 7, 92, 413, 302, 92, 51, 497, 478], [1242, 1981, 62, 421, 1869, 6403, 373, 67, 3583, 2009, 485, 71]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nX_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding=\"post\")\\n\\n\\nencoder = layers.experimental.preprocessing.TextVectorization(\\n    max_tokens=vocab_size)\\n\\nX_train_vec = pd.Series([\" \".join(v) for v in X_train]).astype(str)\\n\\nX_train_dataset = pd.DataFrame(data={\\n    \\'text\\': pd.Series([\" \".join(v) for v in X_train]).astype(str),\\n    \\'label\\': train_df[\"stars\"]\\n})\\n\\n\\n\\n\\n#X_train_dataset = tf.data.Dataset.from_tensor_slices([(line[0],line[1]) for line in X_train_dataset])\\nX_train_dataset = tf.data.Dataset.from_tensor_slices((pd.Series([\" \".join(v) for v in X_train]).astype(str),\\n                                                      to_categorical(train_df[\"stars\"].values)))\\nprint(\"X_train_dataset \",X_train_dataset.element_spec)\\nX_test_vec = pd.Series([\" \".join(v) for v in X_test]).astype(str)\\nencoder.adapt(X_train_dataset.map(lambda text, label: text))\\n#encoder.adapt(X_test_vec.map(lambda text, label: text))\\nX_test_dataset =tf.data.Dataset.from_tensor_slices((pd.Series([\" \".join(v) for v in X_test]).astype(str),\\n                                                      to_categorical(valid_df[\"stars\"].values)))\\n\\nBUFFER_SIZE = 10000\\nBATCH_SIZE = 64\\nX_train_dataset = X_train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\\nX_test_dataset = X_test_dataset.batch(BATCH_SIZE)\\n\\n\\nprint(\"X_test_dataset \",X_test_dataset.element_spec)\\n'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-processing data\n",
    "def nor_ds(dataset):    \n",
    "    text = [re.sub(\"[^A-Za-z]+\", ' ', str(line)).lower() for line in dataset]\n",
    "    tokens = []    \n",
    "    disable_list = [\n",
    "                    \"tagger\",\n",
    "                    \"parser\",\n",
    "                    \"ner\",\n",
    "                    \"entity_linker\",\n",
    "                    \"entity_ruler\",\n",
    "                    \"textcat\",\n",
    "                    \"textcat_multilabel\",\n",
    "                    \"lemmatizer\",\n",
    "                    \"morphologizer\",\n",
    "                    \"sentencizer\",                \n",
    "                   ]\n",
    "    for doc in nlp.pipe(text, n_process=4, batch_size=2000, disable=disable_list):        \n",
    "        line_tokens = [token.text  for token in doc if not token.is_stop and len(token.text) > 1 ]\n",
    "        if len(line_tokens) > 0:\n",
    "            tokens.append(line_tokens)\n",
    "        else:\n",
    "            tokens.append([\"neutral\"])\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def adj_ds(dataset):    \n",
    "    text = [re.sub(\"[^A-Za-z]+\", ' ', str(line)).lower() for line in dataset]\n",
    "    tokens = []    \n",
    "    disable_list = [\n",
    "                    \n",
    "                    \"ner\",\n",
    "                    \"entity_linker\",\n",
    "                    \"entity_ruler\",\n",
    "                    \"textcat\",\n",
    "                    \"textcat_multilabel\",\n",
    "                    \"lemmatizer\",\n",
    "                    \"morphologizer\",\n",
    "                    \"sentencizer\",                \n",
    "                   ]\n",
    "    for doc in nlp.pipe(text, n_process=4, batch_size=2000, disable=disable_list):        \n",
    "        line_tokens = [token.text  for token in doc if not token.is_stop and len(token.text) > 1 and token.pos_ in [\"ADJ\"]]\n",
    "        if len(line_tokens) > 0:\n",
    "            tokens.append(line_tokens)\n",
    "        else:\n",
    "            tokens.append([\"neutral\"])\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# complete sentence layer\n",
    "print(\"processing sentence data\")\n",
    "X_train = nor_ds(train_df[\"text\"].values)\n",
    "X_test  = nor_ds(valid_df[\"text\"].values)\n",
    "print(\"processing helpful data\")\n",
    "X_train_helpful = train_df[\"cool\"] + train_df[\"funny\"] + train_df[\"useful\"]\n",
    "X_test_helpful = valid_df[\"cool\"] + valid_df[\"funny\"] + valid_df[\"useful\"]\n",
    "print(\"processing adjective data\")\n",
    "X_train_adj = adj_ds(train_df[\"text\"].values)\n",
    "X_test_adj = adj_ds(valid_df[\"text\"].values)\n",
    "\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "print(X_train_adj[:10])\n",
    "print(X_test_adj[:10])\n",
    "\n",
    "tokenizer = Tokenizer(split=' ')\n",
    "total_reviews =  [\" \".join(v) for v in X_train] + [\" \".join(v) for v in X_test]\n",
    "print(type(total_reviews),total_reviews[:3])\n",
    "\n",
    "#tokenizer.fit_on_texts(total_reviews)\n",
    "tokenizer.fit_on_texts(total_reviews)\n",
    "\n",
    "\n",
    "#print(\"tokenizer_word_index\",tokenizer.word_index)\n",
    "\n",
    "max_length = max([len(s.split()) for s in total_reviews])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# X_train = tokenizer.texts_to_sequences(train_df['text'].values)\n",
    "# X_train = pad_sequences(X_train)\n",
    "\n",
    "X_train_tokens = tokenizer.texts_to_sequences(total_reviews)\n",
    "\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)\n",
    "print(\"X_train_tokens : \",X_train_tokens[:5])\n",
    "\n",
    "#X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding=\"post\")\n",
    "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length)\n",
    "#X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding=\"post\")\n",
    "\n",
    "'''\n",
    "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding=\"post\")\n",
    "\n",
    "\n",
    "encoder = layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=vocab_size)\n",
    "\n",
    "X_train_vec = pd.Series([\" \".join(v) for v in X_train]).astype(str)\n",
    "\n",
    "X_train_dataset = pd.DataFrame(data={\n",
    "    'text': pd.Series([\" \".join(v) for v in X_train]).astype(str),\n",
    "    'label': train_df[\"stars\"]\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#X_train_dataset = tf.data.Dataset.from_tensor_slices([(line[0],line[1]) for line in X_train_dataset])\n",
    "X_train_dataset = tf.data.Dataset.from_tensor_slices((pd.Series([\" \".join(v) for v in X_train]).astype(str),\n",
    "                                                      to_categorical(train_df[\"stars\"].values)))\n",
    "print(\"X_train_dataset \",X_train_dataset.element_spec)\n",
    "X_test_vec = pd.Series([\" \".join(v) for v in X_test]).astype(str)\n",
    "encoder.adapt(X_train_dataset.map(lambda text, label: text))\n",
    "#encoder.adapt(X_test_vec.map(lambda text, label: text))\n",
    "X_test_dataset =tf.data.Dataset.from_tensor_slices((pd.Series([\" \".join(v) for v in X_test]).astype(str),\n",
    "                                                      to_categorical(valid_df[\"stars\"].values)))\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "X_train_dataset = X_train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "X_test_dataset = X_test_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "print(\"X_test_dataset \",X_test_dataset.element_spec)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nice', 'good', 'good'], ['meh', 'small', 'special', 'better', 'good'], ['good', 'good', 'best', 'schnitzel', 'good', 'busy'], ['phenomenal', 'pricey', 'great', 'attentive'], ['truffle', 'short', 'amazing']]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_adj[:5])\n",
    "print(X_train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    3    1    1]\n",
      " [   0    0    0 ...   32    8    1]\n",
      " [   0    0    0 ... 1466    1   21]\n",
      " ...\n",
      " [   0    0    0 ...  425   71    6]\n",
      " [   0    0    0 ...   41    2   14]\n",
      " [   0    0    0 ...   30    3    1]]\n"
     ]
    }
   ],
   "source": [
    "adj_tokenizer = Tokenizer(split=' ')\n",
    "adj_lines = [\" \".join(line) for line in X_train_adj] + [\" \".join(line) for line in X_test_adj]\n",
    "adj_tokenizer.fit_on_texts(adj_lines)\n",
    "X_train_adj_tokens = adj_tokenizer.texts_to_sequences(adj_lines)\n",
    "\n",
    "#X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding=\"post\")\n",
    "X_train_adj_pad = pad_sequences(X_train_adj_tokens, maxlen=max_length)\n",
    "print(X_train_adj_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:11:28: collecting all words and their counts\n",
      "INFO - 14:11:28: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 14:11:28: collected 392456 token types (unigram + bigrams) from a corpus of 531752 words and 10000 sentences\n",
      "INFO - 14:11:28: merged Phrases<392456 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 14:11:28: Phrases lifecycle event {'msg': 'built Phrases<392456 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.62s', 'datetime': '2021-03-31T14:11:28.684038', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO - 14:11:28: exporting phrases from Phrases<392456 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 14:11:29: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<149 phrases, min_count=30, threshold=10.0> from Phrases<392456 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.82s', 'datetime': '2021-03-31T14:11:29.527469', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO - 14:11:30: using concatenative 352-dimensional layer1\n",
      "INFO - 14:11:30: Doc2Vec lifecycle event {'params': 'Doc2Vec(dm/c,d32,n5,w5,mc2,s0.001,t4)', 'datetime': '2021-03-31T14:11:30.140811', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nice', 'diner', 'food', 'good', 'comforting', 'definitely', 'good', 'spot', 'breakfast'], ['tried', 'got', 'fried_chicken', 'sandwich', 'meh', 'pretty', 'small', 'lacked', 'spices', 'flavors', 'expecting', 'chicken', 'sandwich', 'special', 'opinion', 'mary', 'browns', 'kfc', 'chicken', 'sandwiches', 'better', 'took', 'super', 'long', 'sandwich', 'store', 'pm', 'took', 'good', 'minutes', 'sandwich', 'combo', 'cashier', 'phone', 'time', 'updating', 'order'], ['expected', 'pork', 'selections', 'menu', 'food', 'good', 'beer_selection', 'good', 'pork_belly', 'app', 'best', 've', 'schnitzel', 'good', 'oddly', 'busy', 'friday_night'], ['yummy', 'place', 'phenomenal', 'pricey', 'feel', 'urge', 'splurge', 'place', 'food', 'great', 'server', 'attentive', 'yes', 'server', 'table', 'gets', 'servers'], ['truffle', 'macaroni', 'cheese', 'potatoes', 'au', 'gratin', 'short', 'amazing', 'death', 'row', 'choose', 'meal'], ['time', 'winking', 'lizard', 'area', 'enjoyed', 'bavarian', 'pretzels', 'sauces', 'enjoyed', 'pulled', 'chicken', 'melt', 'onion_rings', 'husband', 'special', 'brisket', 'potato', 'pancakes', 'pleased', 'fun', 'place', 'eat', 'traveling'], ['husbands', 'come', 'time', 'pick', 'gift', 'card', 'order', 'honey', 'chicken', 'meat', 'shrimp', 'fried_rice', 'appetizers', 'visit', 'crab', 'wonton', 'pork', 'egg', 'rolls', 'eat', 'honestly', 'offered', 'soup', 'guest', 'little', 'larger', 'portion_size', 'baffles', 'pay', 'plate', 'overloaded', 'rice', 'noodles', 'server', 'polite', 'complemented', 'son', 'mannerism', 'behavior', 'lovely', 'hear', 'cocktail', 'hold', 'bar', 'cocktail', 'lounge', 'suggest', 'drinks', 'suited', 'sake', 'sangria', 'tequila', 'margaritas', 'll', 'enjoy', 'eat', 'tejas', 'suggestion', 'pf', 'having', 'signature', 'yes', 'lettuce', 'wraps', 'fan', 'fav', 'like', 'signature', 'sake', 'cocktail', 'try', 'green', 'tea', 'dessert', 'sayin'], ['great', 'salad', 'bar', 'selection', 'die', 'garlic', 'cheese', 'biscuits', 'great', 'baked', 'potatoes', 'mixed', 'vegetable', 'platter', 'grilled', 'salmon', 'little', 'cooked', 'graciously', 'taken', 'replaced', 'second', 'try', 'good', 'flavor', 'scale', 'waitress', 'tae', 'delight', 'excellent', 'server', 'conscientious', 'excellent', 'meal', 'recommend', 'coming', 'nice', 'atmosphere', 'right', 'street', 'hampton', 'inn', 'stayed'], ['antique', 'sugar', 'cute', 'leads', 'believe', 'owners', 'anna', 'badass', 'lady', 'awesome', 'store', 'best', 'vintage', 'boutique', 'phoenix', 'definitely_worth', 'checking'], ['girlfriend', 'driving', 'home', 'mall', 'sunday', 'sudden', 'decided', 'needed', 'martinis', 'sure', 'ended', 'places', 'place', 've_eaten', 'delicious', 'decadent', 'brunch', 'mimosa', 'far', 'drinks', 'concerned', 'place', 'ended', 'sat_bar', 'pm', 'ordered', 'dirty', 'martinis', 'told', 'closed', 'pm', 'ok', 'martinis', 'perfect', 'good', 'sized', 'yummy', 'huge', 'olives', 'short', 'looking', 'got', 'checks', 'blown_away', 'craving', 'dirty', 'martini', 'early', 'sunday', 'afternoon', 'shopping', 'delicious', 'classic', 'cheap', 'loved', 'bravo', 'terrace', 'cafe'], ['went', 'girls', 'dinner', 'recently', 'chill', 'tasty', 'highlights', 'lighter', 'smaller', 'plates', 'great', 'sharing', 'beet', 'salad', 'manchego', 'cheese', 'crustini', 'paced', 'things', 'server', 'kind', 'bit', 'slow', 'start', 'ginger', 'beer', 'wine', 'list', 'great', 'creative', 'funky', 'cool', 'grown', 'slightly', 'laid', 'vibe', 'lighting', 'warm', 'romantic', 'wine', 'lining', 'walls', 'older', 'crowd', 'good', 'place', 'taking', 'parents', 'colleagues', 'worked', 'girl', 'chats', 'crazy', 'rush', 'nice', 'place', 'roster', 'fancy', 'wine', 'oriented', 'meal'], ['mad', 'spending', 'time', 'money', 'friends', 'went', 'vegas', 'days', 'aug', 'sept', 'neat', 'little', 'ticket', 'office', 'outside', 'csi', 'experience', 'buy', 'events', 'thought', 'csi', 'experience', 'cool', 'turned', 'laughable', 'crappiest', 'low', 'point', 'trip', 'assigned', 'crime', 'solve', 'discovered', 'solve', 'use', 'brain', 'whatsoever', 'assume', 'idiot', 'basically', 'tell', 'obvious', 'way', 'step', 'awesome', 'probably', 'yr', 'old', 'don', 'wet', 'bed', 'anymore', 'assenine', 'experience', 'leave', 'mad', 'spent', 'money', 'personally', 'vowing', 'review', 'kill', 'craptastic', 'un', 'attraction', 'ground'], ['getting', 'habit', 'giving', 'stars', 'think', 'getting', 'better', 'finding', 'good', 'places', 'went', 'lemongrass', 'lunch', 'mainly', 'hotel', 'aria', 'wanted', 'asian', 'cuisine', 'service', 'gracious', 'kind', 'server', 'passed', 'pleasant', 'eager', 'peach', 'ginger', 'cocktail', 'refreshing', 'creative', 'moved', 'potstickers', 'potsticker', 'lover', 'lookout', 'large', 'nice', 'dough', 'steamed', 'fried', 'perfection', 'nice', 'spicy', 'sauce', 'moved', 'sesame', 'chicken', 'know', 'chinese', 'real', 'original', 'delicious', 'sweet', 'chili', 'sauce', 'wasn', 'cloying', 'sweet', 'ambiance', 'beautiful', 'serene', 'oasis', 'tucked', 'away', 'casino', 'nice', 'romantic', 'spot', 'enjoyable'], ['better', 'bbq', 'better', 'service', 'way', 'money', 'lucille', 'excited_try', 'place', 'parents', 'talked', 'maybe', 'went', 'high_expectations', 'hostess', 'misplaced', 'brain', 'cells', 'asked', 'party', 'times', 'numbers', 'table', 'ready', 'water', 'ok', 'super', 'slow', 'came', 'checks', 'actually', 'checking', 'liked', 'biscuits', 'amazing', 'nice', 'touch', 'waiting', 'long', 'table', 'party', 'amazed', 'meals', 'disappointing', 'mac_cheese', 'sounded', 'good', 'came', 'dry', 'mushy', 'gross', 'turn', 'mac_cheese', 'couple', 'weeks', 'food', 'extremely', 'loud', 'restaurant', 'consider', 'second', 'shot'], ['absolutely', 'horrible', 'company', 'rude', 'general', 'manager', 'customer_service', 'paid', 'look', 'diagnose', 'washing', 'machine', 'didn', 'work', 'day', 'refuse', 'come', 'issue', 'till', 'pay', 'originally', 'costs', 'manager', 'rude', 'mother', 'phone', 'said', 'wasn', 'technician', 'wouldn', 'know', 'costs', 'needs', 'big', 'scam'], ['girlfriends', 'went', 'summerlious', 'appetizer', 'ok', 'pate', 'oyster', 'entree', 'got', 'steak', 'frites', 'fries', 'overly', 'salted', 'steak', 'disappointment', 'asked', 'mediums', 'rare', 'waiter', 'brought', 'mediums', 'sit', 'wait', 'steak', 'friends', 'eating', 'theirs', 'mention', 'medium_rare', 'send', 'dessert', 'good', 'creme', 'brul', 'smooth', 'eggs', 'cooked', 'process', 'service', 'terrible', 'waiters', 'hardly', 'came', 'ask', 'food', 'okay', 'terrible'], ['food', 'bland', 'ordered', 'breakfast', 'burritos', 'steak', 'chorizo', 'blackened', 'chicken', 'nachos', 'burritos', 'came', 'ratio', 'potato', 'steak', 'eggs', 'don_think', 'ratios', 'proportionate', 'taste', 'different', 'adding', 'sauce', 'meal', 'appetizing', 'blackened', 'chicken', 'nachos', 'decent', 'fresh', 'guac', 'gave', 'stars', 'solely', 'guacamole', 'water', 'flavor', 'breakfast', 'burritos'], ['come', 'years_ago', 'different', 'place', 'came', 'early', 'mothers', 'day', 'dinner', 'seated', 'right_away', 'crowded', 'table', 'placed', 'clean', 'food', 'crumbs', 'waiter', 'nice', 'guy', 'took', 'min', 'food', 'stopped', 'table', 'tell', 'right', 'minutes_later', 'waiting', 'bread', 'basket', 'offers', 'comes', 'food', 'son', 'ordered', 'salmon', 'literally', 'smallest', 'piece', 'salmon', 've_seen', 'maybe', 'inches', 'long', 'stared', 'shock', 'yes', 'tasted', 'good', 'ridiculously', 'small', 'ordered', 'reuben', 'sandwich', 'pretty_good', 'special', 'came', 'handful', 'stale', 'chips', 'husband', 'ordered', 'ahi', 'said', 'okay', 'tiny', 'portion', 'certainly', 'dinner', 'sized', 'portion', 'saw', 'server', 'times', 'order', 'tell', 'min', 'waiting', 'food', 'right', 'bring', 'food', 'drop', 'check', 'nice', 'fellow', 'horrible', 'service', 'way', 'priced', 'sadly', 'come'], ['lissie', 'tried', 'contact', 'risk', 'management', 'left', 'messages', 'received', 'return', 'calls', 'nt', 'think', 'want', 'address', 'issue'], ['guy', 'joshua', 'schorr', 'totally', 'soulless', 'inexperienced', 'interested', 'money', 'time', 'better', 'going', 'healer']]\n",
      "create taggedDocument\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:11:30: collecting all words and their counts\n",
      "INFO - 14:11:30: PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO - 14:11:31: collected 26911 word types and 10000 unique tags from a corpus of 10000 examples and 520603 words\n",
      "INFO - 14:11:31: Creating a fresh vocabulary\n",
      "INFO - 14:11:31: Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 15697 unique words (58.32930771803352%% of original 26911, drops 11214)', 'datetime': '2021-03-31T14:11:31.120258', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 14:11:31: Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 509389 word corpus (97.84595939708376%% of original 520603, drops 11214)', 'datetime': '2021-03-31T14:11:31.120921', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:11:31: deleting the raw counts dictionary of 26911 items\n",
      "INFO - 14:11:31: sample=0.001 downsamples 26 most-common words\n",
      "INFO - 14:11:31: Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 489394.82028633525 word corpus (96.1%% of prior 509389)', 'datetime': '2021-03-31T14:11:31.221647', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 14:11:31: estimated required memory for 15697 words and 32 dimensions: 35239092 bytes\n",
      "INFO - 14:11:31: resetting layer weights\n",
      "INFO - 14:11:31: Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 15698 vocabulary and 352 features, using sg=0 hs=0 sample=0.001 negative=5 window=5', 'datetime': '2021-03-31T14:11:31.394190', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:11:31: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:31: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:31: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:31: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:31: EPOCH - 1 : training on 520603 raw words (499239 effective words) took 0.4s, 1272510 effective words/s\n",
      "INFO - 14:11:32: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:32: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:32: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:32: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:32: EPOCH - 2 : training on 520603 raw words (499402 effective words) took 0.4s, 1244814 effective words/s\n",
      "INFO - 14:11:32: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:32: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:32: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:32: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:32: EPOCH - 3 : training on 520603 raw words (499474 effective words) took 0.4s, 1291661 effective words/s\n",
      "INFO - 14:11:32: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:32: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:32: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:32: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:32: EPOCH - 4 : training on 520603 raw words (499467 effective words) took 0.4s, 1268344 effective words/s\n",
      "INFO - 14:11:33: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:33: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:33: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:33: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:33: EPOCH - 5 : training on 520603 raw words (499360 effective words) took 0.4s, 1294257 effective words/s\n",
      "INFO - 14:11:33: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:33: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:33: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:33: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:33: EPOCH - 6 : training on 520603 raw words (499461 effective words) took 0.4s, 1287423 effective words/s\n",
      "INFO - 14:11:34: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:34: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:34: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:34: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:34: EPOCH - 7 : training on 520603 raw words (499503 effective words) took 0.4s, 1301300 effective words/s\n",
      "INFO - 14:11:34: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:34: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:34: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:34: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:34: EPOCH - 8 : training on 520603 raw words (499347 effective words) took 0.4s, 1328659 effective words/s\n",
      "INFO - 14:11:34: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:34: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:34: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:34: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:34: EPOCH - 9 : training on 520603 raw words (499537 effective words) took 0.4s, 1271696 effective words/s\n",
      "INFO - 14:11:35: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:35: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:35: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:35: EPOCH - 10 : training on 520603 raw words (499333 effective words) took 0.4s, 1287857 effective words/s\n",
      "INFO - 14:11:35: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:35: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:35: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:35: EPOCH - 11 : training on 520603 raw words (499373 effective words) took 0.4s, 1325077 effective words/s\n",
      "INFO - 14:11:36: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:36: EPOCH - 12 : training on 520603 raw words (499459 effective words) took 0.4s, 1353314 effective words/s\n",
      "INFO - 14:11:36: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:36: EPOCH - 13 : training on 520603 raw words (499442 effective words) took 0.5s, 964449 effective words/s\n",
      "INFO - 14:11:37: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:37: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:37: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:37: EPOCH - 14 : training on 520603 raw words (499382 effective words) took 0.4s, 1124879 effective words/s\n",
      "INFO - 14:11:37: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:37: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:37: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:37: EPOCH - 15 : training on 520603 raw words (499467 effective words) took 0.4s, 1251054 effective words/s\n",
      "INFO - 14:11:37: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:37: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:37: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:37: EPOCH - 16 : training on 520603 raw words (499452 effective words) took 0.4s, 1226959 effective words/s\n",
      "INFO - 14:11:38: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:38: EPOCH - 17 : training on 520603 raw words (499373 effective words) took 0.4s, 1265709 effective words/s\n",
      "INFO - 14:11:38: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:38: EPOCH - 18 : training on 520603 raw words (499354 effective words) took 0.4s, 1202632 effective words/s\n",
      "INFO - 14:11:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:39: EPOCH - 19 : training on 520603 raw words (499477 effective words) took 0.4s, 1257877 effective words/s\n",
      "INFO - 14:11:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:39: worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:11:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:39: EPOCH - 20 : training on 520603 raw words (499502 effective words) took 0.4s, 1290527 effective words/s\n",
      "INFO - 14:11:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:39: EPOCH - 21 : training on 520603 raw words (499483 effective words) took 0.4s, 1341822 effective words/s\n",
      "INFO - 14:11:40: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:40: EPOCH - 22 : training on 520603 raw words (499375 effective words) took 0.4s, 1350654 effective words/s\n",
      "INFO - 14:11:40: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:40: EPOCH - 23 : training on 520603 raw words (499261 effective words) took 0.4s, 1204232 effective words/s\n",
      "INFO - 14:11:41: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:41: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:41: EPOCH - 24 : training on 520603 raw words (499388 effective words) took 0.4s, 1345135 effective words/s\n",
      "INFO - 14:11:41: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:41: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:41: EPOCH - 25 : training on 520603 raw words (499361 effective words) took 0.4s, 1329123 effective words/s\n",
      "INFO - 14:11:41: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:41: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:41: EPOCH - 26 : training on 520603 raw words (499193 effective words) took 0.4s, 1119162 effective words/s\n",
      "INFO - 14:11:42: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:42: EPOCH - 27 : training on 520603 raw words (499408 effective words) took 0.4s, 1204629 effective words/s\n",
      "INFO - 14:11:42: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:42: EPOCH - 28 : training on 520603 raw words (499420 effective words) took 0.4s, 1208470 effective words/s\n",
      "INFO - 14:11:43: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:43: EPOCH - 29 : training on 520603 raw words (499563 effective words) took 0.4s, 1370811 effective words/s\n",
      "INFO - 14:11:43: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:43: EPOCH - 30 : training on 520603 raw words (499429 effective words) took 0.4s, 1378881 effective words/s\n",
      "INFO - 14:11:43: Doc2Vec lifecycle event {'msg': 'training on 15618090 raw words (14982285 effective words) took 12.0s, 1246926 effective words/s', 'datetime': '2021-03-31T14:11:43.410443', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'train'}\n",
      "/Library/Python/3.7/site-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "WARNING - 14:11:43: destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n",
      "INFO - 14:11:43: storing 15698x32 projection weights into word2vec_model.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 15698\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-3.68459535  1.37444925  0.6841166  ...  1.48991668 -1.68408406\n",
      "  -0.36244282]\n",
      " [ 1.09229255 -0.22801112 -0.3626335  ... -0.74420321 -3.32613134\n",
      "  -4.09105444]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# train dot2vec model\n",
    "\n",
    "embedding_size=32\n",
    "\n",
    "\n",
    "phrases = Phrases(X_train, min_count=30, progress_per=10000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[X_train]\n",
    "\n",
    "all_sentences = [v for v in sentences]\n",
    "\n",
    "print(all_sentences[:20])\n",
    "                     \n",
    "# model_w2v = Word2Vec(vector_size=embedding_size, window=5, min_count=3, workers=4,                                        \n",
    "#                     sample=6e-5, \n",
    "#                      alpha=0.03, \n",
    "#                      min_alpha=0.0007, \n",
    "#                      negative=20,)\n",
    "\n",
    "\n",
    "model_w2v = Doc2Vec(dm=1, dm_concat=1, vector_size=embedding_size, window=5, negative=5, min_count=2, workers=4, alpha=0.065, min_alpha=0.065)\n",
    "print(\"create taggedDocument\")\n",
    "document = [TaggedDocument(doc, [i]) for i, doc in enumerate(all_sentences)]\n",
    "print(\"build vocab\")\n",
    "model_w2v.build_vocab(document, progress_per=10000)\n",
    "print(\"training vocab\")\n",
    "model_w2v.train(document, total_examples=model_w2v.corpus_count, epochs=30, report_delay=1)\n",
    "model_w2v.init_sims(replace=True)\n",
    "\n",
    "#model_w2v = Word2Vec(sentences=common_texts, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
    "model_w2v.wv.save_word2vec_format(\"word2vec_model.txt\",binary=False)\n",
    "\n",
    "print(\"vocab size :\",model_w2v.wv.vectors.shape[0])\n",
    "\n",
    "# read from word2vec model\n",
    "embeddings_index ={}\n",
    "f = open(os.path.join(\"\", \"word2vec_model.txt\"), encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in model_w2v.wv:\n",
    "        embedding_matrix[i] = model_w2v.wv.get_vector(word)\n",
    "\n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:11:51: collecting all words and their counts\n",
      "INFO - 14:11:51: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 14:11:51: collected 59397 token types (unigram + bigrams) from a corpus of 94198 words and 10000 sentences\n",
      "INFO - 14:11:51: merged Phrases<59397 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 14:11:51: Phrases lifecycle event {'msg': 'built Phrases<59397 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.11s', 'datetime': '2021-03-31T14:11:51.383582', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO - 14:11:51: exporting phrases from Phrases<59397 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 14:11:51: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<0 phrases, min_count=30, threshold=10.0> from Phrases<59397 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.12s', 'datetime': '2021-03-31T14:11:51.506924', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO - 14:11:51: using concatenative 352-dimensional layer1\n",
      "INFO - 14:11:51: Doc2Vec lifecycle event {'params': 'Doc2Vec(dm/c,d32,n5,w5,mc2,s0.001,t4)', 'datetime': '2021-03-31T14:11:51.605610', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO - 14:11:51: collecting all words and their counts\n",
      "INFO - 14:11:51: PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO - 14:11:51: collected 5404 word types and 10000 unique tags from a corpus of 10000 examples and 94198 words\n",
      "INFO - 14:11:51: Creating a fresh vocabulary\n",
      "INFO - 14:11:51: Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 2808 unique words (51.961509992598074%% of original 5404, drops 2596)', 'datetime': '2021-03-31T14:11:51.651620', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 14:11:51: Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 91602 word corpus (97.24410284719421%% of original 94198, drops 2596)', 'datetime': '2021-03-31T14:11:51.652328', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 14:11:51: deleting the raw counts dictionary of 5404 items\n",
      "INFO - 14:11:51: sample=0.001 downsamples 74 most-common words\n",
      "INFO - 14:11:51: Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 67751.07137822936 word corpus (74.0%% of prior 91602)', 'datetime': '2021-03-31T14:11:51.672441', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 14:11:51: estimated required memory for 2808 words and 32 dimensions: 8997088 bytes\n",
      "INFO - 14:11:51: resetting layer weights\n",
      "INFO - 14:11:51: Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2809 vocabulary and 352 features, using sg=0 hs=0 sample=0.001 negative=5 window=5', 'datetime': '2021-03-31T14:11:51.712162', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nice', 'good', 'good'], ['meh', 'small', 'special', 'better', 'good'], ['good', 'good', 'best', 'schnitzel', 'good', 'busy'], ['phenomenal', 'pricey', 'great', 'attentive'], ['truffle', 'short', 'amazing'], ['bavarian', 'pleased', 'fun'], ['little', 'larger', 'polite', 'lovely', 'green'], ['great', 'garlic', 'great', 'baked', 'mixed', 'little', 'cooked', 'second', 'good', 'excellent', 'conscientious', 'excellent', 'nice'], ['antique', 'cute', 'awesome', 'best', 'vintage', 'worth'], ['sure', 'delicious', 'decadent', 'concerned', 'dirty', 'perfect', 'good', 'sized', 'yummy', 'huge', 'short', 'dirty', 'delicious', 'classic', 'cheap'], ['tasty', 'smaller', 'great', 'manchego', 'slow', 'great', 'creative', 'funky', 'cool', 'warm', 'romantic', 'older', 'good', 'crazy', 'nice'], ['mad', 'neat', 'little', 'csi', 'csi', 'cool', 'laughable', 'crappiest', 'low', 'obvious', 'awesome', 'old', 'mad', 'craptastic'], ['better', 'good', 'asian', 'gracious', 'pleasant', 'eager', 'peach', 'refreshing', 'creative', 'large', 'nice', 'nice', 'chinese', 'original', 'delicious', 'sweet', 'sweet', 'beautiful', 'serene', 'nice', 'romantic', 'enjoyable'], ['better', 'better', 'excited', 'high', 'ready', 'ok', 'slow', 'amazing', 'nice', 'long', 'amazed', 'disappointing', 'good', 'dry', 'mushy', 'gross', 'loud', 'second'], ['horrible', 'rude', 'general', 'rude', 'big'], ['summerlious', 'rare', 'rare', 'good', 'smooth', 'terrible', 'terrible'], ['bland', 'proportionate', 'different', 'appetizing', 'decent', 'fresh'], ['different', 'early', 'clean', 'nice', 'smallest', 'good', 'small', 'good', 'special', 'stale', 'tiny', 'sized', 'nice', 'fellow', 'horrible'], ['neutral'], ['soulless', 'inexperienced', 'interested', 'better']]\n",
      "create taggedDocument\n",
      "build vocab\n",
      "training vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:11:51: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:51: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:51: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:51: EPOCH - 1 : training on 94198 raw words (77724 effective words) took 0.2s, 334508 effective words/s\n",
      "INFO - 14:11:52: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:52: EPOCH - 2 : training on 94198 raw words (77808 effective words) took 0.2s, 334701 effective words/s\n",
      "INFO - 14:11:52: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:52: EPOCH - 3 : training on 94198 raw words (77640 effective words) took 0.2s, 313609 effective words/s\n",
      "INFO - 14:11:52: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:52: EPOCH - 4 : training on 94198 raw words (77862 effective words) took 0.3s, 308540 effective words/s\n",
      "INFO - 14:11:52: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:52: EPOCH - 5 : training on 94198 raw words (77760 effective words) took 0.3s, 307828 effective words/s\n",
      "INFO - 14:11:53: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:53: EPOCH - 6 : training on 94198 raw words (77811 effective words) took 0.2s, 316478 effective words/s\n",
      "INFO - 14:11:53: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:53: EPOCH - 7 : training on 94198 raw words (77799 effective words) took 0.2s, 337367 effective words/s\n",
      "INFO - 14:11:53: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:53: EPOCH - 8 : training on 94198 raw words (77602 effective words) took 0.2s, 330966 effective words/s\n",
      "INFO - 14:11:53: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:53: EPOCH - 9 : training on 94198 raw words (77805 effective words) took 0.2s, 312779 effective words/s\n",
      "INFO - 14:11:54: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:54: EPOCH - 10 : training on 94198 raw words (77763 effective words) took 0.2s, 327604 effective words/s\n",
      "INFO - 14:11:54: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:54: EPOCH - 11 : training on 94198 raw words (77737 effective words) took 0.2s, 328467 effective words/s\n",
      "INFO - 14:11:54: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:54: EPOCH - 12 : training on 94198 raw words (77681 effective words) took 0.2s, 314903 effective words/s\n",
      "INFO - 14:11:54: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:54: EPOCH - 13 : training on 94198 raw words (77885 effective words) took 0.2s, 314778 effective words/s\n",
      "INFO - 14:11:55: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:55: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:55: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:55: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:55: EPOCH - 14 : training on 94198 raw words (77726 effective words) took 0.2s, 328105 effective words/s\n",
      "INFO - 14:11:55: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:55: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:55: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:55: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:55: EPOCH - 15 : training on 94198 raw words (77774 effective words) took 0.2s, 317457 effective words/s\n",
      "INFO - 14:11:55: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:55: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:55: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:55: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:55: EPOCH - 16 : training on 94198 raw words (77821 effective words) took 0.2s, 326974 effective words/s\n",
      "INFO - 14:11:55: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:55: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:55: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:55: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:55: EPOCH - 17 : training on 94198 raw words (77744 effective words) took 0.2s, 323219 effective words/s\n",
      "INFO - 14:11:56: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:56: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:56: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:56: EPOCH - 18 : training on 94198 raw words (77660 effective words) took 0.2s, 320073 effective words/s\n",
      "INFO - 14:11:56: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:56: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:56: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:56: EPOCH - 19 : training on 94198 raw words (77739 effective words) took 0.2s, 313476 effective words/s\n",
      "INFO - 14:11:56: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:56: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:56: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:56: worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:11:56: EPOCH - 20 : training on 94198 raw words (77630 effective words) took 0.2s, 317909 effective words/s\n",
      "INFO - 14:11:56: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:56: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:56: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:56: EPOCH - 21 : training on 94198 raw words (77878 effective words) took 0.2s, 337069 effective words/s\n",
      "INFO - 14:11:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:57: EPOCH - 22 : training on 94198 raw words (77701 effective words) took 0.2s, 328638 effective words/s\n",
      "INFO - 14:11:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:57: EPOCH - 23 : training on 94198 raw words (77756 effective words) took 0.2s, 332718 effective words/s\n",
      "INFO - 14:11:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:57: EPOCH - 24 : training on 94198 raw words (77590 effective words) took 0.2s, 340061 effective words/s\n",
      "INFO - 14:11:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:57: EPOCH - 25 : training on 94198 raw words (77837 effective words) took 0.2s, 324580 effective words/s\n",
      "INFO - 14:11:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:58: EPOCH - 26 : training on 94198 raw words (77701 effective words) took 0.2s, 324545 effective words/s\n",
      "INFO - 14:11:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:58: EPOCH - 27 : training on 94198 raw words (77876 effective words) took 0.2s, 327365 effective words/s\n",
      "INFO - 14:11:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:58: EPOCH - 28 : training on 94198 raw words (77637 effective words) took 0.2s, 317543 effective words/s\n",
      "INFO - 14:11:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:58: EPOCH - 29 : training on 94198 raw words (77766 effective words) took 0.2s, 327089 effective words/s\n",
      "INFO - 14:11:59: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 14:11:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:11:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:11:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:11:59: EPOCH - 30 : training on 94198 raw words (77758 effective words) took 0.2s, 339567 effective words/s\n",
      "INFO - 14:11:59: Doc2Vec lifecycle event {'msg': 'training on 2825940 raw words (2332471 effective words) took 7.5s, 312688 effective words/s', 'datetime': '2021-03-31T14:11:59.172229', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'train'}\n",
      "/Library/Python/3.7/site-packages/ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "WARNING - 14:11:59: destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n",
      "INFO - 14:11:59: storing 2809x32 projection weights into adj_word2vec_model.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab adj size : 2809\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.72450471  0.29725319 -0.06057299 ... -0.43816075 -0.25987327\n",
      "   0.33079097]\n",
      " [ 0.17616732  0.54496789 -0.28987563 ... -0.33969417 -0.50784272\n",
      "  -0.05899191]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# train dot2vec model for adjective words\n",
    "\n",
    "phrases = Phrases(X_train_adj, min_count=30, progress_per=10000)\n",
    "bigram = Phraser(phrases)\n",
    "adjectives = bigram[X_train_adj]\n",
    "\n",
    "all_adjectives = [v for v in adjectives]\n",
    "\n",
    "print(all_adjectives[:20])\n",
    "                     \n",
    "# model_w2v = Word2Vec(vector_size=embedding_size, window=5, min_count=3, workers=4,                                        \n",
    "#                     sample=6e-5, \n",
    "#                      alpha=0.03, \n",
    "#                      min_alpha=0.0007, \n",
    "#                      negative=20,)\n",
    "\n",
    "\n",
    "model_w2v_adj = Doc2Vec(dm=1, dm_concat=1, vector_size=embedding_size, window=5, negative=5, min_count=2, workers=4, alpha=0.065, min_alpha=0.065)\n",
    "print(\"create taggedDocument\")\n",
    "document = [TaggedDocument(doc, [i]) for i, doc in enumerate(all_adjectives)]\n",
    "print(\"build vocab\")\n",
    "model_w2v_adj.build_vocab(document, progress_per=10000)\n",
    "print(\"training vocab\")\n",
    "model_w2v_adj.train(document, total_examples=model_w2v_adj.corpus_count, epochs=30, report_delay=1)\n",
    "model_w2v_adj.init_sims(replace=True)\n",
    "\n",
    "#model_w2v = Word2Vec(sentences=common_texts, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
    "model_w2v_adj.wv.save_word2vec_format(\"adj_word2vec_model.txt\",binary=False)\n",
    "\n",
    "print(\"vocab adj size :\",model_w2v_adj.wv.vectors.shape[0])\n",
    "\n",
    "# read from word2vec model\n",
    "adj_embeddings_index ={}\n",
    "f = open(os.path.join(\"\", \"adj_word2vec_model.txt\"), encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    adj_embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "adj_embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "\n",
    "for word, i in adj_tokenizer.word_index.items():\n",
    "    if word in model_w2v_adj.wv:\n",
    "        adj_embedding_matrix[i] = model_w2v_adj.wv.get_vector(word)\n",
    "\n",
    "print(adj_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 1 0]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 2 1]\n",
      " [1 1 0]\n",
      " [0 0 0]\n",
      " [0 0 3]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [5 0 3]\n",
      " [0 0 1]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 1 0]]\n",
      "(12000, 3)\n"
     ]
    }
   ],
   "source": [
    "# model input layers\n",
    "# complete sentence layer (pre-trained by Dot2Vec model), Shape=(vocab_size, embedding_size)\n",
    "# helpful information layer (cool, funny, useful), Shape = (3, training_datasize)\n",
    "# adjective words layer (POS == ADJ), shape = (vocab_size, embedding_size)\n",
    "\n",
    "X_train_helpful = np.hstack((np.vstack(train_df[\"cool\"]) , np.vstack(train_df[\"funny\"]) ,np.vstack(train_df[\"useful\"])))\n",
    "X_test_helpful = np.hstack((np.vstack(valid_df[\"cool\"]), np.vstack(valid_df[\"funny\"]), np.vstack(valid_df[\"useful\"])))\n",
    "print(X_train_helpful[:10])\n",
    "print(X_test_helpful[:10])\n",
    "X_combine_helpful = np.vstack((X_train_helpful,X_test_helpful))\n",
    "print(X_combine_helpful[:10])\n",
    "print(X_combine_helpful.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"MLP_Dot2Vec_CNN_RNN\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inp_sentence (InputLayer)       [(None, 506)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inp_adj (InputLayer)            [(None, 506)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_147 (Embedding)       (None, 506, 32)      932608      inp_sentence[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_148 (Embedding)       (None, 506, 32)      932608      inp_adj[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_70 (Concatenate)    (None, 506, 64)      0           embedding_147[0][0]              \n",
      "                                                                 embedding_148[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 504, 128)     24704       concatenate_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 502, 64)      24640       conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 62, 64)       0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_40 (Bidirectional (None, 128)          49920       max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 128)          0           bidirectional_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_75 (Dense)                (None, 64)           8256        flatten_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 64)           256         dense_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 64)           0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_76 (Dense)                (None, 5)            325         dropout_32[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,973,317\n",
      "Trainable params: 107,973\n",
      "Non-trainable params: 1,865,344\n",
      "__________________________________________________________________________________________________\n",
      "<class 'numpy.int64'> 3\n",
      "outlier :  0\n",
      "shuffle_train_pad :  20000 10000 2000 [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 1 1]]\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "X_train_len :  20000\n",
      "shape of Y_train :  (10000, 5)\n",
      "shape of Y_test  :  (2000, 5)\n",
      "Model: \"MLP_Dot2Vec_CNN_RNN\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inp_sentence (InputLayer)       [(None, 506)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inp_adj (InputLayer)            [(None, 506)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_147 (Embedding)       (None, 506, 32)      932608      inp_sentence[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_148 (Embedding)       (None, 506, 32)      932608      inp_adj[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_70 (Concatenate)    (None, 506, 64)      0           embedding_147[0][0]              \n",
      "                                                                 embedding_148[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 504, 128)     24704       concatenate_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 502, 64)      24640       conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 62, 64)       0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_40 (Bidirectional (None, 128)          49920       max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 128)          0           bidirectional_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_75 (Dense)                (None, 64)           8256        flatten_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 64)           256         dense_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 64)           0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_76 (Dense)                (None, 5)            325         dropout_32[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,973,317\n",
      "Trainable params: 107,973\n",
      "Non-trainable params: 1,865,344\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "test pad :  2\n",
      "start training\n",
      "Epoch 1/100\n",
      "40/40 [==============================] - 10s 254ms/step - loss: 1.6835 - accuracy: 0.3275 - val_loss: 1.4868 - val_accuracy: 0.4120\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 10s 242ms/step - loss: 1.3598 - accuracy: 0.4469 - val_loss: 1.4211 - val_accuracy: 0.4600\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 10s 244ms/step - loss: 1.2597 - accuracy: 0.4838 - val_loss: 1.3602 - val_accuracy: 0.4770\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 10s 241ms/step - loss: 1.1898 - accuracy: 0.5081 - val_loss: 1.3138 - val_accuracy: 0.4585\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 10s 247ms/step - loss: 1.1200 - accuracy: 0.5422 - val_loss: 1.2767 - val_accuracy: 0.4855\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 10s 242ms/step - loss: 1.0485 - accuracy: 0.5760 - val_loss: 1.2248 - val_accuracy: 0.4940\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 10s 242ms/step - loss: 0.9863 - accuracy: 0.5966 - val_loss: 1.2744 - val_accuracy: 0.4780\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 10s 251ms/step - loss: 0.9223 - accuracy: 0.6342 - val_loss: 1.4268 - val_accuracy: 0.4125\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 10s 254ms/step - loss: 0.8852 - accuracy: 0.6521 - val_loss: 1.3370 - val_accuracy: 0.4595\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 10s 248ms/step - loss: 0.7870 - accuracy: 0.7040 - val_loss: 1.2471 - val_accuracy: 0.4830\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 10s 249ms/step - loss: 0.7210 - accuracy: 0.7261 - val_loss: 1.4015 - val_accuracy: 0.4720\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 11s 268ms/step - loss: 0.6794 - accuracy: 0.7399 - val_loss: 2.3332 - val_accuracy: 0.3890\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 11s 282ms/step - loss: 0.6326 - accuracy: 0.7631 - val_loss: 1.5326 - val_accuracy: 0.4665\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 12s 290ms/step - loss: 0.5620 - accuracy: 0.7962 - val_loss: 1.7773 - val_accuracy: 0.4120\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 12s 297ms/step - loss: 0.4751 - accuracy: 0.8369 - val_loss: 1.8452 - val_accuracy: 0.4290\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.4871 - accuracy: 0.8234 - val_loss: 2.6890 - val_accuracy: 0.4205\n",
      "Model: \"MLP_Dot2Vec_CNN_RNN\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inp_sentence (InputLayer)       [(None, 506)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inp_adj (InputLayer)            [(None, 506)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_147 (Embedding)       (None, 506, 32)      932608      inp_sentence[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_148 (Embedding)       (None, 506, 32)      932608      inp_adj[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_70 (Concatenate)    (None, 506, 64)      0           embedding_147[0][0]              \n",
      "                                                                 embedding_148[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 504, 128)     24704       concatenate_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 502, 64)      24640       conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 62, 64)       0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_40 (Bidirectional (None, 128)          49920       max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 128)          0           bidirectional_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_75 (Dense)                (None, 64)           8256        flatten_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 64)           256         dense_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 64)           0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_76 (Dense)                (None, 5)            325         dropout_32[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,973,317\n",
      "Trainable params: 107,973\n",
      "Non-trainable params: 1,865,344\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "<tensorflow.python.keras.callbacks.History object at 0x178685978>\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 2.6918 - accuracy: 0.4187\n",
      "score: 2.69\n",
      "acc: 0.42\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "VOCAB_SIZE = vocab_size\n",
    "EMBEDDING_SIZE = embedding_size\n",
    "\n",
    "# Use Input layers, specify input shape (dimensions except first)\n",
    "input_length = max_length\n",
    "#layer_sentence_data = keras.layers.Input(shape=(VOCAB_SIZE, EMBEDDING_SIZE))\n",
    "inp_sentence_data = keras.layers.Input(shape=(input_length,),name=\"inp_sentence\")\n",
    "inp_adj_data = keras.layers.Input(shape=(input_length,), name=\"inp_adj\")\n",
    "\n",
    "\n",
    "#inp_helpful_data = keras.layers.Input(shape=(input_length,))\n",
    "\n",
    "layer_sentence_data = Embedding( vocab_size, \n",
    "                                 embedding_size,\n",
    "                                 weights=[embedding_matrix],\n",
    "                                 input_length=max_length,\n",
    "                                 trainable=False)(inp_sentence_data)\n",
    "layer_adj_data =  Embedding( vocab_size, \n",
    "                                 embedding_size,\n",
    "                                 weights=[adj_embedding_matrix],\n",
    "                                 input_length=max_length,\n",
    "                                 trainable=False)(inp_adj_data)\n",
    "\n",
    "#layer_helpful_data = keras.layers.Input(shape=(input_length,32),name=\"inp_helpful\")\n",
    "\n",
    "input_layers = layers.concatenate([layer_sentence_data, layer_adj_data])\n",
    "# Bind nulti_hot to embedding layer\n",
    "#emb = keras.layers.Embedding(input_dim=no_of_unique_cat, output_dim=embedding_size)(inp_cat_data)  \n",
    "\n",
    "mlp = layers.Conv1D(128, kernel_size=3, padding='valid', activation='relu', )(input_layers)\n",
    "mlp = layers.Conv1D(64, kernel_size=3, padding='valid', activation='relu')(mlp)\n",
    "mlp = layers.MaxPooling1D(pool_size=8)(mlp)\n",
    "# mlp = layers.Flatten()(mlp)\n",
    "# mlp = layers.Dense(20, activation = \"relu\")(mlp)\n",
    "# mlp = tf.expand_dims(mlp, axis=-1)\n",
    "mlp = layers.Bidirectional(GRU(units=64))(mlp)\n",
    "mlp = layers.Flatten()(mlp)\n",
    "mlp = Dense(64,activation='relu')(mlp)\n",
    "mlp = BatchNormalization()(mlp)\n",
    "mlp = Dropout(0.5)(mlp)\n",
    "#mlp = layers.Flatten()(mlp)\n",
    "output_layers = keras.layers.Dense(5, activation=\"softmax\")(mlp)\n",
    "\n",
    "model = keras.Model(name=\"MLP_Dot2Vec_CNN_RNN\",inputs=[inp_sentence_data, inp_adj_data], outputs=output_layers)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "'''\n",
    "model=Sequential()\n",
    "\n",
    "#embeddings_initializer=Constant(embedding_matrix),\n",
    "\n",
    "model.add(Embedding(vocab_size, \n",
    "                    embedding_size,\n",
    "                     weights=[embedding_matrix],\n",
    "                    input_length=max_length,\n",
    "                   trainable=False))\n",
    "\n",
    "#model.add(layers.Bidirectional(GRU(units=64, dropout=0.2, return_sequences=True)))\n",
    "model.add(Conv1D(128, kernel_size=3, padding='valid', activation='relu'))\n",
    "model.add(Conv1D(64, kernel_size=3, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=8))\n",
    "\n",
    "model.add(layers.Bidirectional(GRU(units=64, dropout=0.2)))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "'''\n",
    "\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dropout(0.4))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "#model.add(layers.Bidirectional(LSTM(embedding_size//2, dropout=0.2, recurrent_dropout=0.2)))\n",
    "#model.add(SpatialDropout1D(0.4))\n",
    "#model.add(SpatialDropout1D(0.4))\n",
    "\n",
    "# compile\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "batch_size = 256\n",
    "num_epochs = 7\n",
    "\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X_train_pad,to_categorical(train_df[\"stars\"].values), test_size = 0.33, random_state = 42)\n",
    "#print(X_train.shape,Y_train.shape)\n",
    "#print(X_test.shape,Y_test.shape)\n",
    "\n",
    "#Y_train = np.array(to_categorical(train_df[\"stars\"].values))\n",
    "#Y_test = np.array(to_categorical(valid_df[\"stars\"].values))\n",
    "\n",
    "\n",
    "sentiment = np.concatenate((train_df[\"stars\"].values, valid_df[\"stars\"].values)) - 1\n",
    "isWithInRange = True\n",
    "print(type(sentiment[0]), sentiment[0])\n",
    "for num in sentiment:\n",
    "    if num < 1 or num > 5:\n",
    "        isWithInRange = False\n",
    "        print(\"outlier : \",num)\n",
    "        break\n",
    "        \n",
    "        \n",
    "VALIDATION_SPLIT = 0.2\n",
    "indices = np.arange(X_train_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "#shuffle_train_pad = X_train_pad\n",
    "#shuffle_train_pad = np.array(X_train_helpful) + np.array(X_test_helpful)\n",
    "#shuffle_train_pad = np.array((X_train_pad), X_train_adj_pad,X_combine_helpful)\n",
    "\n",
    "print(\"shuffle_train_pad : \",len(shuffle_train_pad), len(X_train_helpful), len(X_test_helpful),shuffle_train_pad[:5])\n",
    "\n",
    "#shuffle_train_pad = X_train_pad[indices]\n",
    "#sentiment = sentiment[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * X_train_pad.shape[0])\n",
    "\n",
    "sentiment = np.array(to_categorical(sentiment))\n",
    "print(sentiment[:3])\n",
    "\n",
    "# X_train_pad = shuffle_train_pad[:-num_validation_samples]\n",
    "# Y_train = sentiment[:-num_validation_samples]\n",
    "# X_test_pad = shuffle_train_pad[-num_validation_samples:]\n",
    "# Y_test = sentiment[-num_validation_samples:]\n",
    "\n",
    "print(\"X_train_len : \",len(shuffle_train_pad))\n",
    "#X_train_pad_tmp = shuffle_train_pad[:10000]\n",
    "X_train_pad_tmp = [X_train_pad[:10000], X_train_adj_pad[:10000]]\n",
    "Y_train_tmp = sentiment[:10000]\n",
    "X_test_pad_tmp = [X_train_pad[10000:], X_train_adj_pad[10000:]]\n",
    "Y_test_tmp = sentiment[10000:]\n",
    "\n",
    "\n",
    "#print(\"shape of X_train_pad : \",X_train_pad_tmp.shape)\n",
    "print(\"shape of Y_train : \",Y_train_tmp.shape)\n",
    "#print(\"shape of X_test_pad : \",X_test_pad_tmp.shape)\n",
    "print(\"shape of Y_test  : \",Y_test_tmp.shape)\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "callback = EarlyStopping(monitor='val_accuracy', mode=\"max\",baseline=0.025,patience=10)\n",
    "\n",
    "#print(X_test_pad)\n",
    "#print(X_train[0])\n",
    "print(\"test pad : \",len(X_test_pad_tmp))\n",
    "print(\"start training\")\n",
    "#X_train = np.array([\" \".join(v) for v in X_train])\n",
    "#model.fit(X_train_pad, Y_train, epochs = num_epochs, batch_size=batch_size, validation_data=(X_test_pad, Y_test), callbacks=[callback])\n",
    "#            validation_steps=,\n",
    "history = model.fit(  X_train_pad_tmp, Y_train_tmp, \n",
    "            epochs=100,\n",
    "                    batch_size=batch_size,\n",
    "            validation_data=(X_test_pad_tmp,Y_test_tmp), \n",
    "\n",
    "          verbose=1,\n",
    "            callbacks=[callback]              \n",
    "         )\n",
    "# X_valid, Y_valid = X_train[:batch_size], Y_train[:batch_size]\n",
    "# X_train2, Y_train2 = X_train[batch_size:], Y_train[batch_size:]\n",
    "# history = model.fit(X_train2, Y_train2, validation_data=(X_valid, Y_valid), batch_size=batch_size, epochs=num_epochs)\n",
    "print(model.summary())\n",
    "# predict\n",
    "print(history)\n",
    "\n",
    "\n",
    "validation_size = 1500\n",
    "\n",
    "# test_loss, test_acc = model.evaluate(X_test_dataset)\n",
    "\n",
    "# print('Test Loss: {}'.format(test_loss))\n",
    "# print('Test Accuracy: {}'.format(test_acc))\n",
    "\n",
    "#X_validate = X_test_pad_tmp[-validation_size:]\n",
    "#Y_validate = Y_test_tmp[-validation_size:]\n",
    "\n",
    "\n",
    "#X_validate = X_test_pad_tmp[-validation_size:]\n",
    "X_validate = [X_train_pad[12000-validation_size:], X_train_adj_pad[12000-validation_size:]]\n",
    "Y_validate = Y_test_tmp[-validation_size:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Y_test = np.array(to_categorical(X_[\"stars\"].values))\n",
    "\n",
    "#X_test = X_test_pad\n",
    "\n",
    "score,acc = model.evaluate(X_validate, Y_validate, verbose = 1, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n",
    "# plt.figure(figsize=(16,8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plot_graphs(history, 'accuracy')\n",
    "# plt.ylim(None,1)\n",
    "# plt.subplot(1,2,2)\n",
    "# plot_graphs(history, 'loss')\n",
    "# plt.ylim(0,None)\n",
    "# scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id  cool                 date  funny  \\\n",
      "0  39rLHYJOy2774ZIUouuWLw     0  2017-06-28 21:44:02      0   \n",
      "1  E-Kq1Yu1d6N3TL2qX0aqjA     0  2018-04-09 03:45:19      0   \n",
      "2  nWW6fBfBljiRFa4sG7TyxA     0  2014-07-19 02:00:04      0   \n",
      "3  qmIHO-6T_KEfPC9jyGDamQ     0  2011-11-11 08:10:24      0   \n",
      "4  pKk7jCFIm96qDdk0laVT2w     1  2010-01-16 20:04:00      1   \n",
      "\n",
      "                review_id  stars  \\\n",
      "0  ynzOFepQYSCDGdfWDWxiZw      4   \n",
      "1  sQX9ncJBEdBf16AWsvO6Vg      2   \n",
      "2  bVIf2kqbzvif3miNe3ARNw      4   \n",
      "3  LNj1OFxy2ool3PZANGchPA      4   \n",
      "4  bZXxa0hO6wQlHD-MkMf4iw      5   \n",
      "\n",
      "                                                text  useful  \\\n",
      "0  Nice to have a diner still around. Food was go...       0   \n",
      "1  Tried this a while back, got the fried chicken...       0   \n",
      "2  I expected more pork selections on menu. Food ...       0   \n",
      "3  YUMMY!!! This place is phenomenal. It is Price...       0   \n",
      "4  The Truffle Macaroni & Cheese and Potatoes Au ...       1   \n",
      "\n",
      "                  user_id  \n",
      "0  Sl6VgFOB-XXfFIAYp7TFkw  \n",
      "1  gcx01pMqWzkni2UC-zoZrA  \n",
      "2  Mn9VzPbrCYU4EcP_C1oBOg  \n",
      "3  SKV1heo00fdciCbCN9Z33A  \n",
      "4  p1r7rZYruZR92x1A649PTQ  \n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 1 0]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 1 0]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())\n",
    "X_train_helpful = np.hstack((np.vstack(train_df[\"cool\"]) , np.vstack(train_df[\"funny\"]) ,np.vstack(train_df[\"useful\"])))\n",
    "X_test_helpful = np.hstack((np.vstack(valid_df[\"cool\"]), np.vstack(valid_df[\"funny\"]), np.vstack(valid_df[\"useful\"])))\n",
    "print(X_train_helpful[:10])\n",
    "print(X_test_helpful[:10])\n",
    "X_combine_helpful = np.vstack((X_train_helpful,X_test_helpful))\n",
    "print(X_combine_helpful[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
