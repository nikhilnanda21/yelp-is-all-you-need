{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4332_P1_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAckpZDZG-BR"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gEmiUAEG8zx"
      },
      "source": [
        "!wget -q https://hkustconnect-my.sharepoint.com/:u:/g/personal/nnanda_connect_ust_hk/EfREjZqiZTlPqhqUPICBbPABdlgPumlaUVxPncm-_9aWIw?download=1 -O \"Project 1 - data.zip\"\n",
        "!unzip -q \"Project 1 - data.zip\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyCUH6drFgou"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_q2Kr7_Fa-g",
        "outputId": "c7200202-005b-4714-81d9-48347fc68ece"
      },
      "source": [
        "!pip -q install keras-layer-normalization"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_uTe2vvFJTL"
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Dropout, BatchNormalization,\\\n",
        "    Activation, Input, Add, Concatenate, Embedding, Conv1D, MaxPool1D,\\\n",
        "    Flatten, LSTM, Bidirectional, MaxPooling1D, SimpleRNN, GRU, SpatialDropout1D\n",
        "from keras_layer_normalization import LayerNormalization\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21Z1gRo3H3Zh",
        "outputId": "c31bd3ab-b2fd-470c-8f0d-c115cd841666"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noMKFCoiHxq1"
      },
      "source": [
        "stopwords = set(stopwords.words(\"english\"))\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Y4M188FjI3"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LdLcI3EFWIK"
      },
      "source": [
        "def load_data(split_name='train', columns=['text', 'stars']):\n",
        "    try:\n",
        "        print(f\"select [{', '.join(columns)}] columns from the {split_name} split\")\n",
        "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
        "        df = df.loc[:,columns]\n",
        "        print(\"succeed!\")\n",
        "        return df\n",
        "    except:\n",
        "        print(\"Failed, then try to \")\n",
        "        print(f\"select all columns from the {split_name} split\")\n",
        "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
        "        return df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQj5souEFnD2",
        "outputId": "ec7b653a-f9dd-415e-9f32-f8c8d3d20ac4"
      },
      "source": [
        "train_df = load_data('train', columns=['full'])\n",
        "valid_df = load_data('valid', columns=['full'])\n",
        "test_df = load_data('test', columns=['full'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "select [full] columns from the train split\n",
            "Failed, then try to \n",
            "select all columns from the train split\n",
            "select [full] columns from the valid split\n",
            "Failed, then try to \n",
            "select all columns from the valid split\n",
            "select [full] columns from the test split\n",
            "Failed, then try to \n",
            "select all columns from the test split\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73s-6sOp6xil"
      },
      "source": [
        "# Following https://towardsdatascience.com/multi-label-multi-class-text-classification-with-bert-transformer-and-keras-c6355eccb63a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPyBgw7D6-Or",
        "outputId": "0caec584-b95d-49ad-c725-ded2942109fd"
      },
      "source": [
        "!pip -q install transformers"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.0MB 7.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 870kB 46.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 52.0MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFQtc00V60bO"
      },
      "source": [
        "#######################################\n",
        "### -------- Load libraries ------- ###\n",
        "# Load Huggingface transformers\n",
        "from transformers import TFBertModel,  BertConfig, BertTokenizerFast\n",
        "# Then what you need from tensorflow.keras\n",
        "from tensorflow.keras.layers import Input, Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.initializers import TruncatedNormal\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# And pandas for data import + sklearn because you allways need sklearn\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9J9kG6mKUOY"
      },
      "source": [
        "# Bert without Attention Mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mK-cw4D624f",
        "outputId": "aeb8b244-965f-4cd6-9ee9-7a66dfae5517"
      },
      "source": [
        "#######################################\n",
        "### --------- Setup BERT ---------- ###\n",
        "# Name of the BERT model to use\n",
        "model_name = 'bert-base-uncased'\n",
        "# Max length of tokens\n",
        "max_length = 100\n",
        "# Load transformers config and set output_hidden_states to False\n",
        "config = BertConfig.from_pretrained(model_name)\n",
        "config.output_hidden_states = False\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n",
        "# Load the Transformers BERT model\n",
        "transformer_model = TFBertModel.from_pretrained(model_name, config = config)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS14CuiCEfCg",
        "outputId": "8c9b3133-73b1-468a-9d6b-fe7c77343e81"
      },
      "source": [
        "config"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"_name_or_path\": \"bert-base-uncased\",\n",
              "  \"architectures\": [\n",
              "    \"BertForMaskedLM\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.4.2\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4HMotZkqCru",
        "outputId": "8b4056de-aeae-4bfb-d518-bbbb46e762c0"
      },
      "source": [
        "transformer_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_model_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  109482240 \n",
            "=================================================================\n",
            "Total params: 109,482,240\n",
            "Trainable params: 109,482,240\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdCU72D97Rqe",
        "outputId": "09a9c316-dfd8-4a6e-d5e0-a8f75208660f"
      },
      "source": [
        "#######################################\n",
        "### ------- Build the model ------- ###\n",
        "# TF Keras documentation: https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
        "# Load the MainLayer\n",
        "bert = transformer_model.layers[0]\n",
        "# Build your model input\n",
        "input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
        "inputs = {'input_ids': input_ids}\n",
        "# Load the Transformers BERT model as a layer in a Keras model\n",
        "bert_model = bert(inputs)[1]\n",
        "# dropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\n",
        "dropout = Dropout(0.45, name='pooled_output')\n",
        "# pooled_output = dropout(bert_model, training=True)\n",
        "pooled_output = dropout(bert_model, training=False)\n",
        "# Then build your model output\n",
        "outputs = Dense(units=5, kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='stars')(pooled_output)\n",
        "# product = Dense(units=len(data.Product_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='product')(pooled_output)\n",
        "# outputs = {'issue': issue, 'product': product}\n",
        "# And combine it all in a model object\n",
        "model = Model(inputs=inputs, outputs=outputs, name='BERT_MultiClass')\n",
        "# Take a look at the model\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"BERT_MultiClass\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_ids (InputLayer)       [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "bert (TFBertMainLayer)       TFBaseModelOutputWithPool 109482240 \n",
            "_________________________________________________________________\n",
            "pooled_output (Dropout)      (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "stars (Dense)                (None, 5)                 3845      \n",
            "=================================================================\n",
            "Total params: 109,486,085\n",
            "Trainable params: 109,486,085\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZhyqaAG8Gkr",
        "outputId": "8e25aead-b101-4870-f9dc-ad6a1a69f21a"
      },
      "source": [
        "#######################################\n",
        "### ------- Train the model ------- ###\n",
        "# Set an optimizer\n",
        "optimizer = RMSprop(\n",
        "    learning_rate=5e-05,\n",
        "    epsilon=1e-08,\n",
        "    decay=0.01,\n",
        "    clipnorm=1.0)\n",
        "# Set loss and metrics\n",
        "loss = {'issue': CategoricalCrossentropy(from_logits = True), 'product': CategoricalCrossentropy(from_logits = True)}\n",
        "metric = {'issue': CategoricalAccuracy('accuracy'), 'product': CategoricalAccuracy('accuracy')}\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer = optimizer,\n",
        "    loss = CategoricalCrossentropy(from_logits = True), \n",
        "    metrics = CategoricalAccuracy('accuracy'))\n",
        "# Ready output data for the model\n",
        "# y_issue = to_categorical(data['Issue'])\n",
        "# y_product = to_categorical(data['Product'])\n",
        "y_train = to_categorical(train_df[\"stars\"]-1, num_classes=5)\n",
        "y_valid = to_categorical(valid_df[\"stars\"]-1, num_classes=5)\n",
        "# Tokenize the input (takes some time)\n",
        "x_train = tokenizer(\n",
        "    text=train_df[\"text\"].to_list(),\n",
        "    add_special_tokens=True,\n",
        "    max_length=max_length,\n",
        "    truncation=True,\n",
        "    padding=True, \n",
        "    return_tensors='tf',\n",
        "    return_token_type_ids = False,\n",
        "    return_attention_mask = False,\n",
        "    verbose = True)\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(\"models\", \"weights_bert_without_attantion_mask.hdf5\"),\n",
        "    monitor=\"val_accuracy\",\n",
        "    verbose=1,\n",
        "    save_best_only=True)\n",
        "earlystopping = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=3,\n",
        "    verbose=1)\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(\n",
        "    x={'input_ids': x_train['input_ids']},\n",
        "    y={'stars': y_train},\n",
        "    validation_split=0.2,\n",
        "    batch_size=64,\n",
        "    epochs=10,\n",
        "    callbacks=[checkpointer, earlystopping])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 190s 1s/step - loss: 1.3383 - accuracy: 0.4212 - val_loss: 0.9569 - val_accuracy: 0.6050\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.60500, saving model to models/weights_bert_without_attantion_mask.hdf5\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 168s 1s/step - loss: 0.8466 - accuracy: 0.6498 - val_loss: 0.9509 - val_accuracy: 0.6065\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.60500 to 0.60650, saving model to models/weights_bert_without_attantion_mask.hdf5\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 168s 1s/step - loss: 0.7038 - accuracy: 0.7238 - val_loss: 0.9714 - val_accuracy: 0.6180\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.60650 to 0.61800, saving model to models/weights_bert_without_attantion_mask.hdf5\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 168s 1s/step - loss: 0.6089 - accuracy: 0.7696 - val_loss: 1.0574 - val_accuracy: 0.6010\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.61800\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 168s 1s/step - loss: 0.5245 - accuracy: 0.8073 - val_loss: 1.1260 - val_accuracy: 0.6010\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.61800\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 168s 1s/step - loss: 0.4581 - accuracy: 0.8376 - val_loss: 1.1744 - val_accuracy: 0.6085\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.61800\n",
            "Epoch 00006: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cBQ4I4q9QKh",
        "outputId": "76e8cf0f-9874-4a26-b1f2-51322264aa19"
      },
      "source": [
        "#######################################\n",
        "### ----- Evaluate the model ------ ###\n",
        "# Ready test data\n",
        "# y_valid\n",
        "x_valid = tokenizer(\n",
        "    text=valid_df[\"text\"].to_list(),\n",
        "    add_special_tokens=True,\n",
        "    max_length=max_length,\n",
        "    truncation=True,\n",
        "    padding=True, \n",
        "    return_tensors='tf',\n",
        "    return_token_type_ids = False,\n",
        "    return_attention_mask = False,\n",
        "    verbose = True)\n",
        "# Run evaluation\n",
        "model_eval = model.evaluate(\n",
        "    x={'input_ids': x_valid['input_ids']},\n",
        "    y={'stars': y_valid}\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 14s 216ms/step - loss: 1.1665 - accuracy: 0.5995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewiKJfCDC9bJ",
        "outputId": "d8a8d01f-7f31-47bc-b5e0-fb67a38b55b3"
      },
      "source": [
        "model = keras.models.load_model(os.path.join(\"models\", \"weights_bert_without_attantion_mask.hdf5\"))\n",
        "\n",
        "train_score = model.evaluate(\n",
        "    x={'input_ids': x_train['input_ids']},\n",
        "    y={'stars': y_train})\n",
        "\n",
        "valid_score = model.evaluate(\n",
        "    x={'input_ids': x_valid['input_ids']},\n",
        "    y={'stars': y_valid})\n",
        "\n",
        "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
        "print(\"valid loss:\", valid_score[0], \"valid accuracy\", valid_score[1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 71s 219ms/step - loss: 0.6354 - accuracy: 0.7609\n",
            "63/63 [==============================] - 14s 217ms/step - loss: 0.9576 - accuracy: 0.6115\n",
            "training loss: 0.635367214679718 training accuracy 0.7609000205993652\n",
            "valid loss: 0.9575802683830261 valid accuracy 0.6115000247955322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3yJvKFtSmE-"
      },
      "source": [
        "Load the bert model without attention - Adam, Drop=0.1, LR=5e-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd-lTloESlmn",
        "outputId": "1a1dfcc0-03a3-46f6-bfe7-13f0ad7034bd"
      },
      "source": [
        "!wget -q https://hkustconnect-my.sharepoint.com/:u:/g/personal/nnanda_connect_ust_hk/Ebw_XPCer7xHrLTrKafKAC4BGecTS0vORrJXhu_2dM8P_g?download=1 -O \"weights_bert_without_attention_mask.hdf5\"\n",
        "\n",
        "model = keras.models.load_model(os.path.join(\"weights_bert_without_attention_mask.hdf5\"))\n",
        "\n",
        "train_score = model.evaluate(\n",
        "    x={'input_ids': x_train['input_ids']},\n",
        "    y={'stars': y_train})\n",
        "\n",
        "valid_score = model.evaluate(\n",
        "    x={'input_ids': x_valid['input_ids']},\n",
        "    y={'stars': y_valid})\n",
        "\n",
        "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
        "print(\"valid loss:\", valid_score[0], \"valid accuracy\", valid_score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 71s 221ms/step - loss: 0.7238 - accuracy: 0.7187\n",
            "63/63 [==============================] - 14s 221ms/step - loss: 0.9239 - accuracy: 0.6175\n",
            "training loss: 0.72379469871521 training accuracy 0.7186999917030334\n",
            "valid loss: 0.923902153968811 valid accuracy 0.6175000071525574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdYS0ZTbzi3p"
      },
      "source": [
        "Load the bert model without attention - RMSprop, Drop=0.5, LR=5e-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llXLBqqvzuER",
        "outputId": "48b60261-0789-472b-a5d3-166b499188df"
      },
      "source": [
        "!wget -q https://hkustconnect-my.sharepoint.com/:u:/g/personal/nnanda_connect_ust_hk/EWQ7aIv2qZJHuvJcPYHeYSQBR9kxTakulpMixT9JozV0Hw?download=1 -O \"weights_bert_without_attention_mask_62.hdf5\"\n",
        "\n",
        "model = keras.models.load_model(os.path.join(\"weights_bert_without_attention_mask_62.hdf5\"))\n",
        "\n",
        "train_score = model.evaluate(\n",
        "    x={'input_ids': x_train['input_ids']},\n",
        "    y={'stars': y_train})\n",
        "\n",
        "valid_score = model.evaluate(\n",
        "    x={'input_ids': x_valid['input_ids']},\n",
        "    y={'stars': y_valid})\n",
        "\n",
        "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
        "print(\"valid loss:\", valid_score[0], \"valid accuracy\", valid_score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 73s 224ms/step - loss: 0.7079 - accuracy: 0.7212\n",
            "63/63 [==============================] - 14s 216ms/step - loss: 0.9042 - accuracy: 0.6200\n",
            "training loss: 0.7079257965087891 training accuracy 0.7211999893188477\n",
            "valid loss: 0.9042415022850037 valid accuracy 0.6200000047683716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_zgMOVAzNXn"
      },
      "source": [
        "Load the bert model without attention - RMSprop, Drop=0.7, LR=5e-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d1QKOtNzXHp",
        "outputId": "2cac0e00-7e55-4270-ca99-25db7b962749"
      },
      "source": [
        "!wget -q https://hkustconnect-my.sharepoint.com/:u:/g/personal/nnanda_connect_ust_hk/ETeQLB1tzXtJtB8amwOMnhQBzpjMDTbStbn3Zht0sb54oA?download=1 -O \"weights_bert_without_attention_mask_62_3.hdf5\"\n",
        "\n",
        "model = keras.models.load_model(os.path.join(\"weights_bert_without_attention_mask_62_3.hdf5\"))\n",
        "\n",
        "train_score = model.evaluate(\n",
        "    x={'input_ids': x_train['input_ids']},\n",
        "    y={'stars': y_train})\n",
        "\n",
        "valid_score = model.evaluate(\n",
        "    x={'input_ids': x_valid['input_ids']},\n",
        "    y={'stars': y_valid})\n",
        "\n",
        "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
        "print(\"valid loss:\", valid_score[0], \"valid accuracy\", valid_score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 74s 223ms/step - loss: 0.7118 - accuracy: 0.7179\n",
            "63/63 [==============================] - 14s 217ms/step - loss: 0.9179 - accuracy: 0.6230\n",
            "training loss: 0.7118481397628784 training accuracy 0.7178999781608582\n",
            "valid loss: 0.9178875088691711 valid accuracy 0.6230000257492065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPDfIjFMOxn2"
      },
      "source": [
        "# Bert with attention mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B2X50agJVX8"
      },
      "source": [
        "#######################################\n",
        "### --------- Setup BERT ---------- ###\n",
        "# Name of the BERT model to use\n",
        "model_name = 'bert-base-uncased'\n",
        "# Max length of tokens\n",
        "max_length = 100\n",
        "# Load transformers config and set output_hidden_states to False\n",
        "config = BertConfig.from_pretrained(model_name)\n",
        "config.output_hidden_states = False\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n",
        "# Load the Transformers BERT model\n",
        "transformer_model = TFBertModel.from_pretrained(model_name, config = config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtM832JYPn7Q",
        "outputId": "5b2f477f-be1c-44f1-87b8-c90cd816dc06"
      },
      "source": [
        "#######################################\n",
        "### ------- Build the model ------- ###\n",
        "# TF Keras documentation: https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
        "\n",
        "# Load the MainLayer\n",
        "bert = transformer_model.layers[0]\n",
        "\n",
        "# Build your model input\n",
        "input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
        "attention_mask = Input(shape=(max_length,), name='attention_mask', dtype='int32') \n",
        "inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
        "\n",
        "# Load the Transformers BERT model as a layer in a Keras model\n",
        "bert_model = bert(inputs)[1]\n",
        "dropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\n",
        "# dropout = Dropout(0.5, name='pooled_output')\n",
        "pooled_output = dropout(bert_model, training=False)\n",
        "\n",
        "# Then build your model output\n",
        "outputs = Dense(units=5, kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='stars')(pooled_output)\n",
        "\n",
        "# And combine it all in a model object\n",
        "model_attention = Model(inputs=inputs, outputs=outputs, name='BERT_MultiClass')\n",
        "\n",
        "# Take a look at the model\n",
        "model_attention.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"BERT_MultiClass\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "attention_mask (InputLayer)     [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_ids (InputLayer)          [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert (TFBertMainLayer)          TFBaseModelOutputWit 109482240   attention_mask[0][0]             \n",
            "                                                                 input_ids[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "pooled_output (Dropout)         (None, 768)          0           bert[0][1]                       \n",
            "__________________________________________________________________________________________________\n",
            "stars (Dense)                   (None, 5)            3845        pooled_output[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 109,486,085\n",
            "Trainable params: 109,486,085\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RhCY-mLNRwi",
        "outputId": "64d31948-9067-410d-818e-16cc7592cde8"
      },
      "source": [
        "#######################################\n",
        "### ------- Train the model ------- ###\n",
        "# Set an optimizer\n",
        "optimizer = RMSprop(\n",
        "    learning_rate=5e-05,\n",
        "    epsilon=1e-08,\n",
        "    decay=0.01,\n",
        "    clipnorm=1.0)\n",
        "\n",
        "# Compile the model\n",
        "model_attention.compile(\n",
        "    optimizer = optimizer,\n",
        "    loss = CategoricalCrossentropy(from_logits = True), \n",
        "    metrics = CategoricalAccuracy('accuracy'))\n",
        "\n",
        "# Ready output data for the model\n",
        "y_train = to_categorical(train_df[\"stars\"]-1, num_classes=5)\n",
        "y_valid = to_categorical(valid_df[\"stars\"]-1, num_classes=5)\n",
        "\n",
        "# Tokenize the input (takes some time)\n",
        "x_train_attention_mask = tokenizer(\n",
        "    text=train_df[\"text\"].to_list(),\n",
        "    add_special_tokens=True,\n",
        "    max_length=max_length,\n",
        "    truncation=True,\n",
        "    padding=True, \n",
        "    return_tensors='tf',\n",
        "    return_token_type_ids = False,\n",
        "    return_attention_mask = True,\n",
        "    verbose = True)\n",
        "\n",
        "x_valid_attention_mask = tokenizer(\n",
        "    text=valid_df[\"text\"].to_list(),\n",
        "    add_special_tokens=True,\n",
        "    max_length=max_length,\n",
        "    truncation=True,\n",
        "    padding=True, \n",
        "    return_tensors='tf',\n",
        "    return_token_type_ids = False,\n",
        "    return_attention_mask = True,\n",
        "    verbose = True)\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(\"models\", \"weights_bert_with_attention_mask.hdf5\"),\n",
        "    monitor=\"val_accuracy\",\n",
        "    verbose=1,\n",
        "    save_best_only=True)\n",
        "earlystopping = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    verbose=1)\n",
        "\n",
        "# Fit the model\n",
        "history = model_attention.fit(\n",
        "    x={'input_ids': x_train_attention_mask['input_ids'], 'attention_mask': x_train_attention_mask['attention_mask']},\n",
        "    y={'stars': y_train},\n",
        "    validation_split=0.2,\n",
        "    batch_size=64,\n",
        "    epochs=10,\n",
        "    callbacks=[checkpointer, earlystopping])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 184s 1s/step - loss: 1.2488 - accuracy: 0.4653 - val_loss: 0.9380 - val_accuracy: 0.6200\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.62000, saving model to models/weights_bert_with_attention_mask.hdf5\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 167s 1s/step - loss: 0.8302 - accuracy: 0.6616 - val_loss: 0.9287 - val_accuracy: 0.6205\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.62000 to 0.62050, saving model to models/weights_bert_with_attention_mask.hdf5\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 166s 1s/step - loss: 0.6995 - accuracy: 0.7187 - val_loss: 0.9763 - val_accuracy: 0.6225\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.62050 to 0.62250, saving model to models/weights_bert_with_attention_mask.hdf5\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 166s 1s/step - loss: 0.5842 - accuracy: 0.7787 - val_loss: 1.0322 - val_accuracy: 0.6060\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.62250\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 167s 1s/step - loss: 0.5115 - accuracy: 0.8145 - val_loss: 1.1033 - val_accuracy: 0.6045\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.62250\n",
            "Epoch 00005: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1rPnjSuQXpk",
        "outputId": "eb4af9c7-6d2f-4bb3-d585-48001b73a260"
      },
      "source": [
        "model = keras.models.load_model(os.path.join(\"models\", \"weights_bert_with_attention_mask.hdf5\"))\n",
        "\n",
        "train_score = model.evaluate(\n",
        "    x={'input_ids': x_train_attention_mask['input_ids'], 'attention_mask': x_train_attention_mask['attention_mask']},\n",
        "    y={'stars': y_train})\n",
        "\n",
        "valid_score = model.evaluate(\n",
        "    x={'input_ids': x_valid_attention_mask['input_ids'], 'attention_mask': x_valid_attention_mask['attention_mask']},\n",
        "    y={'stars': y_valid})\n",
        "\n",
        "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
        "print(\"valid loss:\", valid_score[0], \"valid accuracy\", valid_score[1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 71s 219ms/step - loss: 0.6308 - accuracy: 0.7638\n",
            "63/63 [==============================] - 14s 217ms/step - loss: 0.9697 - accuracy: 0.6265\n",
            "training loss: 0.6308273673057556 training accuracy 0.7638000249862671\n",
            "valid loss: 0.9697353839874268 valid accuracy 0.6265000104904175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcTz5Ih3T0jA"
      },
      "source": [
        "Load the Bert model with attention - Drop=0.1, Adam, LR=5e-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljkNc5ucQ_aM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b32d3c01-bb52-4f5e-b2c2-2b94a7921455"
      },
      "source": [
        "!wget -q https://hkustconnect-my.sharepoint.com/:u:/g/personal/nnanda_connect_ust_hk/EePikR9TtXhCu5uVzRyZ9P8B2jpezGfALcGQTz_emtudXQ?download=1 -O \"weights_bert_with_attention_mask.hdf5\"\n",
        "\n",
        "model = keras.models.load_model(os.path.join(\"weights_bert_with_attention_mask.hdf5\"))\n",
        "\n",
        "train_score = model.evaluate(\n",
        "    x={'input_ids': x_train_attention_mask['input_ids'], 'attention_mask': x_train_attention_mask['attention_mask']},\n",
        "    y={'stars': y_train})\n",
        "\n",
        "valid_score = model.evaluate(\n",
        "    x={'input_ids': x_valid_attention_mask['input_ids'], 'attention_mask': x_valid_attention_mask['attention_mask']},\n",
        "    y={'stars': y_valid})\n",
        "\n",
        "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
        "print(\"valid loss:\", valid_score[0], \"valid accuracy\", valid_score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 66s 202ms/step - loss: 0.5933 - accuracy: 0.7848\n",
            "63/63 [==============================] - 13s 206ms/step - loss: 0.9735 - accuracy: 0.6210\n",
            "training loss: 0.5933009386062622 training accuracy 0.7847999930381775\n",
            "valid loss: 0.973475456237793 valid accuracy 0.6209999918937683\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4Xcd74cFSjn"
      },
      "source": [
        "Load the Bert model with attention - Drop=0.1, RMSprop, LR=5e-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ydef3WfOIc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17fb36c3-9ac7-4f58-eaff-99beef4d9888"
      },
      "source": [
        "!wget -q https://hkustconnect-my.sharepoint.com/:u:/g/personal/nnanda_connect_ust_hk/EXrPZ5UNreNKsJhqoZgBbuABcVH8WpSlYWxgiNC23weYJg?download=1 -O \"weights_bert_with_attention_mask_62_65.hdf5\"\n",
        "\n",
        "model = keras.models.load_model(os.path.join(\"weights_bert_with_attention_mask_62_65.hdf5\"))\n",
        "\n",
        "train_score = model.evaluate(\n",
        "    x={'input_ids': x_train_attention_mask['input_ids'], 'attention_mask': x_train_attention_mask['attention_mask']},\n",
        "    y={'stars': y_train})\n",
        "\n",
        "valid_score = model.evaluate(\n",
        "    x={'input_ids': x_valid_attention_mask['input_ids'], 'attention_mask': x_valid_attention_mask['attention_mask']},\n",
        "    y={'stars': y_valid})\n",
        "\n",
        "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
        "print(\"valid loss:\", valid_score[0], \"valid accuracy\", valid_score[1])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 71s 219ms/step - loss: 0.6308 - accuracy: 0.7638\n",
            "63/63 [==============================] - 14s 216ms/step - loss: 0.9697 - accuracy: 0.6265\n",
            "training loss: 0.6308273673057556 training accuracy 0.7638000249862671\n",
            "valid loss: 0.9697353839874268 valid accuracy 0.6265000104904175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU-zGln5JglE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}