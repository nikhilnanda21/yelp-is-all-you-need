{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3-final"
    },
    "colab": {
      "name": "demo.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhilnanda21/RMBI4310-COMP4332-Project/blob/jamie/Project1/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAckpZDZG-BR"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gEmiUAEG8zx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93935444-0f53-47bf-809a-2a1a5a30b9ac"
      },
      "source": [
        "!wget https://hkustconnect-my.sharepoint.com/:u:/g/personal/nnanda_connect_ust_hk/ERROCEL56mNPqSWWzbzLj3cBvQTtCS9yzoiUfVlhIx1CCA?download=1 -O \"data_2021_spring.zip\"\n",
        "!unzip \"data_2021_spring.zip\"\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-27 14:35:41--  https://hkustconnect-my.sharepoint.com/:u:/g/personal/nnanda_connect_ust_hk/ERROCEL56mNPqSWWzbzLj3cBvQTtCS9yzoiUfVlhIx1CCA?download=1\n",
            "Resolving hkustconnect-my.sharepoint.com (hkustconnect-my.sharepoint.com)... 13.107.136.9\n",
            "Connecting to hkustconnect-my.sharepoint.com (hkustconnect-my.sharepoint.com)|13.107.136.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /personal/nnanda_connect_ust_hk/Documents/COMP4332%20Project%20Data/data_2021_spring.zip?originalPath=aHR0cHM6Ly9oa3VzdGNvbm5lY3QtbXkuc2hhcmVwb2ludC5jb20vOnU6L2cvcGVyc29uYWwvbm5hbmRhX2Nvbm5lY3RfdXN0X2hrL0VSUk9DRUw1Nm1OUHFTV1d6YnpMajNjQnZRVHRDUzl5em9pVWZWbGhJeDFDQ0E_cnRpbWU9dzZGeG1TM3gyRWc [following]\n",
            "--2021-03-27 14:35:42--  https://hkustconnect-my.sharepoint.com/personal/nnanda_connect_ust_hk/Documents/COMP4332%20Project%20Data/data_2021_spring.zip?originalPath=aHR0cHM6Ly9oa3VzdGNvbm5lY3QtbXkuc2hhcmVwb2ludC5jb20vOnU6L2cvcGVyc29uYWwvbm5hbmRhX2Nvbm5lY3RfdXN0X2hrL0VSUk9DRUw1Nm1OUHFTV1d6YnpMajNjQnZRVHRDUzl5em9pVWZWbGhJeDFDQ0E_cnRpbWU9dzZGeG1TM3gyRWc\n",
            "Reusing existing connection to hkustconnect-my.sharepoint.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4865875 (4.6M) [application/x-zip-compressed]\n",
            "Saving to: ‘data_2021_spring.zip’\n",
            "\n",
            "data_2021_spring.zi 100%[===================>]   4.64M  2.03MB/s    in 2.3s    \n",
            "\n",
            "2021-03-27 14:35:45 (2.03 MB/s) - ‘data_2021_spring.zip’ saved [4865875/4865875]\n",
            "\n",
            "Archive:  data_2021_spring.zip\n",
            "   creating: data_2021_spring/\n",
            "  inflating: data_2021_spring/valid.csv  \n",
            "  inflating: __MACOSX/data_2021_spring/._valid.csv  \n",
            "  inflating: data_2021_spring/test.csv  \n",
            "  inflating: __MACOSX/data_2021_spring/._test.csv  \n",
            "  inflating: data_2021_spring/train.csv  \n",
            "  inflating: __MACOSX/data_2021_spring/._train.csv  \n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXnBAO809AOb"
      },
      "source": [
        "# Instructions for Project 1 - Sentiment Classification\n",
        "\n",
        "Hello everyone, this is Zihao. I am very happy to host the first project\n",
        "\n",
        "In this project, you will conduct a sentiment analysis task.\n",
        "You will build a model to predict the scores (a.k.a. stars, from 1-5) of each review.\n",
        "For each review, you are given a piece of text as well as some other features (Explore yourself!).\n",
        "You can consider the predicted variables to be categorical, ordinal or numerical.\n",
        "\n",
        "DDL: *April 6, 2021*\n",
        "- *March 23, 2021* release the validation score of weak baseline\n",
        "- *March 30, 2021* release the validation score of strong baseline\n",
        "\n",
        "Submission: Each team leader is required to submit the groupNo.zip file in the canvas. It shoud contain \n",
        "- `pre.csv` Predictions on test data (please make sure you can successfully evaluate your validation predictions on the validation data with the help of evaluate.py)\n",
        "- report (1-2 pages of pdf)\n",
        "- code (Frameworks and programming languages are not restricted.)\n",
        "\n",
        "We will check your report with your code and the accuracy.\n",
        "\n",
        "| Grade | Classifier (80%)                                                   | Report (20%)                      |\n",
        "|-------|--------------------------------------------------------------------|-----------------------------------|\n",
        "| 50%   | example code in tutorials or in Project 1 without any modification | submission                        |\n",
        "| 60%   | an easy baseline that most students can outperform                 | algorithm you used                |\n",
        "| 80%   | a competitive baseline that about half students can surpass        | detailed explanation              |\n",
        "| 90%   | a very competitive baseline without any special mechanism          | detailed explanation and analysis, such as explorative data analysis and ablation study |\n",
        "| 100%  | a very competitive baseline with at least one mechanism            | excellent ideas, detailed explanation and solid analysis |\n",
        "\n",
        "\n",
        "\n",
        "In this notebook, you are provided with the code snippets for you to start.\n",
        "\n",
        "The content follows previous lectures and tutorials. But I may mention some useful python packages.\n",
        "\n",
        "## Instruction Content\n",
        "\n",
        "1. Load & Dump the data\n",
        "    1. Load the data\n",
        "    1. Dump the data\n",
        "1. Preprocessing\n",
        "    1. Text data processing recap\n",
        "    1. Explorative data analysis\n",
        "1. Learning Baselines\n",
        "\n",
        "## 1. Load & Dump the data\n",
        "\n",
        "The same as previous tutorials, we use `pandas` as the basic tool to load & dump the data.\n",
        "The key ingredient of our operation is the `DataFrame` in pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug0PS99E9AOq"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q15KcEky9AOr"
      },
      "source": [
        "### A. Load the data\n",
        "\n",
        "Here is a function to load your data, remember put the dataset in the `data_2021_spring` folder.\n",
        "\n",
        "Each year we release different data, so old models are not guaranteed to solve the new data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFEJ1aJv9AOs"
      },
      "source": [
        "def load_data(split_name='train', columns=['text', 'stars']):\n",
        "    try:\n",
        "        print(f\"select [{', '.join(columns)}] columns from the {split_name} split\")\n",
        "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
        "        df = df.loc[:,columns]\n",
        "        print(\"succeed!\")\n",
        "        return df\n",
        "    except:\n",
        "        print(\"Failed, then try to \")\n",
        "        print(f\"select all columns from the {split_name} split\")\n",
        "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
        "        return df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i6RoQO49AOs"
      },
      "source": [
        "Then you can extract the data by specifying the desired split and columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP9SWiuv9AOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc106c9-fb83-42b5-fe37-5682cb67f737"
      },
      "source": [
        "train_df = load_data('train', columns=['text'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "select [text] columns from the train split\n",
            "succeed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKH2Jzn89AOt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7cea7aca-673d-4350-bb81-07a81d5af515"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Nice to have a diner still around. Food was go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tried this a while back, got the fried chicken...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I expected more pork selections on menu. Food ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>YUMMY!!! This place is phenomenal. It is Price...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Truffle Macaroni &amp; Cheese and Potatoes Au ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  Nice to have a diner still around. Food was go...\n",
              "1  Tried this a while back, got the fried chicken...\n",
              "2  I expected more pork selections on menu. Food ...\n",
              "3  YUMMY!!! This place is phenomenal. It is Price...\n",
              "4  The Truffle Macaroni & Cheese and Potatoes Au ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOmXTj9K9AOu",
        "outputId": "c5375eb6-cddb-4afc-8b60-72cb140d6ea4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_df = load_data('test')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "select [text, stars] columns from the test split\n",
            "Failed, then try to \n",
            "select all columns from the test split\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p11tWdcb9AOu",
        "outputId": "f9000307-f739-4035-ff3a-bfe4f0c8b00c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(test_df)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbjS-Eqi9AOu"
      },
      "source": [
        "### B. Dump the random answer\n",
        "\n",
        "In this project, your predictions on test data are supposed to be submitted by a csv file of two columns, i.e. (review_id and stars)\n",
        "\n",
        "Here we compose the random answer in a DataFrame and dump the answer into a csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXpTm5789AOv"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgVA8Q2t9AOv"
      },
      "source": [
        "random_ans = pd.DataFrame(data={\n",
        "    'review_id': test_df['review_id'],\n",
        "    'stars': np.random.randint(0, 6, size=len(test_df))\n",
        "})"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbK8s-B19AOv",
        "outputId": "ea4d9f52-acde-4105-b628-377695885f7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "random_ans.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>stars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b8-ELBwhmDKcmcM8icT86g</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>rBpAJhIen_V-zLoXZIcROg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>_pALaDG6se9OTkGGhyhnNA</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ru8fpA1Uk0tTFtO5hLM49g</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fRPgwuFoY6SriToXZyaOQA</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                review_id  stars\n",
              "0  b8-ELBwhmDKcmcM8icT86g      1\n",
              "1  rBpAJhIen_V-zLoXZIcROg      0\n",
              "2  _pALaDG6se9OTkGGhyhnNA      1\n",
              "3  ru8fpA1Uk0tTFtO5hLM49g      2\n",
              "4  fRPgwuFoY6SriToXZyaOQA      2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqRs8e1N9AOv"
      },
      "source": [
        "group_number = -1\n",
        "random_ans.to_csv(f'{group_number}-random_ans.csv', index=False)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-luJpdpr9AOv"
      },
      "source": [
        "## 2. Preprocessing\n",
        "\n",
        "Preprocessing and feature engineering is important in machine learning\n",
        "\n",
        "### A. Text data processing recap\n",
        "In our tutorials, Haoran have showed you how to extract textual features by the `nltk` package\n",
        "\n",
        "Remember to use the NLTK Downloader to obtain the resource:\n",
        "```\n",
        "  >>> import nltk\n",
        "  >>> nltk.download('stopwords')\n",
        "  >>> nltk.download('punkt')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhYLSBbh9AOw"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = set(stopwords.words('english'))\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def lower(s):\n",
        "    \"\"\"\n",
        "    :param s: a string.\n",
        "    return a string with lower characters\n",
        "    Note that we allow the input to be nested string of a list.\n",
        "    e.g.\n",
        "    Input: 'Text mining is to identify useful information.'\n",
        "    Output: 'text mining is to identify useful information.'\n",
        "    \"\"\"\n",
        "    if isinstance(s, list):\n",
        "        return [lower(t) for t in s]\n",
        "    if isinstance(s, str):\n",
        "        return s.lower()\n",
        "    else:\n",
        "        raise NotImplementedError(\"unknown datatype\")\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    :param text: a doc with multiple sentences, type: str\n",
        "    return a word list, type: list\n",
        "    e.g.\n",
        "    Input: 'Text mining is to identify useful information.'\n",
        "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
        "    \"\"\"\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "\n",
        "def stem(tokens):\n",
        "    \"\"\"\n",
        "    :param tokens: a list of tokens, type: list\n",
        "    return a list of stemmed words, type: list\n",
        "    e.g.\n",
        "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
        "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
        "    \"\"\"\n",
        "    ### equivalent code\n",
        "    # results = list()\n",
        "    # for token in tokens:\n",
        "    #     results.append(ps.stem(token))\n",
        "    # return results\n",
        "\n",
        "    return [ps.stem(token) for token in tokens]\n",
        "\n",
        "def n_gram(tokens, n=1):\n",
        "    \"\"\"\n",
        "    :param tokens: a list of tokens, type: list\n",
        "    :param n: the corresponding n-gram, type: int\n",
        "    return a list of n-gram tokens, type: list\n",
        "    e.g.\n",
        "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
        "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
        "    \"\"\"\n",
        "    if n == 1:\n",
        "        return tokens\n",
        "    else:\n",
        "        results = list()\n",
        "        for i in range(len(tokens)-n+1):\n",
        "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
        "            results.append(\" \".join(tokens[i:i+n]))\n",
        "        return results\n",
        "\n",
        "def filter_stopwords(tokens):\n",
        "    \"\"\"\n",
        "    :param tokens: a list of tokens, type: list\n",
        "    return a list of filtered tokens, type: list\n",
        "    e.g.\n",
        "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
        "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
        "    \"\"\"\n",
        "    ### equivalent code\n",
        "    # results = list()\n",
        "    # for token in tokens:\n",
        "    #     if token not in stopwords and not token.isnumeric():\n",
        "    #         results.append(token)\n",
        "    # return results\n",
        "\n",
        "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def get_onehot_vector(feats, feats_dict):\n",
        "    \"\"\"\n",
        "    :param data: a list of features, type: list\n",
        "    :param feats_dict: a dict from features to indices, type: dict\n",
        "    return a feature vector,\n",
        "    \"\"\"\n",
        "    # initialize the vector as all zeros\n",
        "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
        "    for f in feats:\n",
        "        # get the feature index, return -1 if the feature is not existed\n",
        "        f_idx = feats_dict.get(f, -1)\n",
        "        if f_idx != -1:\n",
        "            # set the corresponding element as 1\n",
        "            vector[f_idx] = 1\n",
        "    return vector"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXGfXEpjCSDV"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dekgQGHT9AOw"
      },
      "source": [
        "Note that you can use the `map` function to apply your preprocessing functions into the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Auz1_Y2U9AOw",
        "outputId": "15990d14-18d1-400d-acd0-f7ef78d5e14e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_df['tokens'] = test_df['text'].map(tokenize).map(filter_stopwords).map(lower)\n",
        "print(test_df['tokens'].head().to_string())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    [i, took, up, train, union, station, catch, ai...\n",
            "1    [we, worked, fitness, twist, part, best, frien...\n",
            "2    [it, 's, typical, ,, average, ,, run-of-the-mi...\n",
            "3    [we, went, outback, today, celebrate, daughter...\n",
            "4    [we, went, see, nashville, unplugged, country,...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nuf78xEW9AOx"
      },
      "source": [
        "Besides `nltk`, I would like to introduce `SpaCy`, a newer text processing toolkit of industrial strength.\n",
        "\n",
        "You can explore it at https://spacy.io/\n",
        "\n",
        "Let's install it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX201ILd9AOx"
      },
      "source": [
        "```bash\n",
        "python -m pip install spacy\n",
        "python -m spacy download en_core_web_sm\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mf62BhX9AOx"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5TClnyt9AOx"
      },
      "source": [
        "SpaCy enables you use linguistic features of texts\n",
        "\n",
        "Example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd029IwJ9AOx",
        "outputId": "ce02b52e-cc93-46a9-d236-2a75d90aafa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "fmt = \"{:10s},\\t \" * 8\n",
        "print(fmt.format('raw', 'stem', 'PartOfSpeech', 'dependency', 'shape', 'is alpha', 'is stop', 'its childrens in the parsing tree'))\n",
        "print('-'*140)\n",
        "for token in doc:\n",
        "    print(fmt.format(token.text, token.lemma_, token.pos_, token.dep_,\n",
        "            token.shape_, str(token.is_alpha), str(token.is_stop), str(list(token.children))))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw       ,\t stem      ,\t PartOfSpeech,\t dependency,\t shape     ,\t is alpha  ,\t is stop   ,\t its childrens in the parsing tree,\t \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Apple     ,\t Apple     ,\t PROPN     ,\t nsubj     ,\t Xxxxx     ,\t True      ,\t False     ,\t []        ,\t \n",
            "is        ,\t be        ,\t AUX       ,\t aux       ,\t xx        ,\t True      ,\t True      ,\t []        ,\t \n",
            "looking   ,\t look      ,\t VERB      ,\t ROOT      ,\t xxxx      ,\t True      ,\t False     ,\t [Apple, is, at],\t \n",
            "at        ,\t at        ,\t ADP       ,\t prep      ,\t xx        ,\t True      ,\t True      ,\t [buying]  ,\t \n",
            "buying    ,\t buy       ,\t VERB      ,\t pcomp     ,\t xxxx      ,\t True      ,\t False     ,\t [startup, for],\t \n",
            "U.K.      ,\t U.K.      ,\t PROPN     ,\t compound  ,\t X.X.      ,\t False     ,\t False     ,\t []        ,\t \n",
            "startup   ,\t startup   ,\t NOUN      ,\t dobj      ,\t xxxx      ,\t True      ,\t False     ,\t [U.K.]    ,\t \n",
            "for       ,\t for       ,\t ADP       ,\t prep      ,\t xxx       ,\t True      ,\t True      ,\t [billion] ,\t \n",
            "$         ,\t $         ,\t SYM       ,\t quantmod  ,\t $         ,\t False     ,\t False     ,\t []        ,\t \n",
            "1         ,\t 1         ,\t NUM       ,\t compound  ,\t d         ,\t False     ,\t False     ,\t []        ,\t \n",
            "billion   ,\t billion   ,\t NUM       ,\t pobj      ,\t xxxx      ,\t True      ,\t False     ,\t [$, 1]    ,\t \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3uMcKJ0DTBI"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3vRHO5I9AOy"
      },
      "source": [
        "SpaCy also allows you use the embeddings for both sentence and words\n",
        "\n",
        "Example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ2SXIwe9AOy",
        "outputId": "5b42884b-b693-47b3-bdee-13f8af3e5928",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(doc, doc.vector[:5], '...')\n",
        "for t in doc:\n",
        "    print(t, t.vector[:5], '...')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple is looking at buying U.K. startup for $1 billion [1.0188444  0.03571144 0.7764361  1.2014427  0.1412878 ] ...\n",
            "Apple [-1.7419337   0.91661656 -0.4636941   4.388701    3.652168  ] ...\n",
            "is [-0.8291022  2.7585855 -4.7504234 -1.7227181  0.7249189] ...\n",
            "looking [ 7.6297693   0.37686205  0.23439455 -0.09577817 -0.13549986] ...\n",
            "at [ 3.2920957  1.7845521  1.8076079 -1.7595236 -0.2605796] ...\n",
            "buying [ 1.9203172   0.36557692 -1.1916409   5.1476283  -0.06492996] ...\n",
            "U.K. [ 1.984084  -3.100997  -2.9799771  1.951805   1.8670516] ...\n",
            "startup [-0.67022705  1.8172026  -0.32442284  2.189914   -1.3265724 ] ...\n",
            "for [ 4.227348    1.4054016   6.191574   -1.134153   -0.43347907] ...\n",
            "$ [-2.1446993 -3.972844   3.802413   3.7863498 -1.4913919] ...\n",
            "1 [-0.9251789  -1.5521184   3.2898462  -1.0225594  -0.40639713] ...\n",
            "billion [-1.535186  -0.4060119  2.9251206  1.4862046 -0.5711231] ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B0mnB5F9AOy"
      },
      "source": [
        "For more usage of SpaCy, you can refer to the documentation of spacy https://spacy.io/usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrePxBEL9AOy"
      },
      "source": [
        "### B. Explorative data analysis\n",
        "\n",
        "For our dataset, we have features more than text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF54MBSy9AOz",
        "outputId": "54c7dc7c-f736-4199-dcd3-00e2d82b4c73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_df_full = load_data('train', columns='full')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "select [f, u, l, l] columns from the train split\n",
            "Failed, then try to \n",
            "select all columns from the train split\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27muOFSg9AOz",
        "outputId": "a47d02cc-c6c5-4205-a005-6af52db2e2ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_df_full.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>cool</th>\n",
              "      <th>date</th>\n",
              "      <th>funny</th>\n",
              "      <th>review_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>text</th>\n",
              "      <th>useful</th>\n",
              "      <th>user_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39rLHYJOy2774ZIUouuWLw</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-06-28 21:44:02</td>\n",
              "      <td>0</td>\n",
              "      <td>ynzOFepQYSCDGdfWDWxiZw</td>\n",
              "      <td>4</td>\n",
              "      <td>Nice to have a diner still around. Food was go...</td>\n",
              "      <td>0</td>\n",
              "      <td>Sl6VgFOB-XXfFIAYp7TFkw</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>E-Kq1Yu1d6N3TL2qX0aqjA</td>\n",
              "      <td>0</td>\n",
              "      <td>2018-04-09 03:45:19</td>\n",
              "      <td>0</td>\n",
              "      <td>sQX9ncJBEdBf16AWsvO6Vg</td>\n",
              "      <td>2</td>\n",
              "      <td>Tried this a while back, got the fried chicken...</td>\n",
              "      <td>0</td>\n",
              "      <td>gcx01pMqWzkni2UC-zoZrA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>nWW6fBfBljiRFa4sG7TyxA</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-07-19 02:00:04</td>\n",
              "      <td>0</td>\n",
              "      <td>bVIf2kqbzvif3miNe3ARNw</td>\n",
              "      <td>4</td>\n",
              "      <td>I expected more pork selections on menu. Food ...</td>\n",
              "      <td>0</td>\n",
              "      <td>Mn9VzPbrCYU4EcP_C1oBOg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>qmIHO-6T_KEfPC9jyGDamQ</td>\n",
              "      <td>0</td>\n",
              "      <td>2011-11-11 08:10:24</td>\n",
              "      <td>0</td>\n",
              "      <td>LNj1OFxy2ool3PZANGchPA</td>\n",
              "      <td>4</td>\n",
              "      <td>YUMMY!!! This place is phenomenal. It is Price...</td>\n",
              "      <td>0</td>\n",
              "      <td>SKV1heo00fdciCbCN9Z33A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pKk7jCFIm96qDdk0laVT2w</td>\n",
              "      <td>1</td>\n",
              "      <td>2010-01-16 20:04:00</td>\n",
              "      <td>1</td>\n",
              "      <td>bZXxa0hO6wQlHD-MkMf4iw</td>\n",
              "      <td>5</td>\n",
              "      <td>The Truffle Macaroni &amp; Cheese and Potatoes Au ...</td>\n",
              "      <td>1</td>\n",
              "      <td>p1r7rZYruZR92x1A649PTQ</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              business_id  cool  ... useful                 user_id\n",
              "0  39rLHYJOy2774ZIUouuWLw     0  ...      0  Sl6VgFOB-XXfFIAYp7TFkw\n",
              "1  E-Kq1Yu1d6N3TL2qX0aqjA     0  ...      0  gcx01pMqWzkni2UC-zoZrA\n",
              "2  nWW6fBfBljiRFa4sG7TyxA     0  ...      0  Mn9VzPbrCYU4EcP_C1oBOg\n",
              "3  qmIHO-6T_KEfPC9jyGDamQ     0  ...      0  SKV1heo00fdciCbCN9Z33A\n",
              "4  pKk7jCFIm96qDdk0laVT2w     1  ...      1  p1r7rZYruZR92x1A649PTQ\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgxCOol69AOz"
      },
      "source": [
        "You can explore the relationship between different features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5eIO0i59AOz"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ejnqkMl9AOz",
        "outputId": "f54f389d-5fb0-4a5c-d2a8-aa954686e654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.scatter(train_df_full['cool'], train_df_full['funny'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f793dd4b150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE_I7piQ9AOz",
        "outputId": "ddde020c-1316-4f8e-b3ef-c293d2e6c5ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "plt.hist(train_df_full['stars'], bins=5)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2534., 1354., 1888., 2110., 2114.]),\n",
              " array([1. , 1.8, 2.6, 3.4, 4.2, 5. ]),\n",
              " <a list of 5 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT4NYbZG9AO0"
      },
      "source": [
        "Moreover, you may use the id feature to aggregate data samples\n",
        "\n",
        "For example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk-n7Fmq9AO0",
        "outputId": "ea4f0914-94b3-470a-fd72-36ad5d0ad59c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "for bid, sub_df in train_df_full.groupby('business_id'):\n",
        "    if len(sub_df) > 1:\n",
        "        print(sub_df[['business_id', 'funny', 'cool', 'stars']].head())\n",
        "        plt.hist(sub_df['stars'], bins=5)\n",
        "        break"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 business_id  funny  cool  stars\n",
            "7043  -0qht1roIqleKiQkBLDkbw      1     0      3\n",
            "7363  -0qht1roIqleKiQkBLDkbw      0     0      5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vihj9FT9AO1",
        "outputId": "55f72f68-5d72-4fc0-8c38-47d5ac95e941",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for bid, sub_df in train_df_full.groupby('user_id'):\n",
        "    if len(sub_df) > 1:\n",
        "        print(sub_df[['user_id', 'funny', 'cool', 'stars']].head())\n",
        "        break"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                     user_id  funny  cool  stars\n",
            "1173  -SjQXQd-IRfOdUdYYwWGOQ      0     1      4\n",
            "4503  -SjQXQd-IRfOdUdYYwWGOQ      0     0      1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8AQAUH79AO1"
      },
      "source": [
        "## 3. Baselines\n",
        "\n",
        "Finally, we come up with two baselines for you to refer.\n",
        "We only use text data here and only consider first 5k training samples.\n",
        "\n",
        "For example, a baseline can be a logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaQHCMJV9AO1",
        "outputId": "8fa6d109-975c-482a-8f28-c6b2411bc41c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_df = load_data('train')[:5000]\n",
        "valid_df = load_data('valid')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "select [text, stars] columns from the train split\n",
            "succeed!\n",
            "select [text, stars] columns from the valid split\n",
            "succeed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzNMVlj_9AO1"
      },
      "source": [
        "The split above is what we have done for you. You can use the data as you like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpYN_3Uq9AO1"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krB_GACB9AO1"
      },
      "source": [
        "x_train = train_df['text']\n",
        "y_train = train_df['stars']"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub-ZIHou9AO2",
        "outputId": "f44cebab-fe4a-4788-a3a8-3c0a10879163",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tfidf = TfidfVectorizer(tokenizer=tokenize)\n",
        "lr = LogisticRegression()\n",
        "steps = [('tfidf', tfidf),('lr', lr)]\n",
        "pipe = Pipeline(steps)\n",
        "print(pipe)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pipeline(memory=None,\n",
            "         steps=[('tfidf',\n",
            "                 TfidfVectorizer(analyzer='word', binary=False,\n",
            "                                 decode_error='strict',\n",
            "                                 dtype=<class 'numpy.float64'>,\n",
            "                                 encoding='utf-8', input='content',\n",
            "                                 lowercase=True, max_df=1.0, max_features=None,\n",
            "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
            "                                 preprocessor=None, smooth_idf=True,\n",
            "                                 stop_words=None, strip_accents=None,\n",
            "                                 sublinear_tf=False,\n",
            "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                                 tokenizer=<function tokenize at 0x7f79940f3560>,\n",
            "                                 use_idf=True, vocabulary=None)),\n",
            "                ('lr',\n",
            "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
            "                                    fit_intercept=True, intercept_scaling=1,\n",
            "                                    l1_ratio=None, max_iter=100,\n",
            "                                    multi_class='auto', n_jobs=None,\n",
            "                                    penalty='l2', random_state=None,\n",
            "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                                    warm_start=False))],\n",
            "         verbose=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywXvoAn_9AO2",
        "outputId": "45ceafab-4282-4304-ff55-1515cb5900ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pipe.fit(x_train, y_train)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=<function tokenize at 0x7f79940f3560>,\n",
              "                                 use_idf=True, vocabulary=None)),\n",
              "                ('lr',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNfdA8xi9AO4",
        "outputId": "cf160f7a-715e-4b05-df37-d9b63eb1463c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x_valid = valid_df['text']\n",
        "y_valid = valid_df['stars']\n",
        "y_pred = pipe.predict(x_valid)\n",
        "print(classification_report(y_valid, y_pred))\n",
        "print(\"\\n\\n\")\n",
        "print(confusion_matrix(y_valid, y_pred))\n",
        "print('accuracy', np.mean(y_valid == y_pred))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.65      0.88      0.75       517\n",
            "           2       0.41      0.13      0.19       278\n",
            "           3       0.44      0.46      0.45       344\n",
            "           4       0.49      0.51      0.50       427\n",
            "           5       0.70      0.67      0.68       434\n",
            "\n",
            "    accuracy                           0.58      2000\n",
            "   macro avg       0.54      0.53      0.52      2000\n",
            "weighted avg       0.56      0.58      0.55      2000\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[[456  21  21  13   6]\n",
            " [120  35  95  23   5]\n",
            " [ 65  21 159  89  10]\n",
            " [ 23   6  79 217 102]\n",
            " [ 33   3  10  98 290]]\n",
            "accuracy 0.5785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD_poAnW9AO4"
      },
      "source": [
        "Of course, you can use deep learning.\n",
        "Here is a pytorch based baseline using CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sdX-3RX9AO4"
      },
      "source": [
        "```bash\n",
        "pip install torch\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8PKseV49AO4"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import tqdm"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMLeCgKO9AO4"
      },
      "source": [
        "train_text = train_df['text'].map(tokenize).map(filter_stopwords).map(stem)\n",
        "valid_text = valid_df['text'].map(tokenize).map(filter_stopwords).map(stem)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhh0J20m9AO4"
      },
      "source": [
        "word2id = {}\n",
        "for tokens in train_text:\n",
        "    for t in tokens:\n",
        "        if not t in word2id:\n",
        "            word2id[t] = len(word2id)\n",
        "word2id['<pad>'] = len(word2id)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G4kD_GE9AO5"
      },
      "source": [
        "def texts_to_id_seq(texts, padding_length=500):\n",
        "    records = []\n",
        "    for tokens in texts:\n",
        "        record = []\n",
        "        for t in tokens:\n",
        "            record.append(word2id.get(t, len(word2id)))\n",
        "        if len(record) >= padding_length:\n",
        "            records.append(record[:padding_length])\n",
        "        else:\n",
        "            records.append(record + [word2id['<pad>']] * (padding_length - len(record)))\n",
        "    return records"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B0SDYA-9AO5"
      },
      "source": [
        "train_seqs = texts_to_id_seq(train_text)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7P3C_3Z9AO5"
      },
      "source": [
        "valid_seqs = texts_to_id_seq(valid_text)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYv0-Piy9AO5"
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, seq, y):\n",
        "        assert len(seq) == len(y)\n",
        "        self.seq = seq\n",
        "        self.y = y-1\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return np.asarray(self.seq[idx]), self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seq)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpeoevZH9AO5"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(MyDataset(train_seqs, y_train), batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(MyDataset(valid_seqs, y_valid), batch_size=batch_size)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSROZU2d9AO5"
      },
      "source": [
        "class mlp(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(mlp, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=len(word2id)+1, embedding_dim=64)\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=64,\n",
        "                      out_channels=64,\n",
        "                      kernel_size=3,\n",
        "                      stride=1),\n",
        "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(in_channels=64,\n",
        "                      out_channels=64,\n",
        "                      kernel_size=3,\n",
        "                      stride=1),\n",
        "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.linear = nn.Linear(64, 5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = torch.transpose(x, 1, 2)\n",
        "        x = self.cnn(x)\n",
        "        x = torch.max(x, dim=-1)[0]\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO0MyAC19AO6"
      },
      "source": [
        "model = mlp()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "_XwoH9xN9AO6"
      },
      "source": [
        "# for e in range(1, 11):    \n",
        "#     print('epoch', e)\n",
        "#     model.train()\n",
        "#     total_acc = 0\n",
        "#     total_loss = 0\n",
        "#     total_count = 0\n",
        "#     with tqdm.tqdm(train_loader) as t:\n",
        "#         for x, y in t:\n",
        "#             optimizer.zero_grad()\n",
        "#             logits = model(x)\n",
        "#             loss = criterion(logits, y)\n",
        "#             loss.backward()\n",
        "#             total_acc += (logits.argmax(1) == y).sum().item()\n",
        "#             total_count += y.size(0)\n",
        "#             total_loss += loss.item()\n",
        "#             optimizer.step()\n",
        "#             t.set_postfix({'loss': total_loss/total_count, 'acc': total_acc/total_count})\n",
        "\n",
        "#     model.eval()\n",
        "#     y_pred = []\n",
        "#     y_true = []\n",
        "#     with tqdm.tqdm(valid_loader) as t:\n",
        "#         for x, y in t:\n",
        "#             logits = model(x)\n",
        "#             total_acc += (logits.argmax(1) == y).sum().item()\n",
        "#             total_count += len(y)\n",
        "#             y_pred += logits.argmax(1).tolist()\n",
        "#             y_true += y.tolist()\n",
        "#     print(classification_report(y_true, y_pred))\n",
        "#     print(\"\\n\\n\")\n",
        "#     print(confusion_matrix(y_true, y_pred))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIkmkLkf9AO6"
      },
      "source": [
        "Deep learning are full of tricks. \n",
        "\n",
        "In the second example above, the implementation of CNN is not good enough to beat even TFIDF+Logistic regression.\n",
        "\n",
        "You can use all the techniques introduced in the lectures and tutorials to enhance your methods.\n",
        "\n",
        "Of course, you can use ideas have not been mentioned to make your model distinguished."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZNOohZdWqJM"
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duPjGtlb9AO6",
        "outputId": "08c93b58-f67e-46ab-ff3f-c8a95835f67c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "#vocabulary_size = 5000\n",
        "#(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
        "\n",
        "# print(\"X_train : \",X_train)\n",
        "# print(\"y_train : \",y_train)\n",
        "# print(\"x_test : \",X_test)\n",
        "#print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))\n",
        "\n",
        "# pre processing data\n",
        "from string import punctuation\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "#all_text = ''.join([c for c in train_df['text'] if c not in punctuation])\n",
        "#print(\"all_text : \",len(all_text))\n",
        "#print(all_text[:3])\n",
        "\n",
        "def encodeWords(df):\n",
        "  all_text2 = ' '.join([c for c in df])\n",
        "  # create a list of words\n",
        "  words = all_text2.split()\n",
        "  # Count all the words using Counter Method\n",
        "  count_words = Counter(words)\n",
        "\n",
        "  total_words = len(words)\n",
        "  sorted_words = count_words.most_common(total_words)\n",
        "  print(total_words,sorted_words)\n",
        "\n",
        "\n",
        "  vocab = sorted(count_words, key=count_words.get, reverse=True)\n",
        "  print(vocab)\n",
        "\n",
        "  vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
        "\n",
        "  reviews_ints = [] \n",
        "  for review in df:     \n",
        "    reviews_ints.append([vocab_to_int[word] for word in review.split()])\n",
        "  return reviews_ints\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "max_features = 1000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer.fit_on_texts(train_df['text'].values)\n",
        "X_train = tokenizer.texts_to_sequences(train_df['text'].values)\n",
        "X_train = pad_sequences(X_train)\n",
        "print(X_train)\n",
        "\n",
        "# X_train = encodeWords(train_df[\"text\"])\n",
        "# y_train = train_df[\"stars\"] \n",
        "\n",
        "# X_test = encodeWords(test_df[\"text\"])\n",
        "# y_test = y_valid\n",
        "\n",
        "# review_lens = Counter([len(x) for x in X_train])\n",
        "# print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
        "# print(\"Maximum review length: {}\".format(max(review_lens)))\n",
        "# y_review_lens = Counter([len(x) for x in X_test])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def pad_features(reviews_ints, seq_length):\n",
        "#   ''' Return features of review_ints, where each review is padded with 0's\n",
        "#   or truncated to the input seq_length.\n",
        "#   '''\n",
        "#   # getting the correct rows x cols shape\n",
        "#   features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
        "\n",
        "#   # for each review, I grab that review and\n",
        "#   for i, row in enumerate(reviews_ints):\n",
        "#   features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "#   return features\n",
        "\n",
        "\n",
        "\n",
        "# print('---review---')\n",
        "# print(X_train[6])\n",
        "# print('---label---')\n",
        "# print(y_train[6])\n",
        "\n",
        "\n",
        "# word2id = imdb.get_word_index()\n",
        "# id2word = {i: word for word, i in word2id.items()}\n",
        "# print('---review with words---')\n",
        "# print([id2word.get(i, ' ') for i in X_train[6]])\n",
        "# print('---label---')\n",
        "# print(y_train[6])\n",
        "\n",
        "\n",
        "# RNN model\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "# pad sequence\n",
        "max_words = max_features\n",
        "#X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
        "#X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
        "\n",
        "# define RNN model\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout,SpatialDropout1D, GRU\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "embedding_size=256\n",
        "model=Sequential()\n",
        "model.add(Embedding(max_features, embedding_size, input_length=X_train.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(GRU(256, return_sequences=True))\n",
        "model.add(LSTM(196,dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "# compile\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "             optimizer='adam', \n",
        "             metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# train\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_train,to_categorical(train_df[\"stars\"].values), test_size = 0.33, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)\n",
        "\n",
        "model.fit(X_train, Y_train, epochs = 7, batch_size=batch_size)\n",
        "\n",
        "# X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
        "# X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
        "# model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "# predict\n",
        "\n",
        "validation_size = 1500\n",
        "\n",
        "X_validate = X_test[-validation_size:]\n",
        "Y_validate = Y_test[-validation_size:]\n",
        "X_test = X_test[:-validation_size]\n",
        "Y_test = Y_test[:-validation_size]\n",
        "score,acc = model.evaluate(X_test, Y_test, verbose = 0, batch_size = batch_size)\n",
        "print(\"score: %.2f\" % (score))\n",
        "print(\"acc: %.2f\" % (acc))\n",
        "\n",
        "# scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "# print('Test accuracy:', scores[1])\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   0 ... 365  10 326]\n",
            " [  0   0   0 ...  59  13  92]\n",
            " [  0   0   0 ...   4 706 160]\n",
            " ...\n",
            " [  0   0   0 ...  26  73 414]\n",
            " [  0   0   0 ... 113  68  38]\n",
            " [  0   0   0 ... 130  83 405]]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 849, 256)          256000    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d (SpatialDr (None, 849, 256)          0         \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 849, 256)          394752    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 196)               355152    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 1182      \n",
            "=================================================================\n",
            "Total params: 1,007,086\n",
            "Trainable params: 1,007,086\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "(3350, 849) (3350, 6)\n",
            "(1650, 849) (1650, 6)\n",
            "Epoch 1/7\n",
            "105/105 [==============================] - 314s 3s/step - loss: 1.6188 - accuracy: 0.2858\n",
            "Epoch 2/7\n",
            "105/105 [==============================] - 278s 3s/step - loss: 1.2694 - accuracy: 0.4611\n",
            "Epoch 3/7\n",
            "105/105 [==============================] - 277s 3s/step - loss: 1.0746 - accuracy: 0.5680\n",
            "Epoch 4/7\n",
            "105/105 [==============================] - 277s 3s/step - loss: 0.9801 - accuracy: 0.6125\n",
            "Epoch 5/7\n",
            "105/105 [==============================] - 278s 3s/step - loss: 0.8781 - accuracy: 0.6411\n",
            "Epoch 6/7\n",
            "105/105 [==============================] - 278s 3s/step - loss: 0.7876 - accuracy: 0.6926\n",
            "Epoch 7/7\n",
            "105/105 [==============================] - 278s 3s/step - loss: 0.6916 - accuracy: 0.7355\n",
            "score: 1.09\n",
            "acc: 0.56\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}