{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nikhilnanda21/RMBI4310-COMP4332-Project/blob/jamie/Project1/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAckpZDZG-BR"
   },
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_gEmiUAEG8zx",
    "outputId": "93935444-0f53-47bf-809a-2a1a5a30b9ac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jamie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jamie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !wget https://hkustconnect-my.sharepoint.com/:u:/g/personal/nnanda_connect_ust_hk/ERROCEL56mNPqSWWzbzLj3cBvQTtCS9yzoiUfVlhIx1CCA?download=1 -O \"data_2021_spring.zip\"\n",
    "# !unzip \"data_2021_spring.zip\"\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXnBAO809AOb"
   },
   "source": [
    "# Instructions for Project 1 - Sentiment Classification\n",
    "\n",
    "Hello everyone, this is Zihao. I am very happy to host the first project\n",
    "\n",
    "In this project, you will conduct a sentiment analysis task.\n",
    "You will build a model to predict the scores (a.k.a. stars, from 1-5) of each review.\n",
    "For each review, you are given a piece of text as well as some other features (Explore yourself!).\n",
    "You can consider the predicted variables to be categorical, ordinal or numerical.\n",
    "\n",
    "DDL: *April 6, 2021*\n",
    "- *March 23, 2021* release the validation score of weak baseline\n",
    "- *March 30, 2021* release the validation score of strong baseline\n",
    "\n",
    "Submission: Each team leader is required to submit the groupNo.zip file in the canvas. It shoud contain \n",
    "- `pre.csv` Predictions on test data (please make sure you can successfully evaluate your validation predictions on the validation data with the help of evaluate.py)\n",
    "- report (1-2 pages of pdf)\n",
    "- code (Frameworks and programming languages are not restricted.)\n",
    "\n",
    "We will check your report with your code and the accuracy.\n",
    "\n",
    "| Grade | Classifier (80%)                                                   | Report (20%)                      |\n",
    "|-------|--------------------------------------------------------------------|-----------------------------------|\n",
    "| 50%   | example code in tutorials or in Project 1 without any modification | submission                        |\n",
    "| 60%   | an easy baseline that most students can outperform                 | algorithm you used                |\n",
    "| 80%   | a competitive baseline that about half students can surpass        | detailed explanation              |\n",
    "| 90%   | a very competitive baseline without any special mechanism          | detailed explanation and analysis, such as explorative data analysis and ablation study |\n",
    "| 100%  | a very competitive baseline with at least one mechanism            | excellent ideas, detailed explanation and solid analysis |\n",
    "\n",
    "\n",
    "\n",
    "In this notebook, you are provided with the code snippets for you to start.\n",
    "\n",
    "The content follows previous lectures and tutorials. But I may mention some useful python packages.\n",
    "\n",
    "## Instruction Content\n",
    "\n",
    "1. Load & Dump the data\n",
    "    1. Load the data\n",
    "    1. Dump the data\n",
    "1. Preprocessing\n",
    "    1. Text data processing recap\n",
    "    1. Explorative data analysis\n",
    "1. Learning Baselines\n",
    "\n",
    "## 1. Load & Dump the data\n",
    "\n",
    "The same as previous tutorials, we use `pandas` as the basic tool to load & dump the data.\n",
    "The key ingredient of our operation is the `DataFrame` in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ug0PS99E9AOq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q15KcEky9AOr"
   },
   "source": [
    "### A. Load the data\n",
    "\n",
    "Here is a function to load your data, remember put the dataset in the `data_2021_spring` folder.\n",
    "\n",
    "Each year we release different data, so old models are not guaranteed to solve the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "jFEJ1aJv9AOs"
   },
   "outputs": [],
   "source": [
    "def load_data(split_name='train', columns=['text', 'stars']):\n",
    "    try:\n",
    "        print(f\"select [{', '.join(columns)}] columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
    "        df = df.loc[:,columns]\n",
    "        print(\"succeed!\")\n",
    "        return df\n",
    "    except:\n",
    "        print(\"Failed, then try to \")\n",
    "        print(f\"select all columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i6RoQO49AOs"
   },
   "source": [
    "Then you can extract the data by specifying the desired split and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YP9SWiuv9AOs",
    "outputId": "1cc106c9-fb83-42b5-fe37-5682cb67f737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text] columns from the train split\n",
      "succeed!\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data('train', columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "IKH2Jzn89AOt",
    "outputId": "7cea7aca-673d-4350-bb81-07a81d5af515"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nice to have a diner still around. Food was go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tried this a while back, got the fried chicken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I expected more pork selections on menu. Food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YUMMY!!! This place is phenomenal. It is Price...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Truffle Macaroni &amp; Cheese and Potatoes Au ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Nice to have a diner still around. Food was go...\n",
       "1  Tried this a while back, got the fried chicken...\n",
       "2  I expected more pork selections on menu. Food ...\n",
       "3  YUMMY!!! This place is phenomenal. It is Price...\n",
       "4  The Truffle Macaroni & Cheese and Potatoes Au ..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VOmXTj9K9AOu",
    "outputId": "c5375eb6-cddb-4afc-8b60-72cb140d6ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, stars] columns from the test split\n",
      "Failed, then try to \n",
      "select all columns from the test split\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7YYrZ9LgjpKLTtF-huhJug</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-04 21:21:45</td>\n",
       "      <td>0</td>\n",
       "      <td>b8-ELBwhmDKcmcM8icT86g</td>\n",
       "      <td>I took the UP Train to Union Station to catch ...</td>\n",
       "      <td>0</td>\n",
       "      <td>9Lglv-v8SRo_S-IyvFBmbw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gyNixTgp1yFX97soBZpZ7Q</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-07-10 00:04:01</td>\n",
       "      <td>0</td>\n",
       "      <td>rBpAJhIen_V-zLoXZIcROg</td>\n",
       "      <td>We worked with Fitness with a Twist for part o...</td>\n",
       "      <td>1</td>\n",
       "      <td>zIl62G84XT2BwSIAjjjvYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vNWfQrQCa_XijstJbylcDQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-10-28 01:23:21</td>\n",
       "      <td>2</td>\n",
       "      <td>_pALaDG6se9OTkGGhyhnNA</td>\n",
       "      <td>It's your typical, average, run-of-the-mill co...</td>\n",
       "      <td>1</td>\n",
       "      <td>WP7FsUsgNW24s7HH5xi7pg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wfxmuA7LbKZKVLV58EiWBw</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-11-19 03:48:40</td>\n",
       "      <td>0</td>\n",
       "      <td>ru8fpA1Uk0tTFtO5hLM49g</td>\n",
       "      <td>We went to Outback today to celebrate my daugh...</td>\n",
       "      <td>0</td>\n",
       "      <td>yLSj54f2YgGQu-lhPIhMTQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5jTmjxb1X34EfcY1gos4tw</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-04 23:29:46</td>\n",
       "      <td>0</td>\n",
       "      <td>fRPgwuFoY6SriToXZyaOQA</td>\n",
       "      <td>We Went to see Nashville unplugged a country c...</td>\n",
       "      <td>1</td>\n",
       "      <td>73-u0a3G9Le4GWG7zLYWtg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                 date  funny  \\\n",
       "0  7YYrZ9LgjpKLTtF-huhJug     0  2018-04-04 21:21:45      0   \n",
       "1  gyNixTgp1yFX97soBZpZ7Q     1  2013-07-10 00:04:01      0   \n",
       "2  vNWfQrQCa_XijstJbylcDQ     1  2015-10-28 01:23:21      2   \n",
       "3  wfxmuA7LbKZKVLV58EiWBw     0  2015-11-19 03:48:40      0   \n",
       "4  5jTmjxb1X34EfcY1gos4tw     0  2016-06-04 23:29:46      0   \n",
       "\n",
       "                review_id                                               text  \\\n",
       "0  b8-ELBwhmDKcmcM8icT86g  I took the UP Train to Union Station to catch ...   \n",
       "1  rBpAJhIen_V-zLoXZIcROg  We worked with Fitness with a Twist for part o...   \n",
       "2  _pALaDG6se9OTkGGhyhnNA  It's your typical, average, run-of-the-mill co...   \n",
       "3  ru8fpA1Uk0tTFtO5hLM49g  We went to Outback today to celebrate my daugh...   \n",
       "4  fRPgwuFoY6SriToXZyaOQA  We Went to see Nashville unplugged a country c...   \n",
       "\n",
       "   useful                 user_id  \n",
       "0       0  9Lglv-v8SRo_S-IyvFBmbw  \n",
       "1       1  zIl62G84XT2BwSIAjjjvYw  \n",
       "2       1  WP7FsUsgNW24s7HH5xi7pg  \n",
       "3       0  yLSj54f2YgGQu-lhPIhMTQ  \n",
       "4       1  73-u0a3G9Le4GWG7zLYWtg  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = load_data('test')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p11tWdcb9AOu",
    "outputId": "f9000307-f739-4035-ff3a-bfe4f0c8b00c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbjS-Eqi9AOu"
   },
   "source": [
    "### B. Dump the random answer\n",
    "\n",
    "In this project, your predictions on test data are supposed to be submitted by a csv file of two columns, i.e. (review_id and stars)\n",
    "\n",
    "Here we compose the random answer in a DataFrame and dump the answer into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "GXpTm5789AOv"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "mgVA8Q2t9AOv"
   },
   "outputs": [],
   "source": [
    "random_ans = pd.DataFrame(data={\n",
    "    'review_id': test_df['review_id'],\n",
    "    'stars': np.random.randint(0, 6, size=len(test_df))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "QbK8s-B19AOv",
    "outputId": "ea4d9f52-acde-4105-b628-377695885f7a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b8-ELBwhmDKcmcM8icT86g</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rBpAJhIen_V-zLoXZIcROg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_pALaDG6se9OTkGGhyhnNA</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ru8fpA1Uk0tTFtO5hLM49g</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fRPgwuFoY6SriToXZyaOQA</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id  stars\n",
       "0  b8-ELBwhmDKcmcM8icT86g      5\n",
       "1  rBpAJhIen_V-zLoXZIcROg      2\n",
       "2  _pALaDG6se9OTkGGhyhnNA      4\n",
       "3  ru8fpA1Uk0tTFtO5hLM49g      5\n",
       "4  fRPgwuFoY6SriToXZyaOQA      2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_ans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "EqRs8e1N9AOv"
   },
   "outputs": [],
   "source": [
    "group_number = -1\n",
    "random_ans.to_csv(f'{group_number}-random_ans.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-luJpdpr9AOv"
   },
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "Preprocessing and feature engineering is important in machine learning\n",
    "\n",
    "### A. Text data processing recap\n",
    "In our tutorials, Haoran have showed you how to extract textual features by the `nltk` package\n",
    "\n",
    "Remember to use the NLTK Downloader to obtain the resource:\n",
    "```\n",
    "  >>> import nltk\n",
    "  >>> nltk.download('stopwords')\n",
    "  >>> nltk.download('punkt')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "id": "QhYLSBbh9AOw"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def lower(s):\n",
    "    \"\"\"\n",
    "    :param s: a string.\n",
    "    return a string with lower characters\n",
    "    Note that we allow the input to be nested string of a list.\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: 'text mining is to identify useful information.'\n",
    "    \"\"\"\n",
    "    if isinstance(s, list):\n",
    "        return [lower(t) for t in s]\n",
    "    if isinstance(s, str):\n",
    "        return s.lower()\n",
    "    else:\n",
    "        raise NotImplementedError(\"unknown datatype\")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results\n",
    "\n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token.lower() for token in tokens if token not in stopwords and not token.isnumeric() and token.isalpha()]\n",
    "\n",
    "def filter_combo(text):\n",
    "    text = re.sub(\"[^A-Za-z]+\", ' ', str(text)).lower()\n",
    "    #doc = nlp(text)  \n",
    "#    print(\"filter. :\",list(filter(None, text.split(\" \"))))\n",
    "    doc = nlp(\" \".join(list(filter(None, text.split(\" \")))))\n",
    "#   doc = spacy.tokens.Doc(nlp.vocab, words=list(filter(None, text.split(\" \"))))\n",
    "    #tokens = [i.lemma_ for indexer,i in enumerate(doc) if not i.is_stop and i.pos_ in [\"ADV\",\"ADJ\"]]\n",
    "    # [\"VERB\",\"ADV\",\"ADJ\",\"NOUN\",\"PROPN\"] ]\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop ]\n",
    "#    tokens = tokenize(\" \".join(tokens))\n",
    "#    tokens = stem(tokens)\n",
    "#    tokens = lower(tokens)\n",
    "#    tokens = filter_stopwords(tokens)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def normalize_dataset(ds):\n",
    "    ds = [tokenize(re.sub(\"[^A-Za-z]+\", ' ', str(line)).lower()) for line in ds]\n",
    "#    print(\"ds : \",ds)\n",
    "    lengths = np.cumsum([0] + list(map(len, ds)))\n",
    "    #print(lengths)\n",
    "\n",
    "    flat_words = [item for sublist in ds for item in sublist]\n",
    "    doc = spacy.tokens.Doc(nlp.vocab, words=flat_words)\n",
    "#    doc = nlp(cwor)\n",
    "    \n",
    "    lemmatized = []\n",
    "    # Iterate starting with 1\n",
    "    for index in range(1, len(lengths)):\n",
    "        span = doc[lengths[index - 1] : lengths[index]]\n",
    "       # print(\"span : \",span)\n",
    "        lemmatized.append([token.pos_ for token in span])\n",
    "#        lemmatized.append([token.lemma_  if not token.is_stop and token.pos_ in [spacy.symbols.ADJ,spacy.symbols.ADV] else token.pos_ for token in span])\n",
    "#    print(lemmatized)\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "def nor_ds(dataset):    \n",
    "    text = [re.sub(\"[^A-Za-z]+\", ' ', str(line)).lower() for line in dataset]\n",
    "    tokens = []    \n",
    "    disable_list = [\"ner\",\n",
    "                    \"entity_linker\",\n",
    "                    \"entity_ruler\",\n",
    "                    \"textcat\",\n",
    "                    \"textcat_multilabel\",\n",
    "                    \"lemmatizer\",\n",
    "                    \"morphologizer\",\n",
    "                    \"sentencizer\",                \n",
    "                   ]\n",
    "    for doc in nlp.pipe(text, n_process=4, batch_size=2000, disable=disable_list):        \n",
    "        line_tokens = [token.text  for token in doc if not token.is_stop and len(token.text) > 1 ]\n",
    "        if len(line_tokens) > 0:\n",
    "            tokens.append(line_tokens)\n",
    "        else:\n",
    "            tokens.append([\"neutral\"])\n",
    "                        #\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "def filter_non_alphabets(tokens):\n",
    "    return [token for token in tokens if token.isalpha()]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_onehot_vector(feats, feats_dict):\n",
    "    \"\"\"\n",
    "    :param data: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
    "    for f in feats:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "yXGfXEpjCSDV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_non_alphabets(['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dekgQGHT9AOw"
   },
   "source": [
    "Note that you can use the `map` function to apply your preprocessing functions into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Auz1_Y2U9AOw",
    "outputId": "15990d14-18d1-400d-acd0-f7ef78d5e14e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nice', 'diner', 'still', 'food', 'good', 'definitely', 'good', 'spot', 'breakfast'], ['while', 'back', 'chicken', 'sandwich', 'meh', 'pretty', 'small', 'spices', 'flavors', 'chicken', 'sandwich', 'special', 'opinion', 'mary', 'browns', 'kfc', 'chicken', 'sandwiches', 'better', 'super', 'long', 'sandwich', 'one', 'even', 'store', 'about', 'pm', 'good', 'minutes', 'sandwich', 'combo', 'cashier', 'phone', 'whole', 'time', 'too', 'really', 'order'], ['more', 'pork', 'selections', 'menu', 'food', 'very', 'good', 'beer', 'selection', 'good', 'pork', 'belly', 'best', 'schnitzel', 'also', 'very', 'good', 'back', 'oddly', 'busy', 'friday', 'night'], ['place', 'phenomenal', 'pricey', 'urge', 'place', 'food', 'great', 'server', 'very', 'attentive', 'server', 'table', 'servers'], ['truffle', 'macaroni', 'cheese', 'potatoes', 'gratin', 'short', 'amazing', 'death', 'row', 'last', 'meal', 'here'], ['first', 'time', 'lizard', 'none', 'area', 'bavarian', 'pretzels', 'sauces', 'chicken', 'melt', 'onion', 'rings', 'side', 'husband', 'special', 'brisket', 'potato', 'pancakes', 'also', 'pleased', 'fun', 'place', 'when'], ['husbands', 'here', 'whenever', 'time', 'gift', 'card', 'always', 'honey', 'meat', 'shrimp', 'rice', 'appetizers', 'visit', 'crab', 'wonton', 'pork', 'egg', 'rolls', 'here', 'more', 'often', 'honestly', 'just', 'soup', 'guest', 'little', 'larger', 'portion', 'size', 'why', 'so', 'much', 'plate', 'rice', 'noodles', 'server', 'polite', 'son', 'mannerism', 'behavior', 'lovely', 'cocktail', 'rather', 'bar', 'cocktail', 'lounge', 'also', 'more', 'drinks', 'sake', 'less', 'sangria', 'tequila', 'margaritas', 'when', 'tejas', 'more', 'suggestion', 'signature', 'lettuce', 'wraps', 'fan', 'fav', 'really', 'signature', 'sake', 'cocktail', 'green', 'tea', 'dessert', 'just', 'sayin'], ['great', 'salad', 'bar', 'selection', 'garlic', 'cheese', 'great', 'baked', 'potatoes', 'mixed', 'vegetable', 'platter', 'salmon', 'little', 'cooked', 'graciously', 'back', 'second', 'try', 'very', 'good', 'only', 'flavor', 'scale', 'waitress', 'tae', 'delight', 'excellent', 'server', 'very', 'conscientious', 'excellent', 'meal', 'here', 'nice', 'atmosphere', 'right', 'street', 'hampton', 'inn', 'where'], ['antique', 'sugar', 'as', 'cute', 'name', 'owners', 'anna', 'badass', 'lady', 'awesome', 'store', 'best', 'vintage', 'boutique', 'phoenix', 'definitely', 'worth'], ['girlfriend', 'home', 'mall', 'sunday', 'when', 'sudden', 'martinis', 'quite', 'sure', 'how', 'here', 'places', 'place', 'where', 'delicious', 'decadent', 'brunch', 'never', 'more', 'mimosa', 'as', 'far', 'drinks', 'concerned', 'yet', 'place', 'where', 'bar', 'pm', 'dirty', 'martinis', 'pm', 'martinis', 'perfect', 'good', 'sized', 'yummy', 'huge', 'olives', 'short', 'just', 'then', 'checks', 'away', 'dirty', 'martini', 'early', 'sunday', 'afternoon', 'shopping', 'here', 'delicious', 'classic', 'cheap', 'terrace', 'cafe'], ['girls', 'dinner', 'recently', 'chill', 'tasty', 'highlights', 'lighter', 'smaller', 'plates', 'great', 'beet', 'salad', 'manchego', 'cheese', 'crustini', 'things', 'well', 'server', 'kind', 'bit', 'slow', 'start', 'ginger', 'beer', 'wine', 'list', 'great', 'creative', 'funky', 'cool', 'really', 'more', 'grown', 'slightly', 'vibe', 'lighting', 'warm', 'romantic', 'wine', 'walls', 'older', 'good', 'place', 'parents', 'colleagues', 'also', 'well', 'girl', 'chats', 'up', 'again', 'crazy', 'rush', 'nice', 'place', 'roster', 'when', 'wine', 'meal'], ['very', 'mad', 'time', 'money', 'here', 'friends', 'vegas', 'days', 'aug', 'neat', 'little', 'ticket', 'office', 'csi', 'experience', 'where', 'events', 'csi', 'experience', 'cool', 'laughable', 'crappiest', 'low', 'point', 'trip', 'crime', 'brain', 'whatsoever', 'idiot', 'basically', 'almost', 'most', 'obvious', 'way', 'step', 'awesome', 'probably', 'yr', 'old', 'over', 'don', 't', 'bed', 'anymore', 'assenine', 'experience', 'mad', 'money', 'at', 'personally', 'craptastic', 'un', 'attraction', 'ground'], ['habit', 'stars', 'better', 'good', 'places', 'lunch', 'mainly', 'hotel', 'aria', 'asian', 'cuisine', 'service', 'gracious', 'kind', 'server', 'pleasant', 'eager', 'peach', 'ginger', 'very', 'refreshing', 'creative', 'potstickers', 'potsticker', 'lover', 'always', 'lookout', 'large', 'very', 'nice', 'dough', 'perfection', 'nice', 'spicy', 'sauce', 'then', 'sesame', 'chicken', 'chinese', 'real', 'original', 'delicious', 'sweet', 'chili', 'sauce', 't', 'sweet', 'ambiance', 'beautiful', 'serene', 'oasis', 'away', 'casino', 'nice', 'romantic', 'spot', 'just', 'really', 'enjoyable'], ['much', 'better', 'bbq', 'much', 'better', 'service', 'way', 'less', 'money', 'lucille', 's', 'excited', 'place', 'parents', 'really', 'maybe', 'just', 'too', 'high', 'expectations', 'hostess', 'few', 'brain', 'cells', 'party', 'times', 'name', 'even', 'numbers', 'when', 'table', 'ready', 'water', 'ok', 'super', 'slow', 'when', 'checks', 'actually', 'biscuits', 'amazing', 'nice', 'touch', 'so', 'long', 'table', 'one', 'party', 'amazed', 'meals', 'most', 'disappointing', 'mac', 'cheese', 'so', 'good', 'dry', 'mushy', 'gross', 'enough', 'mac', 'cheese', 'couple', 'weeks', 'so', 'so', 'food', 'extremely', 'loud', 'restaurant', 'while', 'second', 'shot'], ['absolutely', 'horrible', 'company', 'rude', 'general', 'manager', 'customer', 'service', 'diagnose', 'washing', 'machine', 't', 'work', 'next', 'day', 'issue', 'part', 'originally', 'manager', 'very', 'rude', 'mother', 'phone', 'technician', 't', 'costs', 'needs', 'big', 'scam'], ['girlfriends', 'summerlious', 'appetizer', 'pate', 'entree', 'steak', 'fries', 'overly', 'steak', 'disappointment', 'mediums', 'rare', 'waiter', 'mediums', 'there', 'steak', 'friends', 'medium', 'too', 'rare', 'back', 'too', 'dessert', 'no', 'good', 'creme', 'brul', 'smooth', 'eggs', 'process', 'service', 'terrible', 'waiters', 'hardly', 'one', 'food', 'okay', 'terrible'], ['food', 'very', 'bland', 'breakfast', 'burritos', 'steak', 'chorizo', 'chicken', 'nachos', 'burritos', 'about', 'ratio', 'potato', 'eggs', 'don', 't', 'really', 'even', 'ratios', 'proportionate', 'taste', 'any', 'different', 'even', 'sauce', 'meal', 'more', 'appetizing', 'chicken', 'nachos', 'decent', 'only', 'fresh', 'guac', 'stars', 'solely', 'guacamole', 'otherwise', 'water', 'more', 'flavor', 'then', 'breakfast'], ['here', 'years', 'ago', 'when', 'different', 'place', 'early', 'mothers', 'day', 'dinner', 'right', 'away', 'table', 'clean', 'food', 'crumbs', 'waiter', 'nice', 'enough', 'guy', 'min', 'food', 'only', 'table', 'once', 'right', 'minutes', 'later', 'still', 'bread', 'basket', 'offers', 'then', 'out', 'food', 'son', 'salmon', 'literally', 'smallest', 'piece', 'salmon', 'ever', 'maybe', 'inches', 'long', 'just', 'other', 'shock', 'good', 'ridiculously', 'small', 'reuben', 'sandwich', 'pretty', 'good', 'special', 'at', 'handful', 'stale', 'chips', 'husband', 'tiny', 'portion', 'certainly', 'dinner', 'sized', 'portion', 'only', 'server', 'times', 'order', 'min', 'food', 'right', 'food', 'check', 'again', 'nice', 'enough', 'fellow', 'horrible', 'service', 'way', 'over', 'sadly', 'never', 'back'], ['risk', 'management', 'messages', 'never', 'return', 'calls', 'issue'], ['guy', 'joshua', 'schorr', 'totally', 'soulless', 'inexperienced', 'just', 'interested', 'money', 'next', 'time', 'better', 'healer'], ['there', 'dinner', 'fri', 'nite', 'waitress', 'mushrooms', 'great', 'sausage', 'mushrooms', 'cooked', 'chicken', 'parm', 'average', 'wife', 'red', 'sauce', 'percent', 'better', 'italian', 'back', 'time', 'soon', 'streets', 'new', 'york', 'instead', 'good', 'luck'], ['group', 'plate', 'when', 'relatively', 'busy', 'tables', 'together', 'current', 'photos', 'sense', 'food', 'how', 'many', 'things', 'gravy', 'mediterranean', 'cheese', 'side', 'mixed', 'cooked', 'veggies', 'fairly', 'small', 'sandwich', 'decent', 'size', 'side', 'other', 'pictures', 'overall', 'just', 'small', 'sandwich', 'good', 'saltiness', 'olives', 'also', 'doubt', 'around', 'rest', 'order', 'veggies', 'too', 'wet', 'oily', 'abundance', 'onions', 'really', 'instead', 'just', 'pan', 'oil', 'healthy', 'side', 'sadly', 'care', 'balance', 'flavors', 'other', 'orders', 'decent', 'california', 'egg', 'sandwiches', 'avocado', 'coffee', 'too', 'just', 'local', 'probably', 'elsewhere', 'good', 'morning'], ['place', 'awesome', 'girlfriend', 'here', 'college', 'night', 'tuesday', 'bartenders', 'very', 'friendly', 'drinks', 'as', 'fast', 'dollar', 'drink', 'prolly', 'hours', 'beer', 'pong', 'great', 'night', 'perfect', 'place', 'tuesday', 'night', 'inside', 'hotel'], ['places', 'las', 'vegas', 'thing', 'always', 'owners', 'time', 'around', 'sure', 'well', 'soup', 'yummy', 'dishes', 'perfectly', 'birthdays', 'also', 'fun', 'quick', 'shot', 'thing', 'great', 'place', 'friends', 'family'], ['establishment', 'doesn', 't', 'even', 'star', 'different', 'price', 'customer', 'cars', 'carwash', 'horrible', 'wax', 'job', 'pathetic', 'battery', 'right', 'corner', 'management', 'less', 'when', 'issues', 'attention', 'glass', 'guy', 'sexual', 'harrassment', 'pricing', 'guy', 't', 'money', 'policy', 'acceptable', 'capable', 'right', 'still', 'wax', 'stains', 'moulding', 'car', 'away', 'establishment', 'ripoff'], ['people', 'innocent', 'gullible', 'people', 'distributors', 'deceptive', 'scare', 'tactics', 'spiel', 'health', 'risks', 'family', 'high', 'pressure', 'sale', 'pitches', 'actually', 'as', 'soon', 'sales', 'pitch', 'even', 'interested', 'guy', 'application', 'information', 'credit', 'immediately', 'ready', 'credit', 'card', 'payment', 'interested', 'guilty', 'family', 'health', 'price', 'seller', 'then', 'correspondence', 'd', 'when', 'completely', 'intelligence', 'mail', 'pretends', 'phone', 'then', 'financing', 'only', 'month', 'how', 'world', 'numbers', 'name', 'letter', 'serious', 'how', 'least', 'bit', 'interested', 'point', 'too', 'overwhelming', 'maybe', 'alone', 'sh', 't', 'husband', 'around', 'so', 'satisfied', 'product', 'don', 't', 'husband', 'first', 'then', 'next', 'day', 'so', 'uneasy', 'whole', 'transaction', 'knots', 'stomach', 'just', 'didn', 't', 'right', 'sure', 'enough', 'next', 'day', 'company', 'so', 'many', 'negative', 'reviews', 'point', 't', 'how', 'great', 'products', 'way', 'amazon', 'fraction', 'cost', 'online', 'only', 'way', 'refund', 'transactions', 'first', 'days', 'sale', 'seller', 'course', 'phone', 'incoming', 'calls', 'so', 'order', 'today', 'immediately', 'furthermore', 'paperwork', 'husband', 'don', 't', 'already', 'paperwork', 'again', 'return', 'quickly', 'phone', 'point', 'along', 'frustrated', 'advantage', 'immediately', 'company', 'directly', 'return', 'policy', 'rep', 'returns', 'sales', 'final', 'only', 'able', 'first', 'day', 'transactions', 'days', 'bind', 'contract', 'payments', 'even', 'satisfied', 'product', 'directions', 'returns', 'so', 'first', 'business', 'days', 'transactions', 'company', 'cancellation', 'number', 'back', 'sales', 'transaction', 'cancellation', 'front', 'back', 'form', 'fax', 'fax', 'customer', 'service', 'people', 'courteous', 'pushy', 'at', 'all', 'when', 'fax', 'distributor', 'days', 'trouble', 't', 'fall', 'slick', 'also', 'salesperson', 'points', 'license', 'license', 'maybe', 'pushy', 'interested', 'alone', 'sorry', 'sorry', 'sales', 'person', 'tactics', 'honest', 'so', 'facts', 'back', 'real', 'scientific', 'research', 'products'], ['decent', 'veggie', 'pho', 'loins', 'quiver', 'large', 'bowl', 'really', 'that', 'big', 'kind', 'disappointing', 'actually', 'more', 'veggies', 'there', 'how', 'tofu', 'also', 'pho', 'half', 'off', 'bonus', 'extra', 'cents', 'pho', 'very', 'amusing', 'maxim', 'restaurant', 'pho'], ['away', 'dairy', 'queen', 'town', 'birthday', 'cakes', 'amazing', 'job', 'vision', 'always', 'custom', 'design', 'current', 'interests', 'cake', 'decorator', 'here', 'always', 'expectations'], ['stars', 'best', 'cookies', 'seriously', 'cookies', 't', 'even', 'why', 'rest', 'buffet'], ['vegas', 'food', 'amazing', 'well', 'vegan', 't', 'typically', 'vegetarian', 'food', 'amazing', 't', 'difference', 'bold', 'flavors', 'foodie', 'paradise', 'vegan', 'tasting', 'menu', 'really', 'good', 'desert', 'things', 'just', 'vegan', 'sorry', 'lol'], ['hong', 'kong', 'station', 'lunch', 'only', 'soup', 'bao', 'great', 'spicy', 'broth', 'lips', 'great', 'stuff', 'lunch', 'week', 'communication', 'challenge', 'cold', 'cold', 'inside', 'worth'], ['probably', 'best', 'mexican', 'food', 'ever', 'az', 'decently', 'items', 'food', 'amazing', 'amazing', 'definitely', 'back'], ['nice', 'views', 'service', 'food', 'great', 'here', 'also', 'complimentary', 'glass', 'wine', 'mailing', 'list', 'glass', 'wine', 'menu', 'food', 'highlights', 'beef', 'carpaccio', 'tasty', 'light', 'bbq', 'short', 'rib', 'tostadas', 'sweet', 'flavor', 'crispy', 'skin', 'salmon', 'basil', 'pesto', 'nice', 'touch', 'strip', 'avocado', 'unique', 'food', 'items', 'fantastic', 'ambiance', 'lovely', 'bit', 'too', 'dark', 'night', 'so', 'more', 'candles', 'bit', 'brighter', 'location', 'right', 'state', 'street', 'good', 'easy'], ['vegas', 'way', 'never', 'back', 'years', 'city', 'still', 'new', 'adventures', 'last', 'few', 'times', 'few', 'spots', 'curiosity', 'hakkasan', 'list', 'so', 'good', 'enough', 'size', 'group', 'reservations', 'hubbub', 'bub', 'first', 'impression', 'beautiful', 'sultry', 'mgm', 'often', 'game', 'changer', 'strip', 'constant', 'updates', 'all', 'ready', 't', 'really', 'comfortable', 'photos', 'dishes', 'only', 'yelper', 'group', 'also', 'phone', 'table', 'bit', 'rude', 'photos', 'best', 'likes', 'dislikes', 'dim', 'sum', 'platter', 'as', 'much', 'crispy', 'duck', 'salad', 'lion', 'head', 'truffle', 'sauced', 'meatballs', 'as', 'much', 'spicy', 'scallops', 'where', 'meaty', 'clean', 'perfect', 'amount', 'chili', 'spice', 'pretty', 'great', 'dim', 'sum', 'places', 'bit', 'bad', 'just', 'better', 'free', 'entry', 'night', 'club', 'time', 'buns', 'awww', 'such', 'cute', 'buns', 'too', 'group', 'club', 'hopping', 'type', 'totally', 'fine', 'hakkasan', 'satisfied', 'overly', 'full', 'fine', 'more', 'stops', 'strip', 'evening', 'fan', 'establishment', 'date', 'late', 'dinner', 'vegas', 'winner'], ['daughter', 'here', 'sat', 'around', 'breakfast', 'burritos', 'tax', 'mins', 'other', 'customers', 'there', 't', 'front', 'when', 'place', 'where', 'own', 'drinks', 'soda', 'fountain', 'plastic', 'utensils', 'tin', 'bucket', 'table', 'burrito', 't', 'bad', 'worth', 't'], ['literally', 'worst', 'customer', 'service', 'auto', 'zone', 'way', 'miles', 'location', 'store', 'item', 'single', 'associate', 'location', 'ever', 'again'], ['dealfind', 'terrible', 'partners', 'teambuy', 'so', 'customer', 'service', 'dealfind', 'clear', 'description', 'provisions', 'dealfind', 'rudely', 'go', 'long', 'customer', 'great', 'customer', 'service', 't', 'really', 'crap', 'barely', 'issues', 'only', 'email', 'emails', 'emails', 'times', 'issues', 'rating'], ['food', 'point', 'atmosphere', 'fun', 'trendy', 'seating', 'very', 'tight', 'wait', 'long', 'worth', 'traditional', 'style', 'hot', 'pot', 'great', 'social', 'atmosphere', 'no', 'further', 'prices', 'also', 'super', 'reasonable', 'discount', 'card', 'sale', 'august', 'great', 'value', 'local', 'definitely', 'list', 'vegas'], ['away', 'as', 'fast', 'so', 'guys', 'yelp', 'review', 'sites', 'reviews', 'here', 'reviews', 'only', 'reviews', 'person', 'so', 'so', 'happy', 'service', 'work', 'las', 'vegas', 'rv', 's', 'reviews', 'adult', 'entertainment', 'las', 'vegas', 'purchase', 'price', 'fair', 'warranty', 'work', 'crap', 'here', 'actually', 'gall', 'warranty', 'new', 'saga', 'other', 'lifestyle', 'yelp', 'review', 'area', 'custom', 'car', 'shop', 'mesa', 'also', 'why', 'yelp', 'sites', 'aside', 'when', 'new', 'warranty', 'dutchman', 'right', 'up', 'camping', 'world', 'avondale', 'warranty', 'camping', 'world', 'right', 'first', 'time', 'quickly'], ['restaurant', 'awful', 'staff', 'extremely', 'rude', 'inattentive', 'food', 'mediocre', 'restrooms', 'filthy', 'saturday', 'night', 'one', 'even', 'name', 'minutes', 'wait', 'excessive', 'place', 'unorganized', 'very', 'rude', 'there', 'year', 'ago', 'experience', 'decent', 'tonight', 'never'], ['today', 'first', 'time', 'nails', 'here', 'so', 'glad', 'mary', 'mine', 'amazing', 'job', 'quick', 'nails', 'definitely', 'again', 'here', 'mary', 'sooo', 'sweet'], ['really', 'corbeaux', 'maybe', 'once', 'month', 'half', 'th', 'concept', 'nice', 'so', 'massive', 'skylight', 'natural', 'light', 'tables', 'walls', 'same', 'time', 'all', 'place', 'th', 'lot', 'building', 'exterior', 'pretty', 'spaciousness', 'building', 'advantage', 'pretty', 'terribly', 'layout', 'dining', 'tables', 'side', 'divider', 'counter', 'where', 'goods', 'never', 'sure', 'where', 'kind', 'of', 'defeats', 'purpose', 'line', 'purpose', 'hangout', 'place', 'formal', 'dining', 'experience', 'dinner', 'there', 'day', 't', 'never', 'really', 'restaurant', 'never', 'there', 'exclusively', 'sit', 'meal', 'corbeaux', 'basically', 'stools', 'windows', 'actually', 'best', 'feeling', 'even', 'basically', 'same', 'thing', 'analog', 'first', 'few', 'times', 'when', 'fledgling', 'operation', 'feet', 'cashiers', 'baked', 'goods', 'inexperienced', 'forever', 'just', 'chocolate', 'eclair', 'cashier', 'also', 'retrieving', 'sandwich', 'warming', 'whatnot', 'till', 'really', 'inefficient', 'definitely', 'impatient', 'just', 'eclair', 'good', 'don', 't', 'hampering', 'sweets', 'sandwiches', 'often', 'enough', 'regularly', 'don', 't', 'corbeaux', 'coolest', 'thing', 'ever', 'execution', 'actually', 'quite', 'confusing', 'idea', 'areas', 'back', 'just', 'lot', 'space', 'more', 'cohesive', 'terms', 'food', 'cookies', 'okay', 'didn', 't', 'fresh', 'eggplants', 'so', 'eggplant', 'sandwich', 'very', 'oily', 'all', 'hands', 'definitely', 'when', 'stool', 'straight', 'cashier', 'paper', 'napkins', 'pretty', 'judge', 'hands', 'oil', 'sweets', 'high', 'quality', 'macaroons', 'slightly', 'disappointing', 'overall', 'just', 'phenomenal', 'experience'], ['food', 'pretty', 'good', 'nd', 'time', 'here', 'extremely', 'annoyed', 'server', 'almost', 'bucks', 'back', 'jack'], ['terrible', 'service', 'awful', 'food', 'statement', 'overly', 'kind', 'place', 'wife', 'when', 'in', 'only', 'place', 'hotel', 'brunch', 'dirty', 'curtains', 'doorway', 'disorganized', 'tacky', 'decor', 'charming', 'place', 'owner', 'ashamed', 'place', 'complete', 'tourist', 'trap'], ['vegas', 't', 'cocacola', 'world', 'least', 'once', 'life', 'amazing', 'immediately', 'coke', 'literally', 'single', 'piece', 'merchandise', 'cocacola', 'ever', 'here', 'amazing', 'mini', 'fridges', 'tumblers', 'glasses', 'silverware', 'bags', 'geez', 'here', 'forever', 'so', 'much', 'merch', 'literally', 'souvenirs', 'friends', 'single', 'stop', 'sometimes', 'polar', 'bear', 'around', 'pics', 'too', 'cute', 'employees', 'mostly', 'friendly', 'store', 'cafe', 'upstairs', 'less', 'friendly', 'staff', 'job', 'quick', 'question', 'tumblers', 'counter', 'store', 'way', 't', 'really', 'so', 'happy', 'store', 'stuff', 'cafe', 'amazing', 'even', 'never', 'soda', 'plates', 'cokes', 'world', 'so', 'interesting', 'good', 'disgusting', 'sodas', 'really', 'fanta', 'melon', 'flavour', 'don', 't', 'where', 'italy', 'disgusting', 'definitely', 'interesting', 'also', 'souvenir', 'cup', 'coke', 'slushy', 'inside', 'great', 'treat', 'hot', 'vegas', 'summer', 'cup', 'also', 'beautiful', 'colourful', 'love', 'definitely', 'stop', 'here', 'time', 'vegas'], ['best', 'tons', 'traffic', 'one', 'forever', 'mile', 'barely', 'variety', 'lighting', 'time', 'exit', 'now', 'wife', 'shiver'], ['yelp', 'special', 'area', 'family', 'photos', 'reviews', 'spot', 'dinner', 'sure', 'why', 'awful', 'experiance', 'extremely', 'disappointed', 'staff', 'quality', 'food', 'overall', 'family', 'one', 'meal', 'del', 'taco', 'here', 'ribeye', 'tacos', 'good', 'meat', 'awful', 'super', 'thin', 'very', 'small', 'chunks', 'bad', 'taste', 'even', 'tortilla', 'bad', 'fresh', 'avocado', 'nice', 'touch', 'dish', 'total', 'fail', 'wife', 'mix', 'tacos', 'barbacoa', 'very', 'oily', 'salt', 'chicken', 'very', 'peppery', 'bag', 'ribeye', 'kids', 'meals', 'drink', 'side', 'cool', 'quesadillas', 'unfortunately', 'good', 'kids', 'bad', 'wife', 'super', 'super', 'salty', 'kids', 'son', 'nachos', 'sour', 'cream', 'then', 'when', 'beans', 'sour', 'cream', 'when', 'very', 'politely', 'never', 'sour', 'cream', 'beans', 'son', 'pretty', 'clear', 'register', 'sour', 'perhaps', 'beans', 'nachos', 'awful', 'cheese', 'wiz', 'peppery', 'chicken', 'bag', 'chips', 'very', 'greasy', 'really', 'nachos', 'epic', 'fail', 'very', 'surprised', 'place', 'such', 'good', 'reviews', 'bad'], ['hotel', 'room', 'food', 'minutes', 'ribs', 'dry', 'bbq', 'sauce', 'tempura', 'shrimp', 'calamari', 'batter', 'too', 'salty', 'too', 'thick', 'items', 'tips'], ['place', 'various', 'hakka', 'restaurants', 'gta', 'place', 'top', 'food', 'fresh', 'very', 'flavourful', 'lunch', 'specials', 'as', 'well', 'pm', 'also', 'indian', 'thai', 'fusion', 'menu', 'items', 't'], ['here', 'just', 'pm', 'right', 'menu', 'huge', 'chalkboard', 'lots', 'good', 'stuff', 'thing', 'great', 'good', 'service', 'great', 'atmosphere', 'byob', 'sauces', 'really', 'good', 'too', 'great', 'time', 'great', 'food', 'when', 'pittsburgh', 'strip', 'district']]\n"
     ]
    }
   ],
   "source": [
    "#test_df['tokens'] = test_df['text'].map(tokenize).map(filter_stopwords).map(lower)\n",
    "#print(test_df['tokens'].head().to_string())\n",
    "\n",
    "#combo = test_df['text'].map(filter_combo)\n",
    "import re\n",
    "#print(\"combo : \",combo)\n",
    "#print(normalize_dataset(test_df[\"tokens\"]))\n",
    "\n",
    "def nol(dataset):\n",
    "    print(\"nol dataset\")\n",
    "\n",
    "    \n",
    "    text = [re.sub(\"[^A-Za-z]+\", ' ', str(line)).lower() for line in dataset]\n",
    "    print(\"re all text\")\n",
    "    tokens = []\n",
    "    \n",
    "    for doc in nlp.pipe(text, n_process=4, batch_size=2000, disable=[\"ner\",\"entity_linker\",\"entity_ruler\",\"textcat\",\"textcat_multilabel\",\"lemmatizer\",\"morphologizer\",\"attribute_ruler\",\"senter\",\"sentencizer\",\"tok2vec\",\"transformer\"]):        \n",
    "        for token in doc:\n",
    "            if token.pos_ in [\"PROPN\",\"ADV\",\"ADJ\",\"NOUN\"]:\n",
    "                tokens.append(token)                \n",
    "    return tokens\n",
    "    \n",
    "    #doc = nlp(text)  \n",
    "    #    print(\"filter. :\",list(filter(None, text.split(\" \"))))\n",
    "    \n",
    "    #   doc = spacy.tokens.Doc(nlp.vocab, words=list(filter(None, text.split(\" \"))))\n",
    "    #tokens = [i.lemma_ for indexer,i in enumerate(doc) if not i.is_stop and i.pos_ in [\"ADV\",\"ADJ\"]]\n",
    "    # [\"VERB\",\"ADV\",\"ADJ\",\"NOUN\",\"PROPN\"] ]\n",
    "    \n",
    "    #    tokens = tokenize(\" \".join(tokens))\n",
    "    #    tokens = stem(tokens)\n",
    "    #    tokens = lower(tokens)\n",
    "    #    tokens = filter_stopwords(tokens)\n",
    "\n",
    "#print(nor_ds(train_df[\"text\"].values)[:50])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nuf78xEW9AOx"
   },
   "source": [
    "Besides `nltk`, I would like to introduce `SpaCy`, a newer text processing toolkit of industrial strength.\n",
    "\n",
    "You can explore it at https://spacy.io/\n",
    "\n",
    "Let's install it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cX201ILd9AOx"
   },
   "source": [
    "```bash\n",
    "python -m pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "_Mf62BhX9AOx"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5TClnyt9AOx"
   },
   "source": [
    "SpaCy enables you use linguistic features of texts\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd029IwJ9AOx",
    "outputId": "ce02b52e-cc93-46a9-d236-2a75d90aafa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw       ,\t stem      ,\t PartOfSpeech,\t dependency,\t shape     ,\t is alpha  ,\t is stop   ,\t its childrens in the parsing tree,\t \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Apple     ,\t Apple     ,\t PROPN     ,\t nsubj     ,\t Xxxxx     ,\t True      ,\t False     ,\t []        ,\t \n",
      "is        ,\t be        ,\t AUX       ,\t aux       ,\t xx        ,\t True      ,\t True      ,\t []        ,\t \n",
      "looking   ,\t look      ,\t VERB      ,\t ROOT      ,\t xxxx      ,\t True      ,\t False     ,\t [Apple, is, at, startup],\t \n",
      "at        ,\t at        ,\t ADP       ,\t prep      ,\t xx        ,\t True      ,\t True      ,\t [buying]  ,\t \n",
      "buying    ,\t buy       ,\t VERB      ,\t pcomp     ,\t xxxx      ,\t True      ,\t False     ,\t [U.K.]    ,\t \n",
      "U.K.      ,\t U.K.      ,\t PROPN     ,\t dobj      ,\t X.X.      ,\t False     ,\t False     ,\t []        ,\t \n",
      "startup   ,\t startup   ,\t NOUN      ,\t advcl     ,\t xxxx      ,\t True      ,\t False     ,\t [for]     ,\t \n",
      "for       ,\t for       ,\t ADP       ,\t prep      ,\t xxx       ,\t True      ,\t True      ,\t [billion] ,\t \n",
      "$         ,\t $         ,\t SYM       ,\t quantmod  ,\t $         ,\t False     ,\t False     ,\t []        ,\t \n",
      "1         ,\t 1         ,\t NUM       ,\t compound  ,\t d         ,\t False     ,\t False     ,\t []        ,\t \n",
      "billion   ,\t billion   ,\t NUM       ,\t pobj      ,\t xxxx      ,\t True      ,\t False     ,\t [$, 1]    ,\t \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "fmt = \"{:10s},\\t \" * 8\n",
    "print(fmt.format('raw', 'stem', 'PartOfSpeech', 'dependency', 'shape', 'is alpha', 'is stop', 'its childrens in the parsing tree'))\n",
    "print('-'*140)\n",
    "for token in doc:\n",
    "    print(fmt.format(token.text, token.lemma_, token.pos_, token.dep_,\n",
    "            token.shape_, str(token.is_alpha), str(token.is_stop), str(list(token.children))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3uMcKJ0DTBI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3vRHO5I9AOy"
   },
   "source": [
    "SpaCy also allows you use the embeddings for both sentence and words\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQ2SXIwe9AOy",
    "outputId": "5b42884b-b693-47b3-bdee-13f8af3e5928"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is looking at buying U.K. startup for $1 billion [ 0.4847193   0.34561655  0.23650904 -0.27294627  0.30828613] ...\n",
      "Apple [ 0.9396687   0.46727175 -0.3862503  -0.23296848  0.25683203] ...\n",
      "is [-0.21470308 -0.36800703  1.8618155  -0.43874717 -0.6448474 ] ...\n",
      "looking [ 1.5960355  -0.01218066 -0.1948367   0.7979922   0.36900565] ...\n",
      "at [-1.2617028  -0.8116296  -0.55736023  0.08604071 -0.43663728] ...\n",
      "buying [ 0.3020423  -0.9611639   1.2695026   0.10633498  2.8583994 ] ...\n",
      "U.K. [ 2.2959712   0.78135234 -1.0174923  -0.5566485   0.69199914] ...\n",
      "startup [0.6782811  0.03798376 0.07798427 0.1210558  0.5636424 ] ...\n",
      "for [-0.07904667 -0.21996386 -1.3529027  -0.24131706  0.43687835] ...\n",
      "$ [ 0.44878927  0.75564337  0.5757578  -1.1713823   0.7438692 ] ...\n",
      "1 [-0.3846085  2.7049747  2.7081459 -1.4393395 -0.5412608] ...\n",
      "billion [ 1.011186    1.4275012  -0.38276425 -0.03342953 -0.9067332 ] ...\n"
     ]
    }
   ],
   "source": [
    "print(doc, doc.vector[:5], '...')\n",
    "for t in doc:\n",
    "    print(t, t.vector[:5], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0B0mnB5F9AOy"
   },
   "source": [
    "For more usage of SpaCy, you can refer to the documentation of spacy https://spacy.io/usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrePxBEL9AOy"
   },
   "source": [
    "### B. Explorative data analysis\n",
    "\n",
    "For our dataset, we have features more than text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CF54MBSy9AOz",
    "outputId": "54c7dc7c-f736-4199-dcd3-00e2d82b4c73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [f, u, l, l] columns from the train split\n",
      "Failed, then try to \n",
      "select all columns from the train split\n"
     ]
    }
   ],
   "source": [
    "train_df_full = load_data('train', columns='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "27muOFSg9AOz",
    "outputId": "a47d02cc-c6c5-4205-a005-6af52db2e2ca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39rLHYJOy2774ZIUouuWLw</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-06-28 21:44:02</td>\n",
       "      <td>0</td>\n",
       "      <td>ynzOFepQYSCDGdfWDWxiZw</td>\n",
       "      <td>4</td>\n",
       "      <td>Nice to have a diner still around. Food was go...</td>\n",
       "      <td>0</td>\n",
       "      <td>Sl6VgFOB-XXfFIAYp7TFkw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E-Kq1Yu1d6N3TL2qX0aqjA</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-09 03:45:19</td>\n",
       "      <td>0</td>\n",
       "      <td>sQX9ncJBEdBf16AWsvO6Vg</td>\n",
       "      <td>2</td>\n",
       "      <td>Tried this a while back, got the fried chicken...</td>\n",
       "      <td>0</td>\n",
       "      <td>gcx01pMqWzkni2UC-zoZrA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nWW6fBfBljiRFa4sG7TyxA</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-07-19 02:00:04</td>\n",
       "      <td>0</td>\n",
       "      <td>bVIf2kqbzvif3miNe3ARNw</td>\n",
       "      <td>4</td>\n",
       "      <td>I expected more pork selections on menu. Food ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Mn9VzPbrCYU4EcP_C1oBOg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qmIHO-6T_KEfPC9jyGDamQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-11-11 08:10:24</td>\n",
       "      <td>0</td>\n",
       "      <td>LNj1OFxy2ool3PZANGchPA</td>\n",
       "      <td>4</td>\n",
       "      <td>YUMMY!!! This place is phenomenal. It is Price...</td>\n",
       "      <td>0</td>\n",
       "      <td>SKV1heo00fdciCbCN9Z33A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pKk7jCFIm96qDdk0laVT2w</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-01-16 20:04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>bZXxa0hO6wQlHD-MkMf4iw</td>\n",
       "      <td>5</td>\n",
       "      <td>The Truffle Macaroni &amp; Cheese and Potatoes Au ...</td>\n",
       "      <td>1</td>\n",
       "      <td>p1r7rZYruZR92x1A649PTQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                 date  funny  \\\n",
       "0  39rLHYJOy2774ZIUouuWLw     0  2017-06-28 21:44:02      0   \n",
       "1  E-Kq1Yu1d6N3TL2qX0aqjA     0  2018-04-09 03:45:19      0   \n",
       "2  nWW6fBfBljiRFa4sG7TyxA     0  2014-07-19 02:00:04      0   \n",
       "3  qmIHO-6T_KEfPC9jyGDamQ     0  2011-11-11 08:10:24      0   \n",
       "4  pKk7jCFIm96qDdk0laVT2w     1  2010-01-16 20:04:00      1   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  ynzOFepQYSCDGdfWDWxiZw      4   \n",
       "1  sQX9ncJBEdBf16AWsvO6Vg      2   \n",
       "2  bVIf2kqbzvif3miNe3ARNw      4   \n",
       "3  LNj1OFxy2ool3PZANGchPA      4   \n",
       "4  bZXxa0hO6wQlHD-MkMf4iw      5   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  Nice to have a diner still around. Food was go...       0   \n",
       "1  Tried this a while back, got the fried chicken...       0   \n",
       "2  I expected more pork selections on menu. Food ...       0   \n",
       "3  YUMMY!!! This place is phenomenal. It is Price...       0   \n",
       "4  The Truffle Macaroni & Cheese and Potatoes Au ...       1   \n",
       "\n",
       "                  user_id  \n",
       "0  Sl6VgFOB-XXfFIAYp7TFkw  \n",
       "1  gcx01pMqWzkni2UC-zoZrA  \n",
       "2  Mn9VzPbrCYU4EcP_C1oBOg  \n",
       "3  SKV1heo00fdciCbCN9Z33A  \n",
       "4  p1r7rZYruZR92x1A649PTQ  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgxCOol69AOz"
   },
   "source": [
    "You can explore the relationship between different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "S5eIO0i59AOz"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "0ejnqkMl9AOz",
    "outputId": "f54f389d-5fb0-4a5c-d2a8-aa954686e654"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1727512e8>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(train_df_full['cool'], train_df_full['funny'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "UE_I7piQ9AOz",
    "outputId": "ddde020c-1316-4f8e-b3ef-c293d2e6c5ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2534., 1354., 1888., 2110., 2114.]),\n",
       " array([1. , 1.8, 2.6, 3.4, 4.2, 5. ]),\n",
       " <BarContainer object of 5 artists>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_df_full['stars'], bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LT4NYbZG9AO0"
   },
   "source": [
    "Moreover, you may use the id feature to aggregate data samples\n",
    "\n",
    "For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "uk-n7Fmq9AO0",
    "outputId": "ea4f0914-94b3-470a-fd72-36ad5d0ad59c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 business_id  funny  cool  stars\n",
      "7043  -0qht1roIqleKiQkBLDkbw      1     0      3\n",
      "7363  -0qht1roIqleKiQkBLDkbw      0     0      5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAO0ElEQVR4nO3df6zdd13H8eeLloEKwrBXs/TH2miJVCRu3swlGJ2CoRuk1WBMa4hAJo26ocmIZgQzdfwDLhFCnEKDhIGBUpZgKhQngRGM2rG7ANN2KVzLZK0mK9sgIYvMwds/zrd4enZvz/e233tu9+H5SE76/fG557z27ee+eu73e893qSokSU9/z1jrAJKkYVjoktQIC12SGmGhS1IjLHRJasT6tXrhDRs21NatW9fq5SXpaem+++77elXNLbVvzQp969atLCwsrNXLS9LTUpL/XG6fp1wkqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSI6YWepL3JXk4yb8vsz9J3pVkMcn9Sa4cPqYkaZo+79DfD+w8x/5rge3dYx/w1xceS5K0UlMLvao+Bzx6jiG7gQ/UyBHg+UkuGyqgJKmfIT4puhF4aGz9ZLftvycHJtnH6F08W7ZsOe8X3HrzJ877a5+uHnzbK9c6grQq/H4ezkwvilbV/qqar6r5ubklb0UgSTpPQxT6KWDz2PqmbpskaYaGKPRDwG91v+1yNfDNqnrK6RZJ0uqaeg49yYeBa4ANSU4CfwI8E6Cq3g0cBq4DFoHHgdevVlhJ0vKmFnpV7Z2yv4AbBkskSTovflJUkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1IhehZ5kZ5LjSRaT3LzE/i1J7k7yhST3J7lu+KiSpHOZWuhJ1gG3A9cCO4C9SXZMDPtj4GBVXQHsAf5q6KCSpHPr8w79KmCxqk5U1RPAAWD3xJgCfrhbfh7wX8NFlCT10afQNwIPja2f7LaN+1PgNUlOAoeBNy71REn2JVlIsnD69OnziCtJWs5QF0X3Au+vqk3AdcAHkzzluatqf1XNV9X83NzcQC8tSYJ+hX4K2Dy2vqnbNu564CBAVf0r8GxgwxABJUn99Cn0e4HtSbYluYTRRc9DE2O+BrwMIMmLGBW651QkaYamFnpVPQncCNwFPMDot1mOJrk1ya5u2JuANyT5EvBh4HVVVasVWpL0VOv7DKqqw4wudo5vu2Vs+Rjw0mGjSZJWwk+KSlIjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhrRq9CT7ExyPMlikpuXGfMbSY4lOZrkQ8PGlCRNs37agCTrgNuBXwFOAvcmOVRVx8bGbAfeDLy0qh5L8qOrFViStLQ+79CvAhar6kRVPQEcAHZPjHkDcHtVPQZQVQ8PG1OSNE2fQt8IPDS2frLbNu6FwAuT/HOSI0l2DhVQktTP1FMuK3ie7cA1wCbgc0l+uqq+MT4oyT5gH8CWLVsGemlJEvR7h34K2Dy2vqnbNu4kcKiq/reqvgp8mVHBn6Wq9lfVfFXNz83NnW9mSdIS+hT6vcD2JNuSXALsAQ5NjPk7Ru/OSbKB0SmYE8PFlCRNM7XQq+pJ4EbgLuAB4GBVHU1ya5Jd3bC7gEeSHAPuBv6wqh5ZrdCSpKfqdQ69qg4Dhye23TK2XMBN3UOStAb8pKgkNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDWiV6En2ZnkeJLFJDefY9yrk1SS+eEiSpL6mFroSdYBtwPXAjuAvUl2LDHuucAfAPcMHVKSNF2fd+hXAYtVdaKqngAOALuXGPdW4O3A/wyYT5LUU59C3wg8NLZ+stv2PUmuBDZX1SfO9URJ9iVZSLJw+vTpFYeVJC3vgi+KJnkG8BfAm6aNrar9VTVfVfNzc3MX+tKSpDF9Cv0UsHlsfVO37YznAi8GPpvkQeBq4JAXRiVptvoU+r3A9iTbklwC7AEOndlZVd+sqg1VtbWqtgJHgF1VtbAqiSVJS5pa6FX1JHAjcBfwAHCwqo4muTXJrtUOKEnqZ32fQVV1GDg8se2WZcZec+GxJEkr5SdFJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY3oVehJdiY5nmQxyc1L7L8pybEk9yf5dJLLh48qSTqXqYWeZB1wO3AtsAPYm2THxLAvAPNV9RLgTuDPhw4qSTq3Pu/QrwIWq+pEVT0BHAB2jw+oqrur6vFu9QiwadiYkqRp+hT6RuChsfWT3bblXA98cqkdSfYlWUiycPr06f4pJUlTDXpRNMlrgHngtqX2V9X+qpqvqvm5ubkhX1qSvu+t7zHmFLB5bH1Tt+0sSV4OvAX4xar69jDxJEl99XmHfi+wPcm2JJcAe4BD4wOSXAG8B9hVVQ8PH1OSNM3UQq+qJ4EbgbuAB4CDVXU0ya1JdnXDbgOeA3w0yReTHFrm6SRJq6TPKReq6jBweGLbLWPLLx84lyRphfykqCQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNaJXoSfZmeR4ksUkNy+x/1lJPtLtvyfJ1sGTSpLOaWqhJ1kH3A5cC+wA9ibZMTHseuCxqvoJ4B3A24cOKkk6tz7v0K8CFqvqRFU9ARwAdk+M2Q3c0S3fCbwsSYaLKUmaZn2PMRuBh8bWTwI/t9yYqnoyyTeBHwG+Pj4oyT5gX7f6rSTHzyc0sGHyuS8Sq5YrF/Yzz/fd8bpAF2suuHizmWsF8vYLynX5cjv6FPpgqmo/sP9CnyfJQlXNDxBpUOZaGXOt3MWazVwrs1q5+pxyOQVsHlvf1G1bckyS9cDzgEeGCChJ6qdPod8LbE+yLcklwB7g0MSYQ8Bru+VfBz5TVTVcTEnSNFNPuXTnxG8E7gLWAe+rqqNJbgUWquoQ8DfAB5MsAo8yKv3VdMGnbVaJuVbGXCt3sWYz18qsSq74RlqS2uAnRSWpERa6JDXioir0JM9O8vkkX0pyNMmfLTFm2dsMJHlzt/14klfMONdNSY4luT/Jp5NcPrbvO0m+2D0mLyivdq7XJTk99vq/PbbvtUm+0j1eO/m1q5zrHWOZvpzkG2P7VuV4jT3/uiRfSPLxJfbNfH71zDXz+dUz18znV89cazK/kjyY5N+6515YYn+SvKubR/cnuXJs34Ufr6q6aB5AgOd0y88E7gGunhjze8C7u+U9wEe65R3Al4BnAduA/wDWzTDXLwE/2C3/7plc3fq31vB4vQ74yyW+9gXAie7PS7vlS2eVa2L8GxldbF/V4zX2/DcBHwI+vsS+mc+vnrlmPr965pr5/OqTa63mF/AgsOEc+68DPtl9j1wN3DPk8bqo3qHXyLe61Wd2j8mrtsvdZmA3cKCqvl1VXwUWGd22YCa5quruqnq8Wz3C6Pf1V1XP47WcVwCfqqpHq+ox4FPAzjXKtRf48BCvPU2STcArgfcuM2Tm86tPrrWYX31yncOqza/zyDWz+dXDbuAD3ffIEeD5SS5joON1URU6fO/HqC8CDzP6D7xnYshZtxkAztxmYKlbFGycYa5x1zP6V/iMZydZSHIkya8OlWkFuV7d/Xh3Z5IzHxK7KI5Xd+pgG/CZsc2rdryAdwJ/BHx3mf1rMr965Bo3s/nVM9fM51fPXGsxvwr4xyT3ZXSrk0nLHZdBjtdFV+hV9Z2q+hlG70CuSvLiNY4E9M+V5DXAPHDb2ObLa/Qx398E3pnkx2eY6++BrVX1Ekb/6t/BDKzg73EPcGdVfWds26ocrySvAh6uqvuGeL6hrCTXLOdXz1wzn18r/Huc2fzq/HxVXcno7rQ3JPmFAZ97qouu0M+oqm8Ad/PUHzuWu81An1sUrGYukrwceAuwq6q+PfY1p7o/TwCfBa6YVa6qemQsy3uBn+2W1/x4dfYw8ePwKh6vlwK7kjzI6K6hv5zkbyfGrMX86pNrLebX1FxrNL96Ha/OLOfX+HM/DHyMp56WW+64DHO8hrgQMNQDmAOe3y3/APBPwKsmxtzA2RetDnbLP8XZF61OMNxF0T65rmB0oWz7xPZLgWd1yxuArwA7ZpjrsrHlXwOO1P9fhPlql+/SbvkFs8rV7ftJRheRMovjNfHa17D0Rb6Zz6+euWY+v3rmmvn86pNrLeYX8EPAc8eW/wXYOTHmlZx9UfTzQx6vmd5tsYfLgDsy+p9qPIPRN9PH0+M2AzW6HcFB4BjwJHBDnf1j1mrnug14DvDR0TU0vlZVu4AXAe9J8t3ua99WVcdmmOv3k+xidEweZfRbCVTVo0neyuhePQC3VtWjM8wFo7+7A9XN6M5qHq8lXQTzq0+utZhffXKtxfzqkwtmP79+DPhY9/ezHvhQVf1Dkt8BqKp3A4cZ/abLIvA48Ppu3yDHy4/+S1IjLtpz6JKklbHQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiP+D6V3bGzcl4XKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for bid, sub_df in train_df_full.groupby('business_id'):\n",
    "    if len(sub_df) > 1:\n",
    "        print(sub_df[['business_id', 'funny', 'cool', 'stars']].head())\n",
    "        plt.hist(sub_df['stars'], bins=5)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1vihj9FT9AO1",
    "outputId": "55f72f68-5d72-4fc0-8c38-47d5ac95e941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     user_id  funny  cool  stars\n",
      "1173  -SjQXQd-IRfOdUdYYwWGOQ      0     1      4\n",
      "4503  -SjQXQd-IRfOdUdYYwWGOQ      0     0      1\n"
     ]
    }
   ],
   "source": [
    "for bid, sub_df in train_df_full.groupby('user_id'):\n",
    "    if len(sub_df) > 1:\n",
    "        print(sub_df[['user_id', 'funny', 'cool', 'stars']].head())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8AQAUH79AO1"
   },
   "source": [
    "## 3. Baselines\n",
    "\n",
    "Finally, we come up with two baselines for you to refer.\n",
    "We only use text data here and only consider first 5k training samples.\n",
    "\n",
    "For example, a baseline can be a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YaQHCMJV9AO1",
    "outputId": "8fa6d109-975c-482a-8f28-c6b2411bc41c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, stars] columns from the train split\n",
      "succeed!\n",
      "select [text, stars] columns from the valid split\n",
      "succeed!\n"
     ]
    }
   ],
   "source": [
    "#train_df = load_data('train')[:5000]\n",
    "train_df = load_data('train')\n",
    "valid_df = load_data('valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzNMVlj_9AO1"
   },
   "source": [
    "The split above is what we have done for you. You can use the data as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "XpYN_3Uq9AO1"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "krB_GACB9AO1"
   },
   "outputs": [],
   "source": [
    "x_train = train_df['text']\n",
    "y_train = train_df['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ub-ZIHou9AO2",
    "outputId": "f44cebab-fe4a-4788-a3a8-3c0a10879163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tfidf',\n",
      "                 TfidfVectorizer(tokenizer=<function tokenize at 0x171438e18>)),\n",
      "                ('lr', LogisticRegression())])\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenize)\n",
    "lr = LogisticRegression()\n",
    "steps = [('tfidf', tfidf),('lr', lr)]\n",
    "pipe = Pipeline(steps)\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ywXvoAn_9AO2",
    "outputId": "45ceafab-4282-4304-ff55-1515cb5900ac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(tokenizer=<function tokenize at 0x171438e18>)),\n",
       "                ('lr', LogisticRegression())])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bNfdA8xi9AO4",
    "outputId": "cf160f7a-715e-4b05-df37-d9b63eb1463c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.88      0.75       517\n",
      "           2       0.41      0.14      0.21       278\n",
      "           3       0.44      0.47      0.45       344\n",
      "           4       0.50      0.51      0.50       427\n",
      "           5       0.70      0.67      0.68       434\n",
      "\n",
      "    accuracy                           0.58      2000\n",
      "   macro avg       0.54      0.53      0.52      2000\n",
      "weighted avg       0.56      0.58      0.56      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[456  23  21  11   6]\n",
      " [119  38  96  20   5]\n",
      " [ 64  22 160  87  11]\n",
      " [ 22   6  78 217 104]\n",
      " [ 32   3  11  99 289]]\n",
      "accuracy 0.58\n"
     ]
    }
   ],
   "source": [
    "x_valid = valid_df['text']\n",
    "y_valid = valid_df['stars']\n",
    "y_pred = pipe.predict(x_valid)\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(confusion_matrix(y_valid, y_pred))\n",
    "print('accuracy', np.mean(y_valid == y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XD_poAnW9AO4"
   },
   "source": [
    "Of course, you can use deep learning.\n",
    "Here is a pytorch based baseline using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sdX-3RX9AO4"
   },
   "source": [
    "```bash\n",
    "pip install torch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "z8PKseV49AO4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "SMLeCgKO9AO4"
   },
   "outputs": [],
   "source": [
    "train_text = train_df['text'].map(tokenize).map(filter_stopwords).map(stem)\n",
    "valid_text = valid_df['text'].map(tokenize).map(filter_stopwords).map(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "yhh0J20m9AO4"
   },
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "for tokens in train_text:\n",
    "    for t in tokens:\n",
    "        if not t in word2id:\n",
    "            word2id[t] = len(word2id)\n",
    "word2id['<pad>'] = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "9G4kD_GE9AO5"
   },
   "outputs": [],
   "source": [
    "def texts_to_id_seq(texts, padding_length=500):\n",
    "    records = []\n",
    "    for tokens in texts:\n",
    "        record = []\n",
    "        for t in tokens:\n",
    "            record.append(word2id.get(t, len(word2id)))\n",
    "        if len(record) >= padding_length:\n",
    "            records.append(record[:padding_length])\n",
    "        else:\n",
    "            records.append(record + [word2id['<pad>']] * (padding_length - len(record)))\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "4B0SDYA-9AO5"
   },
   "outputs": [],
   "source": [
    "train_seqs = texts_to_id_seq(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "x7P3C_3Z9AO5"
   },
   "outputs": [],
   "source": [
    "valid_seqs = texts_to_id_seq(valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "UYv0-Piy9AO5"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seq, y):\n",
    "        assert len(seq) == len(y)\n",
    "        self.seq = seq\n",
    "        self.y = y-1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return np.asarray(self.seq[idx]), self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "vpeoevZH9AO5"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(MyDataset(train_seqs, y_train), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(MyDataset(valid_seqs, y_valid), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "YSROZU2d9AO5"
   },
   "outputs": [],
   "source": [
    "class mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mlp, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(word2id)+1, embedding_dim=64)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=3,\n",
    "                      stride=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=64,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=3,\n",
    "                      stride=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.linear = nn.Linear(64, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = self.cnn(x)\n",
    "        x = torch.max(x, dim=-1)[0]\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "xO0MyAC19AO6"
   },
   "outputs": [],
   "source": [
    "model = mlp()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "_XwoH9xN9AO6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for e in range(1, 11):    \n",
    "#     print('epoch', e)\n",
    "#     model.train()\n",
    "#     total_acc = 0\n",
    "#     total_loss = 0\n",
    "#     total_count = 0\n",
    "#     with tqdm.tqdm(train_loader) as t:\n",
    "#         for x, y in t:\n",
    "#             optimizer.zero_grad()\n",
    "#             logits = model(x)\n",
    "#             loss = criterion(logits, y)\n",
    "#             loss.backward()\n",
    "#             total_acc += (logits.argmax(1) == y).sum().item()\n",
    "#             total_count += y.size(0)\n",
    "#             total_loss += loss.item()\n",
    "#             optimizer.step()\n",
    "#             t.set_postfix({'loss': total_loss/total_count, 'acc': total_acc/total_count})\n",
    "\n",
    "#     model.eval()\n",
    "#     y_pred = []\n",
    "#     y_true = []\n",
    "#     with tqdm.tqdm(valid_loader) as t:\n",
    "#         for x, y in t:\n",
    "#             logits = model(x)\n",
    "#             total_acc += (logits.argmax(1) == y).sum().item()\n",
    "#             total_count += len(y)\n",
    "#             y_pred += logits.argmax(1).tolist()\n",
    "#             y_true += y.tolist()\n",
    "#     print(classification_report(y_true, y_pred))\n",
    "#     print(\"\\n\\n\")\n",
    "#     print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIkmkLkf9AO6"
   },
   "source": [
    "Deep learning are full of tricks. \n",
    "\n",
    "In the second example above, the implementation of CNN is not good enough to beat even TFIDF+Logistic regression.\n",
    "\n",
    "You can use all the techniques introduced in the lectures and tutorials to enhance your methods.\n",
    "\n",
    "Of course, you can use ideas have not been mentioned to make your model distinguished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "uZNOohZdWqJM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "              business_id  cool                 date  funny  \\\n",
      "0  7YYrZ9LgjpKLTtF-huhJug     0  2018-04-04 21:21:45      0   \n",
      "1  gyNixTgp1yFX97soBZpZ7Q     1  2013-07-10 00:04:01      0   \n",
      "2  vNWfQrQCa_XijstJbylcDQ     1  2015-10-28 01:23:21      2   \n",
      "3  wfxmuA7LbKZKVLV58EiWBw     0  2015-11-19 03:48:40      0   \n",
      "4  5jTmjxb1X34EfcY1gos4tw     0  2016-06-04 23:29:46      0   \n",
      "\n",
      "                review_id                                               text  \\\n",
      "0  b8-ELBwhmDKcmcM8icT86g  I took the UP Train to Union Station to catch ...   \n",
      "1  rBpAJhIen_V-zLoXZIcROg  We worked with Fitness with a Twist for part o...   \n",
      "2  _pALaDG6se9OTkGGhyhnNA  It's your typical, average, run-of-the-mill co...   \n",
      "3  ru8fpA1Uk0tTFtO5hLM49g  We went to Outback today to celebrate my daugh...   \n",
      "4  fRPgwuFoY6SriToXZyaOQA  We Went to see Nashville unplugged a country c...   \n",
      "\n",
      "   useful                 user_id  \n",
      "0       0  9Lglv-v8SRo_S-IyvFBmbw  \n",
      "1       1  zIl62G84XT2BwSIAjjjvYw  \n",
      "2       1  WP7FsUsgNW24s7HH5xi7pg  \n",
      "3       0  yLSj54f2YgGQu-lhPIhMTQ  \n",
      "4       1  73-u0a3G9Le4GWG7zLYWtg  \n",
      "                                                text  stars\n",
      "0  Nice to have a diner still around. Food was go...      4\n",
      "1  Tried this a while back, got the fried chicken...      2\n",
      "2  I expected more pork selections on menu. Food ...      4\n",
      "3  YUMMY!!! This place is phenomenal. It is Price...      4\n",
      "4  The Truffle Macaroni & Cheese and Potatoes Au ...      5\n"
     ]
    }
   ],
   "source": [
    "print(type(train_df))\n",
    "print(test_df.head())\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_dataset  (TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(6,), dtype=tf.float32, name=None))\n",
      "X_test_dataset  (TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 6), dtype=tf.float32, name=None))\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "#vocabulary_size = 5000\n",
    "#(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "\n",
    "# print(\"X_train : \",X_train)\n",
    "# print(\"y_train : \",y_train)\n",
    "# print(\"x_test : \",X_test)\n",
    "#print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))\n",
    "\n",
    "# pre processing data\n",
    "from string import punctuation\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout,SpatialDropout1D, GRU, SimpleRNN\n",
    "from keras.utils import to_categorical\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "\n",
    "\n",
    "#all_text = ''.join([c for c in train_df['text'] if c not in punctuation])\n",
    "#print(\"all_text : \",len(all_text))\n",
    "#print(all_text[:3])\n",
    "\n",
    "def encodeWords(df):\n",
    "  all_text2 = ' '.join([c for c in df])\n",
    "  # create a list of words\n",
    "  words = all_text2.split()\n",
    "  # Count all the words using Counter Method\n",
    "  count_words = Counter(words)\n",
    "\n",
    "  total_words = len(words)\n",
    "  sorted_words = count_words.most_common(total_words)\n",
    "  print(total_words,sorted_words)\n",
    "\n",
    "\n",
    "  vocab = sorted(count_words, key=count_words.get, reverse=True)\n",
    "  print(vocab)\n",
    "\n",
    "  vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "  reviews_ints = [] \n",
    "  for review in df:     \n",
    "    reviews_ints.append([vocab_to_int[word] for word in review.split()])\n",
    "  return reviews_ints\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "#max_features = 1000\n",
    "#tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "\n",
    "print(\"filtering words\")\n",
    "\n",
    "#X_train = train_df['text'].map(tokenize).map(filter_stopwords).map(stem).map(lower)\n",
    "#X_test = valid_df['text'].map(tokenize).map(filter_stopwords).map(stem).map(lower)\n",
    "\n",
    "\n",
    "#X_train = train_df['text'].map(filter_combo)\n",
    "#X_test = valid_df['text'].map(filter_combo)\n",
    "\n",
    "# X_train = train_df['text'].map(tokenize).map(n_gram).map(lower)\n",
    "# X_test = valid_df['text'].map(tokenize).map(n_gram).map(lower)\n",
    "\n",
    "\n",
    "# complete sentence layer\n",
    "X_train = nor_ds(train_df[\"text\"].values)\n",
    "X_test  = nor_ds(valid_df[\"text\"].values)\n",
    "\n",
    "\n",
    "# additional information layer\n",
    "# cool, funny, useful\n",
    "\n",
    "inp_helpful_info_data = np.hstack([train_df[\"cool\"]+train_df[\"funny\"]+train_df[\"userful\"]])\n",
    "\n",
    "# adjective words layer\n",
    "# Embedding(vocab_size, embedding_size, input_length=max_length,mask_zero=True)\n",
    "inp_adj_data = keras.layers.Input(shape=(,))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "\n",
    "tokenizer = Tokenizer(split=' ')\n",
    "total_reviews =  [\" \".join(v) for v in X_train] + [\" \".join(v) for v in X_test]\n",
    "print(type(total_reviews),total_reviews[:3])\n",
    "\n",
    "#tokenizer.fit_on_texts(total_reviews)\n",
    "tokenizer.fit_on_texts(total_reviews)\n",
    "\n",
    "\n",
    "#print(\"tokenizer_word_index\",tokenizer.word_index)\n",
    "\n",
    "max_length = max([len(s.split()) for s in total_reviews])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# X_train = tokenizer.texts_to_sequences(train_df['text'].values)\n",
    "# X_train = pad_sequences(X_train)\n",
    "\n",
    "X_train_tokens = tokenizer.texts_to_sequences(total_reviews)\n",
    "\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "#X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding=\"post\")\n",
    "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length)\n",
    "#X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding=\"post\")\n",
    "\n",
    "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding=\"post\")\n",
    "\n",
    "\n",
    "encoder = layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=vocab_size)\n",
    "\n",
    "X_train_vec = pd.Series([\" \".join(v) for v in X_train]).astype(str)\n",
    "\n",
    "X_train_dataset = pd.DataFrame(data={\n",
    "    'text': pd.Series([\" \".join(v) for v in X_train]).astype(str),\n",
    "    'label': train_df[\"stars\"]\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#X_train_dataset = tf.data.Dataset.from_tensor_slices([(line[0],line[1]) for line in X_train_dataset])\n",
    "X_train_dataset = tf.data.Dataset.from_tensor_slices((pd.Series([\" \".join(v) for v in X_train]).astype(str),\n",
    "                                                      to_categorical(train_df[\"stars\"].values)))\n",
    "print(\"X_train_dataset \",X_train_dataset.element_spec)\n",
    "X_test_vec = pd.Series([\" \".join(v) for v in X_test]).astype(str)\n",
    "encoder.adapt(X_train_dataset.map(lambda text, label: text))\n",
    "#encoder.adapt(X_test_vec.map(lambda text, label: text))\n",
    "X_test_dataset =tf.data.Dataset.from_tensor_slices((pd.Series([\" \".join(v) for v in X_test]).astype(str),\n",
    "                                                      to_categorical(valid_df[\"stars\"].values)))\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "X_train_dataset = X_train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "X_test_dataset = X_test_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "print(\"X_test_dataset \",X_test_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[h1]pre-trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 04:24:15: collecting all words and their counts\n",
      "INFO - 04:24:15: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 04:24:16: collected 392456 token types (unigram + bigrams) from a corpus of 531752 words and 10000 sentences\n",
      "INFO - 04:24:16: merged Phrases<392456 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 04:24:16: Phrases lifecycle event {'msg': 'built Phrases<392456 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.61s', 'datetime': '2021-03-31T04:24:16.588511', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO - 04:24:16: exporting phrases from Phrases<392456 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 04:24:17: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<149 phrases, min_count=30, threshold=10.0> from Phrases<392456 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.80s', 'datetime': '2021-03-31T04:24:17.476354', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO - 04:24:18: using concatenative 352-dimensional layer1\n",
      "INFO - 04:24:18: Doc2Vec lifecycle event {'params': 'Doc2Vec(dm/c,d32,n5,w5,mc2,s0.001,t4)', 'datetime': '2021-03-31T04:24:18.018982', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO - 04:24:18: collecting all words and their counts\n",
      "INFO - 04:24:18: PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO - 04:24:18: collected 26911 word types and 10000 unique tags from a corpus of 10000 examples and 520603 words\n",
      "INFO - 04:24:18: Creating a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nice', 'diner', 'food', 'good', 'comforting', 'definitely', 'good', 'spot', 'breakfast'], ['tried', 'got', 'fried_chicken', 'sandwich', 'meh', 'pretty', 'small', 'lacked', 'spices', 'flavors', 'expecting', 'chicken', 'sandwich', 'special', 'opinion', 'mary', 'browns', 'kfc', 'chicken', 'sandwiches', 'better', 'took', 'super', 'long', 'sandwich', 'store', 'pm', 'took', 'good', 'minutes', 'sandwich', 'combo', 'cashier', 'phone', 'time', 'updating', 'order'], ['expected', 'pork', 'selections', 'menu', 'food', 'good', 'beer_selection', 'good', 'pork_belly', 'app', 'best', 've', 'schnitzel', 'good', 'oddly', 'busy', 'friday_night'], ['yummy', 'place', 'phenomenal', 'pricey', 'feel', 'urge', 'splurge', 'place', 'food', 'great', 'server', 'attentive', 'yes', 'server', 'table', 'gets', 'servers'], ['truffle', 'macaroni', 'cheese', 'potatoes', 'au', 'gratin', 'short', 'amazing', 'death', 'row', 'choose', 'meal'], ['time', 'winking', 'lizard', 'area', 'enjoyed', 'bavarian', 'pretzels', 'sauces', 'enjoyed', 'pulled', 'chicken', 'melt', 'onion_rings', 'husband', 'special', 'brisket', 'potato', 'pancakes', 'pleased', 'fun', 'place', 'eat', 'traveling'], ['husbands', 'come', 'time', 'pick', 'gift', 'card', 'order', 'honey', 'chicken', 'meat', 'shrimp', 'fried_rice', 'appetizers', 'visit', 'crab', 'wonton', 'pork', 'egg', 'rolls', 'eat', 'honestly', 'offered', 'soup', 'guest', 'little', 'larger', 'portion_size', 'baffles', 'pay', 'plate', 'overloaded', 'rice', 'noodles', 'server', 'polite', 'complemented', 'son', 'mannerism', 'behavior', 'lovely', 'hear', 'cocktail', 'hold', 'bar', 'cocktail', 'lounge', 'suggest', 'drinks', 'suited', 'sake', 'sangria', 'tequila', 'margaritas', 'll', 'enjoy', 'eat', 'tejas', 'suggestion', 'pf', 'having', 'signature', 'yes', 'lettuce', 'wraps', 'fan', 'fav', 'like', 'signature', 'sake', 'cocktail', 'try', 'green', 'tea', 'dessert', 'sayin'], ['great', 'salad', 'bar', 'selection', 'die', 'garlic', 'cheese', 'biscuits', 'great', 'baked', 'potatoes', 'mixed', 'vegetable', 'platter', 'grilled', 'salmon', 'little', 'cooked', 'graciously', 'taken', 'replaced', 'second', 'try', 'good', 'flavor', 'scale', 'waitress', 'tae', 'delight', 'excellent', 'server', 'conscientious', 'excellent', 'meal', 'recommend', 'coming', 'nice', 'atmosphere', 'right', 'street', 'hampton', 'inn', 'stayed'], ['antique', 'sugar', 'cute', 'leads', 'believe', 'owners', 'anna', 'badass', 'lady', 'awesome', 'store', 'best', 'vintage', 'boutique', 'phoenix', 'definitely_worth', 'checking'], ['girlfriend', 'driving', 'home', 'mall', 'sunday', 'sudden', 'decided', 'needed', 'martinis', 'sure', 'ended', 'places', 'place', 've_eaten', 'delicious', 'decadent', 'brunch', 'mimosa', 'far', 'drinks', 'concerned', 'place', 'ended', 'sat_bar', 'pm', 'ordered', 'dirty', 'martinis', 'told', 'closed', 'pm', 'ok', 'martinis', 'perfect', 'good', 'sized', 'yummy', 'huge', 'olives', 'short', 'looking', 'got', 'checks', 'blown_away', 'craving', 'dirty', 'martini', 'early', 'sunday', 'afternoon', 'shopping', 'delicious', 'classic', 'cheap', 'loved', 'bravo', 'terrace', 'cafe'], ['went', 'girls', 'dinner', 'recently', 'chill', 'tasty', 'highlights', 'lighter', 'smaller', 'plates', 'great', 'sharing', 'beet', 'salad', 'manchego', 'cheese', 'crustini', 'paced', 'things', 'server', 'kind', 'bit', 'slow', 'start', 'ginger', 'beer', 'wine', 'list', 'great', 'creative', 'funky', 'cool', 'grown', 'slightly', 'laid', 'vibe', 'lighting', 'warm', 'romantic', 'wine', 'lining', 'walls', 'older', 'crowd', 'good', 'place', 'taking', 'parents', 'colleagues', 'worked', 'girl', 'chats', 'crazy', 'rush', 'nice', 'place', 'roster', 'fancy', 'wine', 'oriented', 'meal'], ['mad', 'spending', 'time', 'money', 'friends', 'went', 'vegas', 'days', 'aug', 'sept', 'neat', 'little', 'ticket', 'office', 'outside', 'csi', 'experience', 'buy', 'events', 'thought', 'csi', 'experience', 'cool', 'turned', 'laughable', 'crappiest', 'low', 'point', 'trip', 'assigned', 'crime', 'solve', 'discovered', 'solve', 'use', 'brain', 'whatsoever', 'assume', 'idiot', 'basically', 'tell', 'obvious', 'way', 'step', 'awesome', 'probably', 'yr', 'old', 'don', 'wet', 'bed', 'anymore', 'assenine', 'experience', 'leave', 'mad', 'spent', 'money', 'personally', 'vowing', 'review', 'kill', 'craptastic', 'un', 'attraction', 'ground'], ['getting', 'habit', 'giving', 'stars', 'think', 'getting', 'better', 'finding', 'good', 'places', 'went', 'lemongrass', 'lunch', 'mainly', 'hotel', 'aria', 'wanted', 'asian', 'cuisine', 'service', 'gracious', 'kind', 'server', 'passed', 'pleasant', 'eager', 'peach', 'ginger', 'cocktail', 'refreshing', 'creative', 'moved', 'potstickers', 'potsticker', 'lover', 'lookout', 'large', 'nice', 'dough', 'steamed', 'fried', 'perfection', 'nice', 'spicy', 'sauce', 'moved', 'sesame', 'chicken', 'know', 'chinese', 'real', 'original', 'delicious', 'sweet', 'chili', 'sauce', 'wasn', 'cloying', 'sweet', 'ambiance', 'beautiful', 'serene', 'oasis', 'tucked', 'away', 'casino', 'nice', 'romantic', 'spot', 'enjoyable'], ['better', 'bbq', 'better', 'service', 'way', 'money', 'lucille', 'excited_try', 'place', 'parents', 'talked', 'maybe', 'went', 'high_expectations', 'hostess', 'misplaced', 'brain', 'cells', 'asked', 'party', 'times', 'numbers', 'table', 'ready', 'water', 'ok', 'super', 'slow', 'came', 'checks', 'actually', 'checking', 'liked', 'biscuits', 'amazing', 'nice', 'touch', 'waiting', 'long', 'table', 'party', 'amazed', 'meals', 'disappointing', 'mac_cheese', 'sounded', 'good', 'came', 'dry', 'mushy', 'gross', 'turn', 'mac_cheese', 'couple', 'weeks', 'food', 'extremely', 'loud', 'restaurant', 'consider', 'second', 'shot'], ['absolutely', 'horrible', 'company', 'rude', 'general', 'manager', 'customer_service', 'paid', 'look', 'diagnose', 'washing', 'machine', 'didn', 'work', 'day', 'refuse', 'come', 'issue', 'till', 'pay', 'originally', 'costs', 'manager', 'rude', 'mother', 'phone', 'said', 'wasn', 'technician', 'wouldn', 'know', 'costs', 'needs', 'big', 'scam'], ['girlfriends', 'went', 'summerlious', 'appetizer', 'ok', 'pate', 'oyster', 'entree', 'got', 'steak', 'frites', 'fries', 'overly', 'salted', 'steak', 'disappointment', 'asked', 'mediums', 'rare', 'waiter', 'brought', 'mediums', 'sit', 'wait', 'steak', 'friends', 'eating', 'theirs', 'mention', 'medium_rare', 'send', 'dessert', 'good', 'creme', 'brul', 'smooth', 'eggs', 'cooked', 'process', 'service', 'terrible', 'waiters', 'hardly', 'came', 'ask', 'food', 'okay', 'terrible'], ['food', 'bland', 'ordered', 'breakfast', 'burritos', 'steak', 'chorizo', 'blackened', 'chicken', 'nachos', 'burritos', 'came', 'ratio', 'potato', 'steak', 'eggs', 'don_think', 'ratios', 'proportionate', 'taste', 'different', 'adding', 'sauce', 'meal', 'appetizing', 'blackened', 'chicken', 'nachos', 'decent', 'fresh', 'guac', 'gave', 'stars', 'solely', 'guacamole', 'water', 'flavor', 'breakfast', 'burritos'], ['come', 'years_ago', 'different', 'place', 'came', 'early', 'mothers', 'day', 'dinner', 'seated', 'right_away', 'crowded', 'table', 'placed', 'clean', 'food', 'crumbs', 'waiter', 'nice', 'guy', 'took', 'min', 'food', 'stopped', 'table', 'tell', 'right', 'minutes_later', 'waiting', 'bread', 'basket', 'offers', 'comes', 'food', 'son', 'ordered', 'salmon', 'literally', 'smallest', 'piece', 'salmon', 've_seen', 'maybe', 'inches', 'long', 'stared', 'shock', 'yes', 'tasted', 'good', 'ridiculously', 'small', 'ordered', 'reuben', 'sandwich', 'pretty_good', 'special', 'came', 'handful', 'stale', 'chips', 'husband', 'ordered', 'ahi', 'said', 'okay', 'tiny', 'portion', 'certainly', 'dinner', 'sized', 'portion', 'saw', 'server', 'times', 'order', 'tell', 'min', 'waiting', 'food', 'right', 'bring', 'food', 'drop', 'check', 'nice', 'fellow', 'horrible', 'service', 'way', 'priced', 'sadly', 'come'], ['lissie', 'tried', 'contact', 'risk', 'management', 'left', 'messages', 'received', 'return', 'calls', 'nt', 'think', 'want', 'address', 'issue'], ['guy', 'joshua', 'schorr', 'totally', 'soulless', 'inexperienced', 'interested', 'money', 'time', 'better', 'going', 'healer']]\n",
      "create taggedDocument\n",
      "build vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 04:24:18: Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 15697 unique words (58.32930771803352%% of original 26911, drops 11214)', 'datetime': '2021-03-31T04:24:18.226626', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 04:24:18: Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 509389 word corpus (97.84595939708376%% of original 520603, drops 11214)', 'datetime': '2021-03-31T04:24:18.227266', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 04:24:18: deleting the raw counts dictionary of 26911 items\n",
      "INFO - 04:24:18: sample=0.001 downsamples 26 most-common words\n",
      "INFO - 04:24:18: Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 489394.82028633525 word corpus (96.1%% of prior 509389)', 'datetime': '2021-03-31T04:24:18.330774', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 04:24:18: estimated required memory for 15697 words and 32 dimensions: 35239092 bytes\n",
      "INFO - 04:24:18: resetting layer weights\n",
      "INFO - 04:24:18: Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 15698 vocabulary and 352 features, using sg=0 hs=0 sample=0.001 negative=5 window=5', 'datetime': '2021-03-31T04:24:18.489292', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 04:24:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:18: EPOCH - 1 : training on 520603 raw words (499430 effective words) took 0.4s, 1145103 effective words/s\n",
      "INFO - 04:24:19: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:19: EPOCH - 2 : training on 520603 raw words (499273 effective words) took 0.4s, 1272150 effective words/s\n",
      "INFO - 04:24:19: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:19: EPOCH - 3 : training on 520603 raw words (499381 effective words) took 0.4s, 1245828 effective words/s\n",
      "INFO - 04:24:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:20: EPOCH - 4 : training on 520603 raw words (499476 effective words) took 0.5s, 978584 effective words/s\n",
      "INFO - 04:24:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:20: EPOCH - 5 : training on 520603 raw words (499206 effective words) took 0.5s, 1045136 effective words/s\n",
      "INFO - 04:24:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:21: EPOCH - 6 : training on 520603 raw words (499347 effective words) took 0.4s, 1137423 effective words/s\n",
      "INFO - 04:24:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:21: EPOCH - 7 : training on 520603 raw words (499402 effective words) took 0.4s, 1223460 effective words/s\n",
      "INFO - 04:24:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:22: EPOCH - 8 : training on 520603 raw words (499553 effective words) took 0.4s, 1206361 effective words/s\n",
      "INFO - 04:24:22: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:22: EPOCH - 9 : training on 520603 raw words (499575 effective words) took 0.5s, 1044124 effective words/s\n",
      "INFO - 04:24:22: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:22: EPOCH - 10 : training on 520603 raw words (499464 effective words) took 0.4s, 1221546 effective words/s\n",
      "INFO - 04:24:23: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:23: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:23: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:23: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:23: EPOCH - 11 : training on 520603 raw words (499469 effective words) took 0.4s, 1255848 effective words/s\n",
      "INFO - 04:24:23: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:23: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:23: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:23: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:23: EPOCH - 12 : training on 520603 raw words (499450 effective words) took 0.4s, 1262005 effective words/s\n",
      "INFO - 04:24:24: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:24: EPOCH - 13 : training on 520603 raw words (499337 effective words) took 0.5s, 1062250 effective words/s\n",
      "INFO - 04:24:24: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:24: EPOCH - 14 : training on 520603 raw words (499416 effective words) took 0.4s, 1115728 effective words/s\n",
      "INFO - 04:24:25: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:25: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:25: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:25: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:25: EPOCH - 15 : training on 520603 raw words (499441 effective words) took 0.4s, 1120800 effective words/s\n",
      "INFO - 04:24:25: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:25: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:25: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:25: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:25: EPOCH - 16 : training on 520603 raw words (499513 effective words) took 0.5s, 1087155 effective words/s\n",
      "INFO - 04:24:25: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:25: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:25: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:25: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:25: EPOCH - 17 : training on 520603 raw words (499289 effective words) took 0.4s, 1207324 effective words/s\n",
      "INFO - 04:24:26: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:26: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:26: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:26: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:26: EPOCH - 18 : training on 520603 raw words (499389 effective words) took 0.4s, 1285244 effective words/s\n",
      "INFO - 04:24:26: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:26: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:26: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:26: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:26: EPOCH - 19 : training on 520603 raw words (499422 effective words) took 0.4s, 1130351 effective words/s\n",
      "INFO - 04:24:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:27: worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 04:24:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:27: EPOCH - 20 : training on 520603 raw words (499284 effective words) took 0.5s, 1072187 effective words/s\n",
      "INFO - 04:24:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:27: EPOCH - 21 : training on 520603 raw words (499286 effective words) took 0.4s, 1212863 effective words/s\n",
      "INFO - 04:24:28: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:28: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:28: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:28: EPOCH - 22 : training on 520603 raw words (499330 effective words) took 0.4s, 1261128 effective words/s\n",
      "INFO - 04:24:28: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:28: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:28: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:28: EPOCH - 23 : training on 520603 raw words (499485 effective words) took 0.4s, 1229839 effective words/s\n",
      "INFO - 04:24:28: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:28: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:28: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:28: EPOCH - 24 : training on 520603 raw words (499486 effective words) took 0.4s, 1338026 effective words/s\n",
      "INFO - 04:24:29: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:29: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:29: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:29: EPOCH - 25 : training on 520603 raw words (499320 effective words) took 0.4s, 1349143 effective words/s\n",
      "INFO - 04:24:29: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:29: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:29: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:29: EPOCH - 26 : training on 520603 raw words (499479 effective words) took 0.4s, 1360637 effective words/s\n",
      "INFO - 04:24:29: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:29: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:29: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:29: EPOCH - 27 : training on 520603 raw words (499527 effective words) took 0.4s, 1347913 effective words/s\n",
      "INFO - 04:24:30: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:30: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:30: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:30: EPOCH - 28 : training on 520603 raw words (499309 effective words) took 0.4s, 1356688 effective words/s\n",
      "INFO - 04:24:30: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:30: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:30: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:30: EPOCH - 29 : training on 520603 raw words (499294 effective words) took 0.4s, 1346490 effective words/s\n",
      "INFO - 04:24:31: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 04:24:31: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 04:24:31: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 04:24:31: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 04:24:31: EPOCH - 30 : training on 520603 raw words (499483 effective words) took 0.4s, 1374165 effective words/s\n",
      "INFO - 04:24:31: Doc2Vec lifecycle event {'msg': 'training on 15618090 raw words (14982116 effective words) took 12.6s, 1188086 effective words/s', 'datetime': '2021-03-31T04:24:31.100397', 'gensim': '4.0.0', 'python': '3.7.3 (default, Apr 24 2020, 18:51:23) \\n[Clang 11.0.3 (clang-1103.0.32.62)]', 'platform': 'Darwin-19.5.0-x86_64-i386-64bit', 'event': 'train'}\n",
      "/Library/Python/3.7/site-packages/ipykernel_launcher.py:115: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "WARNING - 04:24:31: destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n",
      "INFO - 04:24:31: storing 15698x32 projection weights into word2vec_model.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 15698\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.91670227 -1.72722411 -2.72844267 ... -0.46254328  0.47767064\n",
      "  -1.23713028]\n",
      " [ 0.47486478  1.29145908 -2.24726415 ...  1.32959628 -1.32926798\n",
      "   0.0663491 ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import MaxPool1D, MaxPooling1D, Conv1D\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import itertools\n",
    "\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "# X_train = encodeWords(train_df[\"text\"])\n",
    "# y_train = train_df[\"stars\"] \n",
    "\n",
    "# X_test = encodeWords(test_df[\"text\"])\n",
    "# y_test = y_valid\n",
    "\n",
    "# review_lens = Counter([len(x) for x in X_train])\n",
    "# print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "# print(\"Maximum review length: {}\".format(max(review_lens)))\n",
    "# y_review_lens = Counter([len(x) for x in X_test])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def pad_features(reviews_ints, seq_length):\n",
    "#   ''' Return features of review_ints, where each review is padded with 0's\n",
    "#   or truncated to the input seq_length.\n",
    "#   '''\n",
    "#   # getting the correct rows x cols shape\n",
    "#   features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "\n",
    "#   # for each review, I grab that review and\n",
    "#   for i, row in enumerate(reviews_ints):\n",
    "#   features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "#   return features\n",
    "\n",
    "\n",
    "\n",
    "# print('---review---')\n",
    "# print(X_train[6])\n",
    "# print('---label---')\n",
    "# print(y_train[6])\n",
    "\n",
    "\n",
    "# word2id = imdb.get_word_index()\n",
    "# id2word = {i: word for word, i in word2id.items()}\n",
    "# print('---review with words---')\n",
    "# print([id2word.get(i, ' ') for i in X_train[6]])\n",
    "# print('---label---')\n",
    "# print(y_train[6])\n",
    "\n",
    "\n",
    "# RNN model\n",
    "\n",
    "\n",
    "\n",
    "# pad sequence\n",
    "#max_words = max_features\n",
    "#X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "#X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "# define RNN model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "embedding_size=32\n",
    "\n",
    "# review_lines_test = X_train.tolist()+X_test.tolist()\n",
    "# print(type(review_lines_test),review_lines_test[0])\n",
    "# all_line_is_str = True\n",
    "# for i in review_lines_test:\n",
    "#     for j in i:\n",
    "#         if not isinstance(j, str):\n",
    "#             all_line_is_str = False\n",
    "#             break\n",
    "#     if not all_line_is_str:\n",
    "#         break\n",
    "# print(\"all_line_is_str : \",all_line_is_str)\n",
    "\n",
    "#pos_ds = nor_ds(train_df[\"text\"].values)\n",
    "\n",
    "phrases = Phrases(X_train, min_count=30, progress_per=10000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[X_train]\n",
    "\n",
    "all_sentences = [v for v in sentences]\n",
    "#all_sentences = ' '.join(itertools.chain(*all_sentences))\n",
    "print(all_sentences[:20])\n",
    "                     \n",
    "# model_w2v = Word2Vec(vector_size=embedding_size, window=5, min_count=3, workers=4,                                        \n",
    "#                     sample=6e-5, \n",
    "#                      alpha=0.03, \n",
    "#                      min_alpha=0.0007, \n",
    "#                      negative=20,)\n",
    "\n",
    "\n",
    "model_w2v = Doc2Vec(dm=1, dm_concat=1, vector_size=embedding_size, window=5, negative=5, min_count=2, workers=4, alpha=0.065, min_alpha=0.065)\n",
    "print(\"create taggedDocument\")\n",
    "document = [TaggedDocument(doc, [i]) for i, doc in enumerate(all_sentences)]\n",
    "print(\"build vocab\")\n",
    "model_w2v.build_vocab(document, progress_per=10000)\n",
    "print(\"training vocab\")\n",
    "model_w2v.train(document, total_examples=model_w2v.corpus_count, epochs=30, report_delay=1)\n",
    "model_w2v.init_sims(replace=True)\n",
    "\n",
    "#model_w2v = Word2Vec(sentences=common_texts, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
    "model_w2v.wv.save_word2vec_format(\"word2vec_model.txt\",binary=False)\n",
    "#print(\"similar dog \",model_w2v.wv.most_similar(\"dog\"))\n",
    "\n",
    "print(\"vocab size :\",model_w2v.wv.vectors.shape[0])\n",
    "\n",
    "# read from word2vec model\n",
    "embeddings_index ={}\n",
    "f = open(os.path.join(\"\", \"word2vec_model.txt\"), encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    #embedding_vector = embedding_index.get(word)\n",
    "    if word in model_w2v.wv:\n",
    "        #print(\"yes word in model\",word)\n",
    "        embedding_matrix[i] = model_w2v.wv.get_vector(word)\n",
    "\n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "best parameters: \n",
    "filter 256\n",
    "pool_size = 8\n",
    "gru unit 32\n",
    "batch_normal\n",
    "dense 32\n",
    "dropout 0.5\n",
    "dense 32\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "duPjGtlb9AO6",
    "outputId": "08c93b58-f67e-46ab-ff3f-c8a95835f67c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'> 3\n",
      "outlier :  0\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "X_train_len :  12000\n",
      "shape of X_train_pad :  (10000, 506)\n",
      "shape of Y_train :  (10000, 5)\n",
      "shape of X_test_pad :  (2000, 506)\n",
      "shape of Y_test  :  (2000, 5)\n",
      "Model: \"sequential_126\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_126 (Embedding)    (None, 506, 32)           932608    \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 504, 128)          12416     \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 502, 64)           24640     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_133 (MaxPoolin (None, 62, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_79 (Bidirectio (None, 128)               49920     \n",
      "_________________________________________________________________\n",
      "dense_240 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_181 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_241 (Dense)            (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,028,421\n",
      "Trainable params: 95,685\n",
      "Non-trainable params: 932,736\n",
      "_________________________________________________________________\n",
      "None\n",
      "test pad :  2000\n",
      "start training\n",
      "Epoch 1/100\n",
      "40/40 [==============================] - 9s 234ms/step - loss: 1.9385 - accuracy: 0.2538 - val_loss: 1.5869 - val_accuracy: 0.2255\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 9s 218ms/step - loss: 1.6003 - accuracy: 0.3462 - val_loss: 1.4735 - val_accuracy: 0.3920\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 9s 221ms/step - loss: 1.4525 - accuracy: 0.3981 - val_loss: 1.4135 - val_accuracy: 0.3970\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 9s 221ms/step - loss: 1.3524 - accuracy: 0.4308 - val_loss: 1.3544 - val_accuracy: 0.4365\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 9s 229ms/step - loss: 1.2787 - accuracy: 0.4578 - val_loss: 1.3405 - val_accuracy: 0.4400\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 10s 246ms/step - loss: 1.2333 - accuracy: 0.4772 - val_loss: 1.3271 - val_accuracy: 0.4230\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 9s 237ms/step - loss: 1.1849 - accuracy: 0.4964 - val_loss: 1.2125 - val_accuracy: 0.4900\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 9s 236ms/step - loss: 1.1405 - accuracy: 0.5229 - val_loss: 1.2360 - val_accuracy: 0.4940\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 9s 234ms/step - loss: 1.1189 - accuracy: 0.5305 - val_loss: 1.2121 - val_accuracy: 0.4875\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 9s 231ms/step - loss: 1.0763 - accuracy: 0.5576 - val_loss: 1.1801 - val_accuracy: 0.4960\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 10s 240ms/step - loss: 1.0389 - accuracy: 0.5666 - val_loss: 1.1784 - val_accuracy: 0.5075\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 9s 235ms/step - loss: 0.9949 - accuracy: 0.5922 - val_loss: 1.1742 - val_accuracy: 0.5055\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 10s 242ms/step - loss: 0.9751 - accuracy: 0.5969 - val_loss: 1.1877 - val_accuracy: 0.5000\n",
      "Epoch 14/100\n",
      "14/40 [=========>....................] - ETA: 5s - loss: 0.9290 - accuracy: 0.6177"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-306-0b7e8aa31401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m          )\n\u001b[1;32m    171\u001b[0m \u001b[0;31m# X_valid, Y_valid = X_train[:batch_size], Y_train[:batch_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#model.add(encoder),\n",
    "#embeddings_initializer=Constant(embedding_matrix),\n",
    "\n",
    "model.add(Embedding(vocab_size, \n",
    "                    embedding_size,\n",
    "                     weights=[embedding_matrix],\n",
    "                  #   embeddings_initializer=Constant(embedding_matrix),\n",
    "                    input_length=max_length,\n",
    "#                    mask_zero=True,\n",
    "                   trainable=False))\n",
    "\n",
    "#model.add(Embedding(vocab_size, embedding_size, input_length=max_length,mask_zero=True))\n",
    "#model.add(SpatialDropout1D(0.5))\n",
    "#model.add(layers.Bidirectional(layers.LSTM(embedding_size,return_sequences=True)))\n",
    "#model.add(layers.Bidirectional(layers.LSTM(embedding_size//2,return_sequences=True )))\n",
    "#model.add(SpatialDropout1D(0.5))\n",
    "#model.add(layers.Bidirectional(layers.LSTM(32, dropout=0.2, return_sequences=True)))\n",
    "#model.add(layers.Bidirectional(GRU(units=64, dropout=0.2, return_sequences=True)))\n",
    "#model.add(layers.Bidirectional(GRU(units=64, dropout=0.2, return_sequences=True)))\n",
    "\n",
    "\n",
    "#model.add(layers.Bidirectional(GRU(units=64, dropout=0.2, return_sequences=True)))\n",
    "model.add(Conv1D(128, kernel_size=3, padding='valid', activation='relu'))\n",
    "model.add(Conv1D(64, kernel_size=3, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=8))\n",
    "\n",
    "model.add(layers.Bidirectional(GRU(units=64, dropout=0.2)))\n",
    "\n",
    "\n",
    "\n",
    "#model.add(layers.Flatten())\n",
    "\n",
    "#model.add(layers.Bidirectional(GRU(units=32, dropout=0.2)))\n",
    "#model.add(layers.Bidirectional(LSTM(64,dropout=0.2, recurrent_dropout=0.2)))\n",
    "#model.add(layers.Bidirectional(LSTM(16)))\n",
    "#model.add(MaxPooling1D(pool_size=6))\n",
    "\n",
    "#model.add(layers.SimpleRNN(embedding_size//2))\n",
    "#model.add(SpatialDropout1D(0.2))\n",
    "#model.add(layers.Dropout(0.5))\n",
    "#model.add(layers.Bidirectional(layers.LSTM(256,dropout=0.6)))\n",
    "#model.add(layers.GlobalMaxPool1D())\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(LSTM(196,dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(64,activation='relu'))\n",
    "\n",
    "#model.add(layers.Bidirectional(LSTM(embedding_size//2, dropout=0.2, recurrent_dropout=0.2)))\n",
    "#model.add(SpatialDropout1D(0.4))\n",
    "#model.add(SpatialDropout1D(0.4))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dropout(0.4))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(layers.Dropout(0.5))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "# compile\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "batch_size = 256\n",
    "num_epochs = 7\n",
    "\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X_train_pad,to_categorical(train_df[\"stars\"].values), test_size = 0.33, random_state = 42)\n",
    "#print(X_train.shape,Y_train.shape)\n",
    "#print(X_test.shape,Y_test.shape)\n",
    "\n",
    "#Y_train = np.array(to_categorical(train_df[\"stars\"].values))\n",
    "#Y_test = np.array(to_categorical(valid_df[\"stars\"].values))\n",
    "\n",
    "\n",
    "\n",
    "sentiment = np.concatenate((train_df[\"stars\"].values, valid_df[\"stars\"].values)) - 1\n",
    "isWithInRange = True\n",
    "print(type(sentiment[0]), sentiment[0])\n",
    "for num in sentiment:\n",
    "    if num < 1 or num > 5:\n",
    "        isWithInRange = False\n",
    "        print(\"outlier : \",num)\n",
    "        break\n",
    "# train_feats_matrix = np.vstack(\n",
    "#     [model_w2v.wv.get_vector(word) for word in ])\n",
    "# valid_feats_matrix = np.vstack(\n",
    "#     [get_index_vector(f, feats_dict, max_len) for f in valid_feats])\n",
    "'''\n",
    "VALIDATION_SPLIT = 0.2\n",
    "indices = np.arange(X_train_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "shuffle_train_pad = X_train_pad\n",
    "shuffle_train_pad = X_train_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * X_train_pad.shape[0])\n",
    "\n",
    "sentiment = np.array(to_categorical(sentiment))\n",
    "\n",
    "\n",
    "X_train_pad = shuffle_train_pad[:-num_validation_samples]\n",
    "Y_train = sentiment[:-num_validation_samples]\n",
    "X_test_pad = shuffle_train_pad[-num_validation_samples:]\n",
    "Y_test = sentiment[-num_validation_samples:]\n",
    "\n",
    "# X_train_pad = np.vstack((X_train_pad))\n",
    "# Y_train = np.vstack((Y_train))\n",
    "# X_test_pad = np.vstack((X_test_pad))\n",
    "# Y_test = np.vstack((Y_test))\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "indices = np.arange(X_train_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "shuffle_train_pad = X_train_pad\n",
    "#shuffle_train_pad = X_train_pad[indices]\n",
    "#sentiment = sentiment[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * X_train_pad.shape[0])\n",
    "\n",
    "sentiment = np.array(to_categorical(sentiment))\n",
    "print(sentiment[:3])\n",
    "\n",
    "# X_train_pad = shuffle_train_pad[:-num_validation_samples]\n",
    "# Y_train = sentiment[:-num_validation_samples]\n",
    "# X_test_pad = shuffle_train_pad[-num_validation_samples:]\n",
    "# Y_test = sentiment[-num_validation_samples:]\n",
    "\n",
    "print(\"X_train_len : \",len(shuffle_train_pad))\n",
    "X_train_pad_tmp = shuffle_train_pad[:10000]\n",
    "Y_train_tmp = sentiment[:10000]\n",
    "X_test_pad_tmp = shuffle_train_pad[10000:]\n",
    "Y_test_tmp = sentiment[10000:]\n",
    "\n",
    "\n",
    "print(\"shape of X_train_pad : \",X_train_pad_tmp.shape)\n",
    "print(\"shape of Y_train : \",Y_train_tmp.shape)\n",
    "print(\"shape of X_test_pad : \",X_test_pad_tmp.shape)\n",
    "print(\"shape of Y_test  : \",Y_test_tmp.shape)\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "callback = EarlyStopping(monitor='val_accuracy', mode=\"max\",baseline=0.025,patience=5)\n",
    "\n",
    "#print(X_test_pad)\n",
    "#print(X_train[0])\n",
    "print(\"test pad : \",len(X_test_pad_tmp))\n",
    "print(\"start training\")\n",
    "#X_train = np.array([\" \".join(v) for v in X_train])\n",
    "#model.fit(X_train_pad, Y_train, epochs = num_epochs, batch_size=batch_size, validation_data=(X_test_pad, Y_test), callbacks=[callback])\n",
    "#            validation_steps=,\n",
    "history = model.fit(  X_train_pad_tmp, Y_train_tmp, \n",
    "            epochs=100,\n",
    "                    batch_size=batch_size,\n",
    "            validation_data=(X_test_pad_tmp,Y_test_tmp), \n",
    "\n",
    "          verbose=1,\n",
    "            callbacks=[callback]              \n",
    "         )\n",
    "# X_valid, Y_valid = X_train[:batch_size], Y_train[:batch_size]\n",
    "# X_train2, Y_train2 = X_train[batch_size:], Y_train[batch_size:]\n",
    "# history = model.fit(X_train2, Y_train2, validation_data=(X_valid, Y_valid), batch_size=batch_size, epochs=num_epochs)\n",
    "print(model.summary())\n",
    "# predict\n",
    "print(history)\n",
    "\n",
    "\n",
    "validation_size = 1500\n",
    "\n",
    "# test_loss, test_acc = model.evaluate(X_test_dataset)\n",
    "\n",
    "# print('Test Loss: {}'.format(test_loss))\n",
    "# print('Test Accuracy: {}'.format(test_acc))\n",
    "\n",
    "X_validate = X_test_pad_tmp[-validation_size:]\n",
    "Y_validate = Y_test_tmp[-validation_size:]\n",
    "\n",
    "#Y_test = np.array(to_categorical(X_[\"stars\"].values))\n",
    "\n",
    "#X_test = X_test_pad\n",
    "\n",
    "score,acc = model.evaluate(X_validate, Y_validate, verbose = 1, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n",
    "# plt.figure(figsize=(16,8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plot_graphs(history, 'accuracy')\n",
    "# plt.ylim(None,1)\n",
    "# plt.subplot(1,2,2)\n",
    "# plot_graphs(history, 'loss')\n",
    "# plt.ylim(0,None)\n",
    "# scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print('Test accuracy:', scores[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7348806e52d9b2d8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7348806e52d9b2d8\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mlxtend in /Users/jamie/Library/Python/3.7/lib/python/site-packages (0.18.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in /Library/Python/3.7/site-packages (from mlxtend) (0.24.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /Library/Python/3.7/site-packages (from mlxtend) (3.3.1)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/site-packages (from mlxtend) (40.8.0)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Library/Python/3.7/site-packages (from mlxtend) (1.2.0)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /Library/Python/3.7/site-packages (from mlxtend) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /Library/Python/3.7/site-packages (from mlxtend) (1.18.5)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /Library/Python/3.7/site-packages (from mlxtend) (1.4.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Python/3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (7.2.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /Library/Python/3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Library/Python/3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Python/3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Library/Python/3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Python/3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: six in /Library/Python/3.7/site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Library/Python/3.7/site-packages (from pandas>=0.24.2->mlxtend) (2020.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/jamie/Library/Python/3.7/lib/python/site-packages (from scikit-learn>=0.20.3->mlxtend) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y must be an integer array. Found float32. Try passing the array as y.astype(np.integer)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-274-eca30b9d645b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install mlxtend'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_decision_regions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_decision_regions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/mlxtend/plotting/decision_regions.py\u001b[0m in \u001b[0;36mplot_decision_regions\u001b[0;34m(X, y, clf, feature_index, filler_feature_values, filler_feature_ranges, ax, X_highlight, res, zoom_factor, legend, hide_spines, markers, colors, scatter_kwargs, contourf_kwargs, scatter_highlight_kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \"\"\"\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mcheck_Xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Validate X and y arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/mlxtend/utils/checking.py\u001b[0m in \u001b[0;36mcheck_Xy\u001b[0;34m(X, y, y_int)\u001b[0m\n\u001b[1;32m     21\u001b[0m         raise ValueError('y must be an integer array. Found %s. '\n\u001b[1;32m     22\u001b[0m                          \u001b[0;34m'Try passing the array as y.astype(np.integer)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                          % y.dtype)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: y must be an integer array. Found float32. Try passing the array as y.astype(np.integer)"
     ]
    }
   ],
   "source": [
    "%pip install mlxtend\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "plot_decision_regions(X_validate, Y_validate, clf=model, legend=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'History'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-276-671dbc05b9f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Python/3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2824\u001b[0m     return gca().plot(\n\u001b[1;32m   2825\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2826\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1746\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_autoscale_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36madd_line\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_line%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_update_line_limits\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   1989\u001b[0m         \u001b[0mFigures\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdating\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m         \"\"\"\n\u001b[0;32m-> 1991\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1992\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1993\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mget_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \"\"\"\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mrecache\u001b[0;34m(self, always)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0malways\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0myconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_yorig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_unmasked_float_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_to_unmasked_float_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'History'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model=Sequential()\n",
    "#model.add(encoder),\n",
    "#embeddings_initializer=Constant(embedding_matrix),\n",
    "\n",
    "model.add(Embedding(vocab_size, \n",
    "                    embedding_size,\n",
    "                     #weights=[embedding_matrix],\n",
    "                     embeddings_initializer=Constant(embedding_matrix),\n",
    "                    input_length=max_length,\n",
    "#                    mask_zero=True,\n",
    "                   trainable=False))\n",
    "\n",
    "#model.add(Embedding(vocab_size, embedding_size, input_length=max_length,mask_zero=True))\n",
    "#model.add(SpatialDropout1D(0.5))\n",
    "#model.add(layers.Bidirectional(layers.LSTM(embedding_size,return_sequences=True)))\n",
    "#model.add(layers.Bidirectional(layers.LSTM(embedding_size//2,return_sequences=True )))\n",
    "\n",
    "model.add(Conv1D(filters=embedding_size, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "#model.add(GRU(units=embedding_size//2))\n",
    "model.add(LSTM(embedding_size * 2,dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "\n",
    "\n",
    "#model.add(layers.SimpleRNN(embedding_size//2))\n",
    "#model.add(SpatialDropout1D(0.2))\n",
    "#model.add(layers.Dropout(0.5))\n",
    "#model.add(layers.Bidirectional(layers.LSTM(256,dropout=0.6)))\n",
    "#model.add(layers.Flatten())\n",
    "#model.add(LSTM(196,dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "#model.add(layers.Bidirectional(LSTM(embedding_size//2, dropout=0.2, recurrent_dropout=0.2)))\n",
    "#model.add(SpatialDropout1D(0.4))\n",
    "#model.add(SpatialDropout1D(0.4))\n",
    "model.add(Dense(embedding_size, activation='relu'))\n",
    "#model.add(layers.Dropout(0.2))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "# compile\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "batch_size = embedding_size\n",
    "num_epochs = 7\n",
    "\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X_train_pad,to_categorical(train_df[\"stars\"].values), test_size = 0.33, random_state = 42)\n",
    "#print(X_train.shape,Y_train.shape)\n",
    "#print(X_test.shape,Y_test.shape)\n",
    "\n",
    "#Y_train = np.array(to_categorical(train_df[\"stars\"].values))\n",
    "#Y_test = np.array(to_categorical(valid_df[\"stars\"].values))\n",
    "\n",
    "\n",
    "\n",
    "sentiment = np.concatenate((train_df[\"stars\"].values, valid_df[\"stars\"].values))\n",
    "\n",
    "\n",
    "# train_feats_matrix = np.vstack(\n",
    "#     [model_w2v.wv.get_vector(word) for word in ])\n",
    "# valid_feats_matrix = np.vstack(\n",
    "#     [get_index_vector(f, feats_dict, max_len) for f in valid_feats])\n",
    "'''\n",
    "VALIDATION_SPLIT = 0.2\n",
    "indices = np.arange(X_train_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "shuffle_train_pad = X_train_pad\n",
    "shuffle_train_pad = X_train_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * X_train_pad.shape[0])\n",
    "\n",
    "sentiment = np.array(to_categorical(sentiment))\n",
    "\n",
    "\n",
    "X_train_pad = shuffle_train_pad[:-num_validation_samples]\n",
    "Y_train = sentiment[:-num_validation_samples]\n",
    "X_test_pad = shuffle_train_pad[-num_validation_samples:]\n",
    "Y_test = sentiment[-num_validation_samples:]\n",
    "\n",
    "# X_train_pad = np.vstack((X_train_pad))\n",
    "# Y_train = np.vstack((Y_train))\n",
    "# X_test_pad = np.vstack((X_test_pad))\n",
    "# Y_test = np.vstack((Y_test))\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "indices = np.arange(X_train_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "shuffle_train_pad = X_train_pad\n",
    "#shuffle_train_pad = X_train_pad[indices]\n",
    "#sentiment = sentiment[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * X_train_pad.shape[0])\n",
    "\n",
    "sentiment = np.array(to_categorical(sentiment))\n",
    "\n",
    "# X_train_pad = shuffle_train_pad[:-num_validation_samples]\n",
    "# Y_train = sentiment[:-num_validation_samples]\n",
    "# X_test_pad = shuffle_train_pad[-num_validation_samples:]\n",
    "# Y_test = sentiment[-num_validation_samples:]\n",
    "\n",
    "print(\"X_train_len : \",len(shuffle_train_pad))\n",
    "X_train_pad_tmp = shuffle_train_pad[:5000]\n",
    "Y_train_tmp = sentiment[:5000]\n",
    "X_test_pad_tmp = shuffle_train_pad[5000:]\n",
    "Y_test_tmp = sentiment[5000:]\n",
    "\n",
    "\n",
    "print(\"shape of X_train_pad : \",X_train_pad_tmp.shape)\n",
    "print(\"shape of Y_train : \",Y_train_tmp.shape)\n",
    "print(\"shape of X_test_pad : \",X_test_pad_tmp.shape)\n",
    "print(\"shape of Y_test  : \",Y_test_tmp.shape)\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#print(X_test_pad)\n",
    "#print(X_train[0])\n",
    "print(\"test pad : \",len(X_test_pad_tmp))\n",
    "print(\"start training\")\n",
    "#X_train = np.array([\" \".join(v) for v in X_train])\n",
    "#model.fit(X_train_pad, Y_train, epochs = num_epochs, batch_size=batch_size, validation_data=(X_test_pad, Y_test), callbacks=[callback])\n",
    "#            validation_steps=,\n",
    "history = model.fit(  X_train_pad_tmp, Y_train_tmp, \n",
    "            epochs=10,\n",
    "                    batch_size=batch_size,\n",
    "            validation_data=(X_test_pad_tmp,Y_test_tmp), \n",
    "\n",
    "          verbose=1,\n",
    "            callbacks=[callback]              \n",
    "         )\n",
    "# X_valid, Y_valid = X_train[:batch_size], Y_train[:batch_size]\n",
    "# X_train2, Y_train2 = X_train[batch_size:], Y_train[batch_size:]\n",
    "# history = model.fit(X_train2, Y_train2, validation_data=(X_valid, Y_valid), batch_size=batch_size, epochs=num_epochs)\n",
    "print(model.summary())\n",
    "# predict\n",
    "print(history)\n",
    "\n",
    "\n",
    "validation_size = 1500\n",
    "\n",
    "# test_loss, test_acc = model.evaluate(X_test_dataset)\n",
    "\n",
    "# print('Test Loss: {}'.format(test_loss))\n",
    "# print('Test Accuracy: {}'.format(test_acc))\n",
    "\n",
    "X_validate = X_test_pad_tmp[-validation_size:]\n",
    "Y_validate = Y_test_tmp[-validation_size:]\n",
    "\n",
    "#Y_test = np.array(to_categorical(X_[\"stars\"].values))\n",
    "\n",
    "#X_test = X_test_pad\n",
    "\n",
    "score,acc = model.evaluate(X_validate, Y_validate, verbose = 1, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n",
    "# plt.figure(figsize=(16,8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plot_graphs(history, 'accuracy')\n",
    "# plt.ylim(None,1)\n",
    "# plt.subplot(1,2,2)\n",
    "# plot_graphs(history, 'loss')\n",
    "# plt.ylim(0,None)\n",
    "# scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print('Test accuracy:', scores[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 2s 68ms/step - loss: 0.3944 - accuracy: 0.8387\n",
      "score: 0.39\n",
      "acc: 0.84\n"
     ]
    }
   ],
   "source": [
    "validation_size = 1500\n",
    "'''\n",
    "valid_df = load_data('train')[5000:]\n",
    "\n",
    "\n",
    "X_test = valid_df['text'].map(tokenize).map(filter_stopwords).map(stem).map(lower)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "\n",
    "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding=\"post\")\n",
    "Y_test = np.array(to_categorical(valid_df[\"stars\"].values))\n",
    "#X_validate = X_test_pad[-validation_size:]\n",
    "#Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test_pad\n",
    "Y_test = Y_test\n",
    "print(X_test)\n",
    "print(Y_test)\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 1, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n",
    "'''\n",
    "\n",
    "X_validate = X_train_pad_tmp[-validation_size:]\n",
    "Y_validate = Y_train_tmp[-validation_size:]\n",
    "\n",
    "#Y_test = np.array(to_categorical(X_[\"stars\"].values))\n",
    "\n",
    "#X_test = X_test_pad\n",
    "\n",
    "score,acc = model.evaluate(X_validate, Y_validate, verbose = 1, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 3 4 3 3 2 2 3]\n"
     ]
    }
   ],
   "source": [
    "prob = model.predict(X_validate[1:10,])\n",
    "labels = np.argmax(prob, axis=-1)    \n",
    "print(labels+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sweet', 0.7394323945045471),\n",
       " ('strawberries', 0.6807177662849426),\n",
       " ('creamy', 0.6744882464408875),\n",
       " ('worship', 0.6715373992919922),\n",
       " ('tablemate', 0.6709817051887512),\n",
       " ('lite', 0.6688794493675232),\n",
       " ('volleyball', 0.6613449454307556),\n",
       " ('pineapple', 0.6601486206054688),\n",
       " ('posh', 0.6543916463851929),\n",
       " ('wholly', 0.6528293490409851)]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.most_similar(\"rich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_df[\"stars\"].values))\n",
    "print(np.concatenate((train_df[\"stars\"].values, valid_df[\"stars\"].values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = model.predict(X_test_tokens)\n",
    "words = [word for word, i in tokenizer.word_index.items() if word in model_w2v.wv]\n",
    "print(len(words))\n",
    "\n",
    "print(words)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "demo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
