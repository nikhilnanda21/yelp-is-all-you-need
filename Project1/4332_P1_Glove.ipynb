{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4332_P1_Glove.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAckpZDZG-BR"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gEmiUAEG8zx"
      },
      "source": [
        "!wget -q https://hkustconnect-my.sharepoint.com/:u:/g/personal/nnanda_connect_ust_hk/EfREjZqiZTlPqhqUPICBbPABdlgPumlaUVxPncm-_9aWIw?download=1 -O \"Project 1 - data.zip\"\n",
        "!unzip -q \"Project 1 - data.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyCUH6drFgou"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_q2Kr7_Fa-g",
        "outputId": "6c206cad-3645-47ee-fb5a-8687cb1c3d12"
      },
      "source": [
        "!pip -q install keras-layer-normalization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_uTe2vvFJTL"
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Dropout, BatchNormalization,\\\n",
        "    Activation, Input, Add, Concatenate, Embedding, Conv1D, MaxPool1D,\\\n",
        "    Flatten, LSTM, Bidirectional, MaxPooling1D, SimpleRNN, GRU, SpatialDropout1D\n",
        "from keras_layer_normalization import LayerNormalization\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21Z1gRo3H3Zh",
        "outputId": "12ac4f6d-a22b-431c-9a7c-56aa4021eddf"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noMKFCoiHxq1"
      },
      "source": [
        "stopwords = set(stopwords.words(\"english\"))\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Y4M188FjI3"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LdLcI3EFWIK"
      },
      "source": [
        "def load_data(split_name='train', columns=['text', 'stars']):\n",
        "    try:\n",
        "        print(f\"select [{', '.join(columns)}] columns from the {split_name} split\")\n",
        "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
        "        df = df.loc[:,columns]\n",
        "        print(\"succeed!\")\n",
        "        return df\n",
        "    except:\n",
        "        print(\"Failed, then try to \")\n",
        "        print(f\"select all columns from the {split_name} split\")\n",
        "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
        "        return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQj5souEFnD2",
        "outputId": "9dc177e3-be1d-43bf-c115-0b1873bcaa09"
      },
      "source": [
        "train_df = load_data('train', columns=['full'])\n",
        "valid_df = load_data('valid', columns=['full'])\n",
        "test_df = load_data('test', columns=['full'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "select [full] columns from the train split\n",
            "Failed, then try to \n",
            "select all columns from the train split\n",
            "select [full] columns from the valid split\n",
            "Failed, then try to \n",
            "select all columns from the valid split\n",
            "select [full] columns from the test split\n",
            "Failed, then try to \n",
            "select all columns from the test split\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM6i0QIj2ylX"
      },
      "source": [
        "# Feature Extractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbHQbGsU0sDl"
      },
      "source": [
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    :param text: a doc with multiple sentences, type: str\n",
        "    return a word list, type: list\n",
        "    e.g.\n",
        "    Input: 'Text mining is to identify useful information.'\n",
        "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
        "    \"\"\"\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "def stem(tokens):\n",
        "    \"\"\"\n",
        "    :param tokens: a list of tokens, type: list\n",
        "    return a list of stemmed words, type: list\n",
        "    e.g.\n",
        "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
        "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
        "    \"\"\"\n",
        "\n",
        "    return [ps.stem(token).lower() for token in tokens]\n",
        "\n",
        "# Just for testing, was not used in tutorial - removing stopwords doesn't help much\n",
        "def filter_stopwords(tokens):\n",
        "    \"\"\"\n",
        "    :param tokens: a list of tokens, type: list\n",
        "    return a list of filtered tokens, type: list\n",
        "    e.g.\n",
        "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
        "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
        "    \"\"\"\n",
        "    ### equivalent code\n",
        "    # results = list()\n",
        "    # for token in tokens:\n",
        "    #     if token not in stopwords and not token.isnumeric():\n",
        "    #         results.append(token)\n",
        "    # return results\n",
        "\n",
        "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J_DHdpi253v"
      },
      "source": [
        "def get_feats_dict(feats, min_freq=-1, max_freq=-1, max_size=-1):\n",
        "    \"\"\"\n",
        "    :param data: a list of features, type: list(list)\n",
        "    :param min_freq: the lowest fequency that the fequency of a feature smaller than it will be filtered out, type: int\n",
        "    :param max_freq: the highest fequency that the fequency of a feature larger than it will be filtered out, type: int\n",
        "    :param max_size: the max size of feature dict, type: int\n",
        "    return a feature dict that maps features to indices, sorted by frequencies\n",
        "    # Counter document: https://docs.python.org/3.6/library/collections.html#collections.Counter\n",
        "    \"\"\"\n",
        "    # count all features\n",
        "    feat_cnt = Counter(feats) # [\"text\", \"text\", \"mine\"] --> {\"text\": 2, \"mine\": 1}\n",
        "    if max_size > 0 and min_freq == -1 and max_freq == -1:\n",
        "        valid_feats = [\"<pad>\", \"<unk>\"] + [f for f, cnt in feat_cnt.most_common(max_size-2)]\n",
        "    else:\n",
        "        valid_feats = [\"<pad>\", \"<unk>\"]\n",
        "        for f, cnt in feat_cnt.most_common():\n",
        "            if (min_freq == -1 or cnt >= min_freq) and \\\n",
        "                (max_freq == -1 or cnt <= max_freq):\n",
        "                valid_feats.append(f)\n",
        "    if max_size > 0 and len(valid_feats) > max_size:\n",
        "        valid_feats = valid_feats[:max_size]\n",
        "    print(\"Size of features:\", len(valid_feats))\n",
        "    \n",
        "    # build a mapping from features to indices\n",
        "    feats_dict = dict(zip(valid_feats, range(len(valid_feats))))\n",
        "    return feats_dict\n",
        "\n",
        "def get_index_vector(feats, feats_dict, max_len):\n",
        "    \"\"\"\n",
        "    :param feats: a list of features, type: list\n",
        "    :param feats_dict: a dict from features to indices, type: dict\n",
        "    :param feats: a list of features, type: list\n",
        "    return a feature vector,\n",
        "    \"\"\"\n",
        "    # initialize the vector as all zeros\n",
        "    vector = np.zeros(max_len, dtype=np.int64)\n",
        "    for i, f in enumerate(feats):\n",
        "        if i == max_len:\n",
        "            break\n",
        "        # get the feature index, return 1 (<unk>) if the feature is not existed\n",
        "        f_idx = feats_dict.get(f, 1)\n",
        "        vector[i] = f_idx\n",
        "    return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji667tHM4oC6"
      },
      "source": [
        "# Create Input Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEMY1eAX3KEv",
        "outputId": "c5fba9c7-8a22-4c57-e12a-f8d4c21e48cc"
      },
      "source": [
        "min_freq = 3\n",
        "\n",
        "# load data\n",
        "train_texts, train_labels = train_df[\"text\"], train_df[\"stars\"]\n",
        "valid_texts, valid_labels = valid_df[\"text\"], valid_df[\"stars\"]\n",
        "\n",
        "# extract features\n",
        "train_tokens = [tokenize(text) for text in train_texts]\n",
        "valid_tokens = [tokenize(text) for text in valid_texts]\n",
        "\n",
        "\n",
        "train_stemmed = [stem(tokens) for tokens in train_tokens]\n",
        "valid_stemmed = [stem(tokens) for tokens in valid_tokens]\n",
        "\n",
        "# If stopwords not used\n",
        "train_feats = train_stemmed\n",
        "valid_feats = valid_stemmed\n",
        "\n",
        "# filtering stopwords didn't help much\n",
        "# train_feats = [filter_stopwords(tokens) for tokens in train_stemmed]\n",
        "# valid_feats = [filter_stopwords(tokens) for tokens in valid_stemmed]\n",
        "\n",
        "# build a mapping from features to indices\n",
        "feats_dict = get_feats_dict(chain.from_iterable(train_feats), min_freq=min_freq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of features: 9357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e96iHVrmK961"
      },
      "source": [
        "max_len = 100\n",
        "\n",
        "# build the feats_matrix\n",
        "# convert each example to a index vector, and then stack vectors as a matrix\n",
        "train_feats_matrix = np.vstack(\n",
        "    [get_index_vector(f, feats_dict, max_len) for f in train_feats])\n",
        "valid_feats_matrix = np.vstack(\n",
        "    [get_index_vector(f, feats_dict, max_len) for f in valid_feats])\n",
        "\n",
        "# convert labels to label_matrix\n",
        "num_classes = max(train_labels)\n",
        "# convert each label to a ont-hot vector, and then stack vectors as a matrix\n",
        "train_label_matrix = keras.utils.to_categorical(train_labels-1, num_classes=num_classes)\n",
        "valid_label_matrix = keras.utils.to_categorical(valid_labels-1, num_classes=num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIKkkPv4eCOK"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=100)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(train_texts).batch(128)\n",
        "vectorizer.adapt(text_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0sbPPPreLw8",
        "outputId": "de96419a-31f2-47fa-cd9c-5acedc330482"
      },
      "source": [
        "vectorizer.get_vocabulary()[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'and', 'i']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvvGLWHPeOS4",
        "outputId": "c68aa6c3-ef4d-4378-d9e1-33b97325787c"
      },
      "source": [
        "output = vectorizer([[\"the cat sat on the mat\"]])\n",
        "output.numpy()[0, :6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2, 2116,  465,   21,    2, 7576])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7xpRmFWeUXv"
      },
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGzYS4Qyebyx",
        "outputId": "83aeb2c5-850b-468a-d7d3-62d432032863"
      },
      "source": [
        "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "[word_index[w] for w in test]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 2116, 465, 21, 2, 7576]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j_0qnKyegOC",
        "outputId": "322b320d-e2a7-44fd-dfb2-aea7fe27be83"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-03 06:47:10--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-04-03 06:47:10--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-04-03 06:47:10--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.16MB/s    in 2m 40s  \n",
            "\n",
            "2021-04-03 06:49:50 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmSFDGP9eoKc",
        "outputId": "7bc428f8-b55a-4063-8394-14f4210f850d"
      },
      "source": [
        "path_to_glove_file = os.path.join(\n",
        "    \"glove.6B.100d.txt\"\n",
        ")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlzirz0RfZv8",
        "outputId": "f36d45b4-1297-4b34-af89-e7013d9a199e"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        # print(word)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converted 17209 words (2791 misses)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogMeN1K0fh37"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG_0mUJ6gBj4"
      },
      "source": [
        "x_train = vectorizer(np.array([[s] for s in train_texts])).numpy()\n",
        "x_valid = vectorizer(np.array([[s] for s in valid_texts])).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKC8Af7fh7h8",
        "outputId": "7b452761-4765-423a-aa1e-f8a0b285bfe9"
      },
      "source": [
        "x_train.shape, train_label_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 100), (10000, 5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6TZmnQ_ftVC",
        "outputId": "520aa995-87ea-4b78-ca39-a66ddc91c5a3"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "# embedded_sequences = embedding_layer(int_sequences_input)\n",
        "# x = layers.Conv1D(128, 5, activation=\"relu\", padding=\"same\")(embedded_sequences)\n",
        "# x = layers.MaxPooling1D(5)(x)\n",
        "# x = layers.Conv1D(128, 5, activation=\"relu\", padding=\"same\")(x)\n",
        "# x = layers.MaxPooling1D(5)(x)\n",
        "# x = layers.Conv1D(128, 5, activation=\"relu\", padding=\"same\")(x)\n",
        "# x = layers.GlobalMaxPooling1D()(x)\n",
        "# x = layers.Dense(128, activation=\"relu\")(x)\n",
        "# x = layers.Dropout(0.5)(x)\n",
        "# preds = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "# model = keras.Model(int_sequences_input, preds)\n",
        "# model.summary()\n",
        "\n",
        "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded_sequences = embedding_layer(int_sequences_input)\n",
        "x = layers.SpatialDropout1D(0.2)(embedded_sequences)\n",
        "# x = layers.Conv1D(128, 5, activation=\"relu\", padding=\"same\")(embedded_sequences)\n",
        "# x = layers.MaxPooling1D(5)(x)\n",
        "# x = layers.Conv1D(128, 5, activation=\"relu\", padding=\"same\")(x)\n",
        "# x = layers.MaxPooling1D(5)(x)\n",
        "# x = layers.Conv1D(128, 5, activation=\"relu\", padding=\"same\")(x)\n",
        "x = Bidirectional(LSTM(200))(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "# x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "preds = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "model = keras.Model(int_sequences_input, preds)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_34 (InputLayer)        [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_11 (Embedding)     (None, None, 100)         2000200   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d (SpatialDr (None, None, 100)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_10 (Bidirectio (None, 400)               481600    \n",
            "_________________________________________________________________\n",
            "dropout_35 (Dropout)         (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 128)               51328     \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 2,533,773\n",
            "Trainable params: 2,533,773\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3pfqlceg12c",
        "outputId": "244c7e0a-0ec7-4249-845f-fa6994595349"
      },
      "source": [
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
        ")\n",
        "\n",
        "model.fit(x_train, train_label_matrix, batch_size=128, epochs=20, validation_split=0.2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "63/63 [==============================] - 6s 47ms/step - loss: 1.4814 - acc: 0.3607 - val_loss: 1.3333 - val_acc: 0.4405\n",
            "Epoch 2/20\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 1.0817 - acc: 0.5626 - val_loss: 1.1501 - val_acc: 0.5390\n",
            "Epoch 3/20\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.8538 - acc: 0.6642 - val_loss: 1.2104 - val_acc: 0.4990\n",
            "Epoch 4/20\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.7059 - acc: 0.7120 - val_loss: 1.3664 - val_acc: 0.5205\n",
            "Epoch 5/20\n",
            "63/63 [==============================] - 2s 34ms/step - loss: 0.5664 - acc: 0.7817 - val_loss: 1.4966 - val_acc: 0.5350\n",
            "Epoch 6/20\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.4625 - acc: 0.8266 - val_loss: 1.5409 - val_acc: 0.5240\n",
            "Epoch 7/20\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.3517 - acc: 0.8672 - val_loss: 1.8221 - val_acc: 0.4990\n",
            "Epoch 8/20\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.2950 - acc: 0.8870 - val_loss: 1.8737 - val_acc: 0.4890\n",
            "Epoch 9/20\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.2458 - acc: 0.9110 - val_loss: 2.0095 - val_acc: 0.4950\n",
            "Epoch 10/20\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.2181 - acc: 0.9238 - val_loss: 2.1526 - val_acc: 0.4990\n",
            "Epoch 11/20\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.1886 - acc: 0.9342 - val_loss: 2.2605 - val_acc: 0.4970\n",
            "Epoch 12/20\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.1731 - acc: 0.9374 - val_loss: 2.1525 - val_acc: 0.4970\n",
            "Epoch 13/20\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.1620 - acc: 0.9422 - val_loss: 2.3462 - val_acc: 0.5005\n",
            "Epoch 14/20\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.1471 - acc: 0.9508 - val_loss: 2.5468 - val_acc: 0.4960\n",
            "Epoch 15/20\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.1076 - acc: 0.9646 - val_loss: 2.6854 - val_acc: 0.4740\n",
            "Epoch 16/20\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.1098 - acc: 0.9595 - val_loss: 2.8491 - val_acc: 0.5025\n",
            "Epoch 17/20\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.1085 - acc: 0.9625 - val_loss: 2.7968 - val_acc: 0.4585\n",
            "Epoch 18/20\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.0925 - acc: 0.9661 - val_loss: 2.8178 - val_acc: 0.4865\n",
            "Epoch 19/20\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.0810 - acc: 0.9733 - val_loss: 2.8531 - val_acc: 0.4900\n",
            "Epoch 20/20\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.0772 - acc: 0.9704 - val_loss: 3.1286 - val_acc: 0.4950\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb40f7985d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cfe97ZGk3zG"
      },
      "source": [
        "# LSTM + CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12g8c7IQhFlQ"
      },
      "source": [
        "input_length = max_len\n",
        "vocab_size = len(voc) + 2\n",
        "embedding_size = 100\n",
        "hidden_size = 100\n",
        "num_filters = 100\n",
        "kernel_size = 2\n",
        "strides = 1\n",
        "output_size = num_classes\n",
        "dropout_rate = 0.5\n",
        "recurrent_dropout_rate = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpJ8dWnek6qm",
        "outputId": "424cb9f8-ff38-4780-f352-da9c805dad9b"
      },
      "source": [
        "x = Input(shape=(input_length,))\n",
        "\n",
        "# emb = Embedding(input_dim=vocab_size,\n",
        "#                         output_dim=embedding_size,\n",
        "#                         input_length=input_length,\n",
        "#                         embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "#                         trainable=True)(x)\n",
        "\n",
        "emb = Embedding(\n",
        "    vocab_size,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=True,\n",
        ")(x)\n",
        "\n",
        "emb = Dropout(dropout_rate)(emb)\n",
        "\n",
        "rec = Bidirectional(LSTM(hidden_size,\n",
        "                    kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n",
        "                    recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n",
        "                    return_sequences=True, dropout=dropout_rate, recurrent_activation=\"sigmoid\"))(emb)\n",
        "\n",
        "h = Concatenate()([emb, rec])\n",
        "\n",
        "conv = Conv1D(filters=num_filters, kernel_size=kernel_size, padding=\"valid\", strides=strides, activation=\"relu\")(h)\n",
        "\n",
        "# conv = Activation(\"tanh\")(conv)\n",
        "    \n",
        "maxpool = MaxPool1D(pool_size=(input_length-kernel_size)//strides+1)(conv)\n",
        "maxpool = Flatten()(maxpool)\n",
        "\n",
        "# maxpool = layers.GlobalMaxPooling1D()(conv)\n",
        "\n",
        "y = Dense(output_size,\n",
        "          activation=\"softmax\",\n",
        "          kernel_initializer=keras.initializers.he_normal(seed=0),\n",
        "          bias_initializer=\"zeros\")(maxpool)\n",
        "\n",
        "model = Model(x, y)\n",
        "# optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_25\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_37 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_14 (Embedding)        (None, 100, 100)     2000200     input_37[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 100, 100)     0           embedding_14[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_13 (Bidirectional (None, 100, 200)     160800      dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 100, 300)     0           dropout_39[0][0]                 \n",
            "                                                                 bidirectional_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_41 (Conv1D)              (None, 99, 100)      60100       concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_23 (MaxPooling1D) (None, 1, 100)       0           conv1d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 100)          0           max_pooling1d_23[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_48 (Dense)                (None, 5)            505         flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,221,605\n",
            "Trainable params: 2,221,605\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8onW41lhmM-X",
        "outputId": "f42b5baa-7c3f-4a61-edf1-238c40c1ec3e"
      },
      "source": [
        "model.fit(x_train, train_label_matrix,\n",
        "          validation_split=0.1,\n",
        "          epochs=30, batch_size=128, verbose=1, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "71/71 [==============================] - 7s 57ms/step - loss: 1.8600 - accuracy: 0.2641 - val_loss: 1.4677 - val_accuracy: 0.3760\n",
            "Epoch 2/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 1.4207 - accuracy: 0.3938 - val_loss: 1.2970 - val_accuracy: 0.4550\n",
            "Epoch 3/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 1.2812 - accuracy: 0.4625 - val_loss: 1.3371 - val_accuracy: 0.4420\n",
            "Epoch 4/30\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 1.2106 - accuracy: 0.4959 - val_loss: 1.2628 - val_accuracy: 0.4830\n",
            "Epoch 5/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 1.1568 - accuracy: 0.5175 - val_loss: 1.2233 - val_accuracy: 0.4940\n",
            "Epoch 6/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 1.0991 - accuracy: 0.5393 - val_loss: 1.1658 - val_accuracy: 0.5120\n",
            "Epoch 7/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 1.0743 - accuracy: 0.5562 - val_loss: 1.1742 - val_accuracy: 0.5210\n",
            "Epoch 8/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 1.0375 - accuracy: 0.5658 - val_loss: 1.1262 - val_accuracy: 0.5180\n",
            "Epoch 9/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 1.0387 - accuracy: 0.5704 - val_loss: 1.1668 - val_accuracy: 0.5130\n",
            "Epoch 10/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.9839 - accuracy: 0.5901 - val_loss: 1.1538 - val_accuracy: 0.5220\n",
            "Epoch 11/30\n",
            "71/71 [==============================] - 3s 38ms/step - loss: 0.9528 - accuracy: 0.6045 - val_loss: 1.1145 - val_accuracy: 0.5340\n",
            "Epoch 12/30\n",
            "71/71 [==============================] - 3s 38ms/step - loss: 0.9183 - accuracy: 0.6211 - val_loss: 1.1195 - val_accuracy: 0.5290\n",
            "Epoch 13/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.9002 - accuracy: 0.6299 - val_loss: 1.1334 - val_accuracy: 0.5360\n",
            "Epoch 14/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.8799 - accuracy: 0.6328 - val_loss: 1.2151 - val_accuracy: 0.5120\n",
            "Epoch 15/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.8442 - accuracy: 0.6533 - val_loss: 1.1569 - val_accuracy: 0.5300\n",
            "Epoch 16/30\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.8401 - accuracy: 0.6582 - val_loss: 1.1689 - val_accuracy: 0.5250\n",
            "Epoch 17/30\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.7997 - accuracy: 0.6769 - val_loss: 1.1514 - val_accuracy: 0.5390\n",
            "Epoch 18/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.7668 - accuracy: 0.6986 - val_loss: 1.1529 - val_accuracy: 0.5450\n",
            "Epoch 19/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.7567 - accuracy: 0.6873 - val_loss: 1.1398 - val_accuracy: 0.5500\n",
            "Epoch 20/30\n",
            "71/71 [==============================] - 3s 38ms/step - loss: 0.7192 - accuracy: 0.7073 - val_loss: 1.2730 - val_accuracy: 0.5180\n",
            "Epoch 21/30\n",
            "71/71 [==============================] - 3s 38ms/step - loss: 0.7097 - accuracy: 0.7181 - val_loss: 1.1543 - val_accuracy: 0.5530\n",
            "Epoch 22/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.6764 - accuracy: 0.7326 - val_loss: 1.2219 - val_accuracy: 0.5340\n",
            "Epoch 23/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.6654 - accuracy: 0.7390 - val_loss: 1.1618 - val_accuracy: 0.5590\n",
            "Epoch 24/30\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.6415 - accuracy: 0.7430 - val_loss: 1.2449 - val_accuracy: 0.5300\n",
            "Epoch 25/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.6162 - accuracy: 0.7554 - val_loss: 1.2366 - val_accuracy: 0.5380\n",
            "Epoch 26/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.5933 - accuracy: 0.7667 - val_loss: 1.2495 - val_accuracy: 0.5330\n",
            "Epoch 27/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.5688 - accuracy: 0.7824 - val_loss: 1.2870 - val_accuracy: 0.5150\n",
            "Epoch 28/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.5423 - accuracy: 0.7964 - val_loss: 1.2811 - val_accuracy: 0.5360\n",
            "Epoch 29/30\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.5133 - accuracy: 0.8076 - val_loss: 1.3105 - val_accuracy: 0.5350\n",
            "Epoch 30/30\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.4983 - accuracy: 0.8099 - val_loss: 1.3057 - val_accuracy: 0.5440\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb40b66ef90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plwXdLs1m_M2",
        "outputId": "769cc4eb-3180-42b2-ff5d-7b0802fa79e8"
      },
      "source": [
        "train_score = model.evaluate(x_train, train_label_matrix,\n",
        "                             batch_size=128)\n",
        "valid_score = model.evaluate(x_valid, valid_label_matrix,\n",
        "                            batch_size=128)\n",
        "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
        "print(\"valid loss:\", valid_score[0], \"valid accuracy\", valid_score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 1s 12ms/step - loss: 0.3605 - accuracy: 0.8911\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.2000 - accuracy: 0.5635\n",
            "training loss: 0.36045441031455994 training accuracy 0.8910999894142151\n",
            "valid loss: 1.2000067234039307 valid accuracy 0.5634999871253967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA8U5ujdvg8J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}